[["index.html", "Time Series 1 Chapter 1 Time series", " Time Series 1 by Dr. Susan Simmons 2025-10-30 Chapter 1 Time series This book contains the codes for Time Series. By the end of this course, you should be able to: * Create plots and clean time series data * Understand format of time series data * Identify the different characteristics of a time series (trend, cycle, seasonality, etc) * Perform a time series decomposition and understand the different decomposition techniques * Perform several exponential smoothing models * Understand stationarity and how to test for it * Understand and be able to use ACF and PACF plots to develop potential ARMA models * Be able to fit basic ARIMA models * Identify if white noise exists * Identify seasonality within time series data sets All of the data sets needed for this class are on the github repository. Enjoy your TIME in Time Series!! "],["introduction.html", "Chapter 2 Introduction 2.1 Other plots 2.2 Time series decomposition 2.3 STL features 2.4 Missing values 2.5 Python code and results for Time Series plot 2.6 SAS code for time series plot and decomposition", " Chapter 2 Introduction This document provides the code and output from Time Series. This is a living document and may be updated throughout the semester. First, here is a list of some the libraries that you will need: library(readr) library(fpp3) library(fable) library(fabletools) library(lubridate) library(expsmooth) library(lmtest) library(zoo) library(seasonal) library(ggplot2) library(seasonalview) library(aTSA) ### Note that the following packages and information is for Python: library(reticulate) use_python(&quot;C:\\\\ProgramData\\\\Anaconda3\\\\python.exe&quot;) The data sets that you will need are as follows: file.dir = &quot;https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/&quot; input.file1 = &quot;usairlines.csv&quot; input.file2 = &quot;steel.csv&quot; input.file3 = &quot;leadyear2.csv&quot; input.file4 = &quot;ebay9899.csv&quot; input.file5 = &quot;fpp_insurance.csv&quot; input.file6 = &quot;ar2.csv&quot; input.file7 = &quot;MA2.csv&quot; input.file8 = &quot;hurrican.csv&quot; USAirlines = read_csv(paste(file.dir, input.file1,sep = &quot;&quot;),show_col_types = FALSE) Steel = read_csv(paste(file.dir, input.file2, sep = &quot;&quot;),show_col_types = FALSE) Lead.Year = read_csv(paste(file.dir, input.file3, sep = &quot;&quot;),show_col_types = FALSE) Ebay = read_csv(paste(file.dir, input.file4, sep = &quot;&quot;),show_col_types = FALSE) Quotes= read_csv(paste(file.dir, input.file5, sep = &quot;&quot;),show_col_types = FALSE) Y= read_csv(paste(file.dir, input.file6, sep = &quot;&quot;),show_col_types = FALSE) x=read_csv(paste(file.dir, input.file7, sep = &quot;&quot;),show_col_types = FALSE) hurricane=read_csv(paste(file.dir, input.file8, sep = &quot;&quot;),show_col_types = FALSE) temp&lt;-read_csv(&quot;Q:\\\\My Drive\\\\Fall 2017 - Time Series\\\\DataR\\\\minntemp.csv&quot;, show_col_types = FALSE) For most of the applications we will be using, you will first need to create a tsibble. In order to create a tsibble, you need to have a column that identifies the date (which we will indicate with “index”). The code below creates tsibble for the Steel data set and the Minneapolis temperature data set and plots the data: Steel &lt;- Steel |&gt; mutate(date = seq(ymd(&#39;1932-07-01&#39;),ymd(&#39;1980-01-01&#39;),by=&#39;6 months&#39;)) steel_ts&lt;-Steel |&gt; mutate(Month=yearmonth(date)) |&gt; as_tsibble(index=Month) autoplot(steel_ts,steelshp) + labs(title= &quot;Iron and Steel Exports&quot;, subtitle = &quot;Weight in tons&quot;, y= &quot;Export&quot;) + theme_classic() temp_ts&lt;-temp |&gt; mutate(date = seq(ymd(&#39;1900-01-01&#39;),ymd(&#39;1966-03-01&#39;),by=&#39;months&#39;)) |&gt; mutate(Month=yearmonth(date)) |&gt; as_tsibble(index=Month) autoplot(temp_ts,Temp) + labs(title=&quot;Minneapolis temperature&quot;, y= &quot;Temperature&quot;) + theme_classic() Another example of creating a tsibble and time plot with the US Airlines data set: USAirlines_ts &lt;- USAirlines |&gt; mutate(date=myd(paste(Month, Year, &quot;1&quot;))) |&gt; mutate(Month2=yearmonth(date)) |&gt; as_tsibble(index=Month2) autoplot(USAirlines_ts,Passengers) 2.1 Other plots Some other interesting plots we can look at is the seasonal plot (only if our time series has a seasonal component….if you have yearly data, this is not possible since each line in the graph represents one year). USAirlines_ts |&gt; gg_season(Passengers, labels = &quot;both&quot;) + labs(y = &quot;Passengers&quot;, title = &quot;Seasonal plot: US Airline Passengers&quot;) We can further explore seasonal patterns by a subseries plot (again ONLY if there is a seasonal component): USAirlines_ts |&gt; gg_subseries(Passengers) + labs( y = &quot;Passengers&quot;, title = &quot;US Airline Passengers&quot; ) 2.2 Time series decomposition If your time series has a seasonal component to it, a useful visualization is the decomposition. We will first look at the STL decomposition (which can only do the additive decomposition, NOT multiplicative!). The following code creates the decomposition and then plots it: # Time Series Decomposition ...STL# dcmp &lt;- USAirlines_ts |&gt; model(stl = STL(Passengers)) components(dcmp) ## # A dable: 219 x 7 [1M] ## # Key: .model [1] ## # : Passengers = trend + season_year + remainder ## .model Month2 Passengers trend season_year remainder season_adjust ## &lt;chr&gt; &lt;mth&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 stl 1990 Jan 34348 39033. -4676. -8.97 39024. ## 2 stl 1990 Feb 33536 38897. -5842. 481. 39378. ## 3 stl 1990 Mar 40578 38762. 1238. 578. 39340. ## 4 stl 1990 Apr 38267 38626. -563. 204. 38830. ## 5 stl 1990 May 38249 38494. 378. -623. 37871. ## 6 stl 1990 Jun 40792 38362. 2950. -519. 37842. ## 7 stl 1990 Jul 42225 38229. 5219. -1223. 37006. ## 8 stl 1990 Aug 44943 38090. 6551. 302. 38392. ## 9 stl 1990 Sep 35708 37951. -1956. -287. 37664. ## 10 stl 1990 Oct 38286 37812. 345. 129. 37941. ## # i 209 more rows components(dcmp) |&gt; autoplot() + theme_classic() We can visualize the time plot with the trend component overlaid with it: components(dcmp) |&gt; as_tsibble() |&gt; autoplot(Passengers, colour=&quot;gray&quot;) + geom_line(aes(y=trend), colour = &quot;#D55E00&quot;) + labs( y = &quot;Passengers&quot;, title = &quot;US Airline Passengers with trend overlaid&quot; ) We can also overlay plots with the seasonally adjusted values (plot below has both trend and seasonally adjusted overlaid): components(dcmp) |&gt; as_tsibble() |&gt; autoplot(Passengers, colour=&quot;gray&quot;) + geom_line(aes(y=season_adjust), colour = &quot;#D55E00&quot;) + labs( y = &quot;Passengers&quot;, title = &quot;US Airline Passengers with seasonally adjusted overlaid&quot; ) ## Both components(dcmp) |&gt; as_tsibble() |&gt; autoplot(Passengers, colour=&quot;gray&quot;) + geom_line(aes(y=season_adjust), colour = &quot;#D55E00&quot;) + geom_line(aes(y=trend),colour=&quot;blue&quot;)+ labs( y = &quot;Passengers&quot;, title = &quot;US Airline Passengers &quot; ) Notice that the trend component is VERY similar to the “seasonally adjusted” data! Do you know what the difference between the lines are? We can also do a classical decomposition (this can do an additive decomposition and a multiplicative decomposition…just change type = “multiplicative” to do a multiplicative decomposition). USAirlines_ts |&gt; model( classical_decomposition(Passengers, type = &quot;additive&quot;) ) |&gt; components() |&gt; autoplot() + labs(title = &quot;Classical additive decomposition of US Airline Passengers&quot;) ## Warning: Removed 6 rows containing missing values or values outside the scale range (`geom_line()`). The last decomposition we will look at is the X11 decomposition (done by the government). This is a very complex algorithm that does the decomposition. x11_dcmp &lt;- USAirlines_ts |&gt; model(x11 = X_13ARIMA_SEATS(Passengers ~ x11())) |&gt; components() autoplot(x11_dcmp) + labs(title = &quot;Decomposition of US Airline Passengers using X-11.&quot;) 2.3 STL features We can also look at the strength of the trend and seasonal components in with a STL decomposition: USAirlines_ts |&gt; features(Passengers, feat_stl) ## # A tibble: 1 x 9 ## trend_strength seasonal_strength_year seasonal_peak_year seasonal_trough_year spikiness linearity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.976 0.919 7 2 994798525. 118740. ## # i 3 more variables: curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt; 2.4 Missing values We actually need to put the series in the OLD format to use the package imputeTS (will probably get updated, just not yet). We will illustrate this with the US Airlines data set (but first we need to create the missing values). #### Make some missing values: set.seed(19076) miss_val&lt;-sample(1:nrow(USAirlines),30,replace=F) miss_val&lt;-c(miss_val,102,104:107) US_temp&lt;-USAirlines US_temp$Passengers[miss_val]&lt;-NA col_vector=rep(&quot;blue&quot;,nrow(USAirlines)) col_vector[miss_val]=&quot;red&quot; ### Create ts using the old way: library(imputeTS) ## ## Attaching package: &#39;imputeTS&#39; ## The following object is masked from &#39;package:zoo&#39;: ## ## na.locf library(forecast) Pass_miss&lt;-ts(US_temp$Passengers,start = c(1990,1),frequency = 12) ggplot_na_distribution(Pass_miss)+labs(y=&quot;US Airline Passengers&quot;) # Interpolate the missing observations in this data set Pass_impute&lt;-Pass_miss %&gt;% na_interpolation(option = &quot;spline&quot;) autoplot(Pass_impute,color=col_vector) + labs(y=&quot;Airline Passengers&quot;) Pass_impute&lt;-Pass_miss %&gt;% na_seadec(algorithm = &quot;interpolation&quot;) autoplot(Pass_impute,color=col_vector) + labs(y=&quot;Airline Passengers&quot;) 2.5 Python code and results for Time Series plot The following code shows the time series plot and classical decomposition (using additive model). import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib import pyplot from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.seasonal import STL from statsforecast import StatsForecast usair_p=pd.read_csv(&quot;Q:\\\\My Drive\\\\Fall 2017 - Time Series\\\\DataR\\\\usairlines.csv&quot;) df=pd.date_range(start=&#39;1/1/1990&#39;, end=&#39;3/1/2008&#39;, freq=&#39;MS&#39;) usair_p.index=pd.to_datetime(df) plt.plot(usair_p[&quot;Passengers&quot;]) plt.xlabel(&quot;Date&quot;) plt.ylabel(&quot;Passengers&quot;) plt.title(&quot;Airline Passengers&quot;) plt.show() ## Classical Decomposition result = seasonal_decompose(usair_p[&quot;Passengers&quot;], model=&#39;additive&#39;, period=12) result.plot() pyplot.show() You can also specify multiplicative by changing “additive” to “multiplicative” in the Python code. If you want to do the STL decomposition, then you can apply the following code: import pandas as pd import matplotlib.pyplot as plt from matplotlib import pyplot from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.seasonal import STL usair_p=pd.read_csv(&quot;Q:\\\\My Drive\\\\Fall 2017 - Time Series\\\\DataR\\\\usairlines.csv&quot;) df=pd.date_range(start=&#39;1/1/1990&#39;, end=&#39;3/1/2008&#39;, freq=&#39;MS&#39;) usair_p.index=pd.to_datetime(df) ## STL Decomposition from statsforecast.models import MSTL d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: usair_p.index, &#39;y&#39;: usair_p[&#39;Passengers&#39;]} usair_sf = pd.DataFrame(data = d) dcmp = StatsForecast(models = [MSTL(season_length = 12)], freq = &#39;M&#39;) dcmp.fit(df = usair_sf) ## StatsForecast(models=[MSTL]) result = dcmp.fitted_[0,0].model_ result ## data trend seasonal remainder ## 0 34348.0 39019.259582 -4656.824185 -14.435397 ## 1 33536.0 38883.032235 -5824.359162 477.326927 ## 2 40578.0 38749.492861 1252.473058 576.034080 ## 3 38267.0 38618.252549 -554.514256 203.261707 ## 4 38249.0 38488.211906 388.629101 -627.841007 ## .. ... ... ... ... ## 214 61967.0 64483.390709 -2942.503405 426.112696 ## 215 60907.0 64538.640065 -1897.489966 -1734.150099 ## 216 57800.0 64584.717962 -6231.124988 -553.592974 ## 217 57109.0 64621.674775 -7705.357902 192.683127 ## 218 67818.0 64650.268751 3776.567142 -608.835893 ## ## [219 rows x 4 columns] dcmp.fitted_[0, 0].model_.plot(subplots=True, grid=True) ## array([&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;], dtype=object) plt.tight_layout() plt.show() ## The following code calculates the Fs and Ft (many thanks to Fred!) data = usair_p[&quot;Passengers&quot;] # Perform STL decomposition stl = STL(data, period=12) # Adjust period as needed for your data result = stl.fit() # Extract components trend = result.trend seasonal = result.seasonal residual = result.resid t_and_r = trend + residual # Ensure all series are aligned if not trend.empty and not residual.empty and not seasonal.empty: # Calculate Trend Strength var_residual = residual.var() var_t_plus_r = t_and_r.var() var_detrended = (data - trend).var() if var_detrended == 0: trend_strength = 0 print(&quot;Trend Strength: 0 (Detrended variance is zero)&quot;) else: trend_strength = 1 - (var_residual / var_t_plus_r) trend_strength = max(0, trend_strength) # Avoid negative strength print(f&quot;Trend Strength: {trend_strength:.2f}&quot;) # Calculate Seasonal Strength var_seasonal = seasonal.var() if var_detrended == 0: seasonal_strength = 0 print(&quot;Seasonal Strength: 0 (Detrended variance is zero)&quot;) else: seasonal_strength = var_seasonal / var_detrended seasonal_strength = max(0, seasonal_strength) # Avoid negative strength print(f&quot;Seasonal Strength: {seasonal_strength:.2f}&quot;) else: print(&quot;Trend, Seasonal, or Residual series is empty or misaligned.&quot;) ## Trend Strength: 0.98 ## Seasonal Strength: 0.92 2.6 SAS code for time series plot and decomposition The following code is in SAS (creates the time series plot and the decomposition…which is the next section). The use of the id statement can only be used if the series has a properly formatted date column. Great thing about SAS is that if there is a missing value (or missing date), it will impute it for you. It can also aggregate easily in SAS. If you do not have a date column, you can still use this command, you will use seasonality = and then set the length of the season. However, if you use the seasonality option, SAS will not recognize missing dates. Create library for data sets   libname Time ‘Q:Drive - Time Series’;   run;   Time series plot and decomposition of Airline Passengers   proc timeseries data=Time.USAirlines plots=(series decomp sc);   id date interval=month;   var Passengers;   run;   Time series and decomposition of Airline Passengers, but now the seasonal cycle is set to a year   proc timeseries data=Time.USAirlines plots=(series decomp sc) seasonality=12;   var Passengers;   run;   proc timeseries data=Time.Steel plots=(series decomp sc) seasonality=12;   var steelshp;   run;   "],["exponential-smoothing.html", "Chapter 3 Exponential Smoothing 3.1 Simple Exponential Smoothing 3.2 Holt ESM 3.3 Holt-Winters 3.4 Comparing forecasts 3.5 ETS 3.6 Python Code for Exponential Smoothing 3.7 SAS Code for Exponential Smoothing Models", " Chapter 3 Exponential Smoothing We will now get into our first modeling, which are exponential smoothing models. They have been around for quite some time, but can still be useful if interested in one-step ahead forecasting. In R, we are able to do Simple (or Single) Exponential Smoothing Models, Holt Exponential Smoothing Models and Holt-Winters Exponential Smoothing Models. We will be using the Steel data set and the airline data set to illustrate these models. Each of these are shown below. 3.1 Simple Exponential Smoothing For Simple Exponential Smoothing Models (SES), we have only one component, referred to as the level component. \\[\\hat{Y}_{t+1}= L_{t}\\\\ L_{t} = \\alpha Y_{t} + (1-\\alpha)L_{t-1}\\] This is basically a weighted average with the last observation and the last predicted value. Since this only has a level component, forecasts from SES models will be a horizontal line (hence why this method is called “one-step ahead” forecasting). Before modeling, be sure to divide your data into a training, validation and test (or at least training and test). The below code illustrates the Simple (Single) Exponential Smoothing Model. # Building a Single Exponential Smoothing (SES) Model - Steel Data # Steel &lt;- Steel |&gt; mutate(date = seq(ymd(&#39;1932-07-01&#39;),ymd(&#39;1980-01-01&#39;),by=&#39;6 months&#39;)) steel_ts&lt;-Steel |&gt; mutate(Month=yearmonth(date)) |&gt; as_tsibble(index=Month) steel_train &lt;-steel_ts |&gt; filter(year(date) &lt;= 1975) SES.Steel &lt;- steel_train |&gt; model(ETS(steelshp ~ error(&quot;A&quot;) + trend(&quot;N&quot;) + season(&quot;N&quot;))) Steel.for &lt;- SES.Steel |&gt; fabletools::forecast(h = 9) report(SES.Steel) ## Series: steelshp ## Model: ETS(A,N,N) ## Smoothing parameters: ## alpha = 0.466543 ## ## Initial states: ## l[0] ## 6269.498 ## ## sigma^2: 214894.3 ## ## AIC AICc BIC ## 1460.688 1460.977 1468.086 # Plot the SES model on steel data Steel.for |&gt; autoplot(steel_ts) + geom_line(aes(y = .fitted), col=&quot;#D55E00&quot;, data = augment(SES.Steel)) + labs(y=&quot;Steel Exports&quot;, title=&quot;Steel Exports&quot;) + guides(colour = &quot;none&quot;) # To get fitted values for training data set: Steel_fitted &lt;-fitted(SES.Steel)$.fitted # To get fitted values for test data set: Steel_test &lt;- Steel.for$.mean # Computes accuracy statistics for SES model on steel data (test data) fabletools::accuracy(Steel.for, steel_ts) ## # A tibble: 1 × 10 ## .model .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;ETS(steelshp ~ error(\\&quot;A\\&quot;) + trend(\\&quot;N\\&quot;)… Test 468. 599. 486. 6.75 7.04 1.17 1.13 -0.0500 3.2 Holt ESM The Holt model incorporates trend information. So, now there are two components: level and trend. For each component, there will be a smoothing coefficient (or weight). CAREFUL, when you look at parameter estimates, these are NOT the estimates for the mean nor the linear trend…you should be thinking of them as weights (between 0 and 1). The overall form for Holt’s method is: \\[\\hat{Y}_{t+h}= L_{t}+hT_{t}\\\\ L_{t} = \\alpha Y_{t} + (1-\\alpha)(L_{t-1}+T_{t-1})\\\\ T_{t} = \\beta (L_{t}-L_{t-1}) + (1-\\beta) T_{t-1}\\] For the Holt’s method, when you forecast, you will see a trending line. # Building a Linear Exponential Smoothing Model - US Airlines Data # USAirlines_ts &lt;- USAirlines |&gt; mutate(date=myd(paste(Month, Year, &quot;1&quot;))) |&gt; mutate(Month2=yearmonth(date)) |&gt; as_tsibble(index=Month2) air_train &lt;-USAirlines_ts |&gt; filter(year(date) &lt;= 2005) LES.air &lt;- air_train |&gt; model(ETS(Passengers ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;))) air.for &lt;- LES.air |&gt; fabletools::forecast(h = 27) report(LES.air) ## Series: Passengers ## Model: ETS(A,A,N) ## Smoothing parameters: ## alpha = 0.5860139 ## beta = 0.003740218 ## ## Initial states: ## l[0] b[0] ## 37303.09 526.6595 ## ## sigma^2: 23188824 ## ## AIC AICc BIC ## 4271.560 4271.882 4287.847 # Plot the data air.for |&gt; autoplot(USAirlines_ts) + geom_line(aes(y = .fitted), col=&quot;#D55E00&quot;, data = augment(LES.air)) + labs(y=&quot;Passengers&quot;, title=&quot;US Airline Passengers&quot;) + guides(colour = &quot;none&quot;) We can also perform Holt’s method with a damped trend. You will see the formula for the damped trend is similar to the previous Holt formula with an addition of a dampening parameter. \\[\\hat{Y}_{t+h}= L_{t}+\\sum_{i}^{k}\\phi^{i}T_{t}\\\\ L_{t} = \\alpha Y_{t} + (1-\\alpha)(L_{t-1}+\\phi T_{t-1})\\\\ T_{t} = \\beta (L_{t}-L_{t-1}) + (1-\\beta) \\phi T_{t-1}\\] We will illustrate the damped trend on the Airline data set. LdES.air &lt;- air_train |&gt; model(ETS(Passengers ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;) + season(&quot;N&quot;))) air.for &lt;- LdES.air |&gt; fabletools::forecast(h = 27) report(LdES.air) ## Series: Passengers ## Model: ETS(A,Ad,N) ## Smoothing parameters: ## alpha = 0.5705768 ## beta = 0.0001003564 ## phi = 0.8085131 ## ## Initial states: ## l[0] b[0] ## 36885.52 526.5046 ## ## sigma^2: 23007825 ## ## AIC AICc BIC ## 4271.031 4271.485 4290.576 # Plot the data air.for |&gt; autoplot(USAirlines_ts) + geom_line(aes(y = .fitted), col=&quot;#D55E00&quot;, data = augment(LdES.air)) + labs(y=&quot;Passengers&quot;, title=&quot;US Airline Passengers&quot;) + guides(colour = &quot;none&quot;) 3.3 Holt-Winters The Holt-Winters (HW) model has three components to it (level, trend and seasonality). Seasonality is an interesting component to model since we can have an additive seasonal component or a multiplicative seasonal component. Both models are shown below: Additive HW \\[\\hat{Y}_{t+h}= L_{t}+hT_{t} + S_{t-p+h}\\\\ L_{t} = \\alpha (Y_{t} - S_{t-p}) + (1-\\alpha)(L_{t-1}+T_{t-1})\\\\ T_{t} = \\beta (L_{t}-L_{t-1}) + (1-\\beta) T_{t-1}\\\\ S_{t} = \\gamma (Y_{t}-L_{t-1}-T_{t-1}) + (1-\\gamma) S_{t-p}\\] Multiplicative HW \\[\\hat{Y}_{t+h}= (L_{t}+hT_{t}) S_{t-p+h}\\\\ L_{t} = \\alpha \\frac{Y_{t}} {S_{t-p}} + (1-\\alpha)(L_{t-1}+T_{t-1})\\\\ T_{t} = \\beta (L_{t}-L_{t-1}) + (1-\\beta) T_{t-1}\\\\ S_{t} = \\gamma \\frac{Y_{t}}{L_{t-1}+T_{t-1}} + (1-\\gamma) S_{t-p}\\] Where p is the frequency of the seasonality (i.e. how many “seasons” there are within one year). # Building a Holt-Winters ESM - US Airlines Data - Additive Seasonality HWadd.air &lt;- air_train |&gt; model(ETS(Passengers ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;))) air.for &lt;- HWadd.air |&gt; fabletools::forecast(h = 27) report(HWadd.air) ## Series: Passengers ## Model: ETS(A,A,A) ## Smoothing parameters: ## alpha = 0.5913618 ## beta = 0.0001002723 ## gamma = 0.0001000455 ## ## Initial states: ## l[0] b[0] s[0] s[-1] s[-2] s[-3] s[-4] s[-5] s[-6] s[-7] s[-8] ## 38409 159.5944 -1520.744 -2728.928 -117.3477 -4305.732 6420.735 6299.852 4002.381 1210.882 122.2352 ## s[-9] s[-10] s[-11] ## 2660.169 -6471.051 -5572.452 ## ## sigma^2: 4099578 ## ## AIC AICc BIC ## 3950.201 3953.718 4005.578 # Plot the data air.for |&gt; autoplot(USAirlines_ts) + geom_line(aes(y = .fitted), col=&quot;#D55E00&quot;, data = augment(HWadd.air)) + labs(y=&quot;Passengers&quot;, title=&quot;US Airline Passengers&quot;) + guides(colour = &quot;none&quot;) ### Multiplicative model HWmult.air &lt;- air_train |&gt; model(ETS(Passengers ~ error(&quot;M&quot;) + trend(&quot;A&quot;) + season(&quot;M&quot;))) air.for &lt;- HWmult.air |&gt; fabletools::forecast(h = 27) report(HWmult.air) ## Series: Passengers ## Model: ETS(M,A,M) ## Smoothing parameters: ## alpha = 0.5132388 ## beta = 0.006772624 ## gamma = 0.0614166 ## ## Initial states: ## l[0] b[0] s[0] s[-1] s[-2] s[-3] s[-4] s[-5] s[-6] s[-7] ## 38328.12 132.2916 0.9879613 0.9516784 1.015964 0.9248462 1.127338 1.111153 1.063682 1.012367 ## s[-8] s[-9] s[-10] s[-11] ## 0.9931891 1.041972 0.8832477 0.8866022 ## ## sigma^2: 0.0015 ## ## AIC AICc BIC ## 3922.549 3926.066 3977.926 # Plot the data air.for |&gt; autoplot(USAirlines_ts) + geom_line(aes(y = .fitted), col=&quot;#D55E00&quot;, data = augment(HWmult.air)) + labs(y=&quot;Passengers&quot;, title=&quot;US Airline Passengers&quot;) + guides(colour = &quot;none&quot;) 3.4 Comparing forecasts This should be done on the validation data set (test data should ONLY be used ONCE…at the very end). air_fit &lt;- air_train |&gt; model( SES = ETS(Passengers ~ error(&quot;A&quot;) + trend(&quot;N&quot;) + season(&quot;N&quot;)), `Linear` = ETS(Passengers ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;)), `Damped Linear` = ETS(Passengers ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;) + season(&quot;N&quot;)), HWAdd = ETS(Passengers ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)), HWMult = ETS(Passengers ~ error(&quot;M&quot;) + trend(&quot;A&quot;) + season(&quot;M&quot;)) ) air_fc &lt;- air_fit |&gt; fabletools::forecast(h = 27) fabletools::accuracy(air_fc, USAirlines_ts) ## # A tibble: 5 × 10 ## .model .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Damped Linear Test 3351. 6251. 5238. 4.65 8.06 1.84 1.69 0.382 ## 2 HWAdd Test -175. 1462. 1259. -0.421 2.02 0.443 0.395 0.448 ## 3 HWMult Test 467. 1319. 1110. 0.705 1.75 0.391 0.356 0.219 ## 4 Linear Test -67.3 5345. 4736. -0.789 7.62 1.67 1.44 0.436 ## 5 SES Test 3346. 6248. 5236. 4.64 8.05 1.84 1.69 0.382 Based upone this information, which model would you choose? 3.5 ETS You can also allow the computer to search for the best model. The ETS (Error, Trend, Seasonality) algorithm will search for the best model and estimate the parameters. For the error term, we can have either an additive or multiplicative error structure. For the trend, we can have none, additive, or damped additive . For the seasonal component, we can have none, additive or multiplicative (lots of choices!). An example of how to run this is: air_auto &lt;- air_train |&gt; model(ETS(Passengers)) report(air_auto) ## Series: Passengers ## Model: ETS(M,Ad,M) ## Smoothing parameters: ## alpha = 0.6388447 ## beta = 0.0001026043 ## gamma = 0.0001060611 ## phi = 0.979993 ## ## Initial states: ## l[0] b[0] s[0] s[-1] s[-2] s[-3] s[-4] s[-5] s[-6] s[-7] ## 38326.08 97.13345 0.9672127 0.9438917 0.9983376 0.9209977 1.132478 1.132414 1.079442 1.022098 ## s[-8] s[-9] s[-10] s[-11] ## 1.000804 1.051137 0.8657376 0.8854499 ## ## sigma^2: 0.0014 ## ## AIC AICc BIC ## 3910.149 3914.103 3968.784 # Now compare this to the HW models: air_fit &lt;- air_train |&gt; model( HWAdd = ETS(Passengers ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)), HWMult = ETS(Passengers ~ error(&quot;M&quot;) + trend(&quot;A&quot;) + season(&quot;M&quot;)), AutoETS = ETS(Passengers ~ error(&quot;M&quot;) + trend(&quot;Ad&quot;) + season(&quot;M&quot;)) ) air_fc &lt;- air_fit |&gt; fabletools::forecast(h = 27) fabletools::accuracy(air_fc, USAirlines_ts) ## # A tibble: 3 × 10 ## .model .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AutoETS Test 1734. 2343. 2020. 2.75 3.19 0.711 0.633 0.596 ## 2 HWAdd Test -175. 1462. 1259. -0.421 2.02 0.443 0.395 0.448 ## 3 HWMult Test 467. 1319. 1110. 0.705 1.75 0.391 0.356 0.219 ### Can change to bic air_auto2 &lt;- air_train |&gt; model(ETS(Passengers, ic =&quot;bic&quot;)) report(air_auto2) ## Series: Passengers ## Model: ETS(M,N,M) ## Smoothing parameters: ## alpha = 0.7358545 ## gamma = 0.001388497 ## ## Initial states: ## l[0] s[0] s[-1] s[-2] s[-3] s[-4] s[-5] s[-6] s[-7] s[-8] ## 41491.07 0.9705455 0.9480705 0.9988812 0.9158342 1.119004 1.126052 1.073332 1.022722 1.003232 ## s[-9] s[-10] s[-11] ## 1.057297 0.8713743 0.8936557 ## ## sigma^2: 0.0015 ## ## AIC AICc BIC ## 3912.504 3915.231 3961.366 3.6 Python Code for Exponential Smoothing The following Python codes will produce exponential smoothing models. The exponential smoothing models are using an older version in statsmodels (a new format is in statsforecast). import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import pyplot import seaborn as sns from statsforecast.models import ETS from statsforecast import StatsForecast from statsforecast.utils import AirPassengers from statsmodels.tsa.tsatools import detrend from sklearn.metrics import mean_squared_error from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt from statsforecast.models import AutoETS steel=pd.read_csv(&quot;Q:\\\\My Drive\\\\Fall 2017 - Time Series\\\\DataR\\\\steel.csv&quot;) df = pd.date_range(start=&#39;1932-07-01&#39;, end=&#39;1980-07-01&#39;, freq=&#39;6ME&#39;) steel.index=pd.to_datetime(df) steel_train=steel.head(87) steel_test = steel.tail(9) fit = SimpleExpSmoothing(steel_train[&#39;steelshp&#39;]).fit() fit.params[&#39;smoothing_level&#39;] ## 0.4769767441860465 fcast = fit.forecast(9) fcast ## 1976-01-31 6188.807035 ## 1976-07-31 6188.807035 ## 1977-01-31 6188.807035 ## 1977-07-31 6188.807035 ## 1978-01-31 6188.807035 ## 1978-07-31 6188.807035 ## 1979-01-31 6188.807035 ## 1979-07-31 6188.807035 ## 1980-01-31 6188.807035 ## Freq: 6ME, dtype: float64 plt.plot(steel_train[&quot;steelshp&quot;],color=&quot;black&quot;) plt.plot(fcast,color=&quot;blue&quot;) plt.show() ##### Using Statsforecast d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: steel.index, &#39;y&#39;: steel[&#39;steelshp&#39;]} steel_sf = pd.DataFrame(data = d) steel_train=steel_sf.head(87) steel_test = steel_sf.tail(9) steel_SES = StatsForecast(models = [AutoETS(model=[&quot;A&quot;,&quot;N&quot;,&quot;N&quot;], alias=&quot;AutoETS&quot;, season_length=2)], freq = &#39;6ME&#39;) steel_model = steel_SES.fit(df = steel_train) result = steel_SES.fitted_[0,0].model_[&#39;par&#39;] result ## array([4.66739272e-01, nan, nan, nan, ## 6.27096958e+03]) y_hat1=steel_model.predict(h=9) ## C:\\PROGRA~3\\ANACON~1\\Lib\\site-packages\\statsforecast\\core.py:492: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning. ## warnings.warn( yhat=y_hat1.reset_index(drop=True) forecast=pd.Series(pd.date_range(&quot;1976-01-31&quot;, freq=&quot;6ME&quot;, periods=9)) forecast=pd.DataFrame(forecast) forecast.columns=[&quot;ds&quot;] forecast[&quot;hat&quot;]=yhat[&#39;AutoETS&#39;] forecast[&quot;unique_id&quot;]=&quot;1&quot; sns.lineplot(steel_train,x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Train&quot;) sns.lineplot(steel_test, x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Test&quot;) sns.lineplot(forecast,x=&quot;ds&quot;, y=&quot;hat&quot;, label=&quot;Forecast&quot;,) plt.show() The Holt models in Python: #### This code uses the new Statsforecast package ### Holt Model usair_p=pd.read_csv(&quot;Q:\\\\My Drive\\\\Fall 2017 - Time Series\\\\DataR\\\\usairlines.csv&quot;) df=pd.date_range(start=&#39;1/1/1990&#39;, end=&#39;3/1/2008&#39;, freq=&#39;MS&#39;) usair_p.index=pd.to_datetime(df) d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: usair_p.index, &#39;y&#39;: usair_p[&#39;Passengers&#39;]} usair_sf = pd.DataFrame(data = d) ### train has 192 observations ### test has 27 observations air_train = usair_sf.head(192) air_test = usair_sf.tail(27) air_holt = StatsForecast(models = [AutoETS(model=[&quot;A&quot;,&quot;A&quot;,&quot;N&quot;], alias=&quot;AutoETS&quot;, season_length=12)], freq = &#39;ME&#39;) air_model = air_holt.fit(df = air_train) result = air_holt.fitted_[0,0].model_[&#39;par&#39;] result ## array([5.69871723e-01, 1.00000000e-04, nan, nan, ## 3.48788866e+04, 1.29335525e+02]) y_hat1=air_model.predict(h=27) ## C:\\PROGRA~3\\ANACON~1\\Lib\\site-packages\\statsforecast\\core.py:492: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning. ## warnings.warn( yhat=y_hat1.reset_index(drop=True) forecast=pd.Series(pd.date_range(&quot;2006-01-01&quot;, freq=&quot;ME&quot;, periods=27)) forecast=pd.DataFrame(forecast) forecast.columns=[&quot;ds&quot;] forecast[&quot;hat&quot;]=yhat[&#39;AutoETS&#39;] forecast[&quot;unique_id&quot;]=&quot;1&quot; sns.lineplot(air_train,x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Train&quot;) sns.lineplot(air_test, x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Test&quot;) sns.lineplot(forecast,x=&quot;ds&quot;, y=&quot;hat&quot;, label=&quot;Forecast&quot;,) plt.show() ### Damped trend air_holt = StatsForecast(models = [AutoETS(model=[&quot;A&quot;,&quot;A&quot;,&quot;N&quot;], damped=True, alias=&quot;AutoETS&quot;, season_length=12)], freq = &#39;ME&#39;) air_model = air_holt.fit(df = air_train) result = air_holt.fitted_[0,0].model_[&#39;par&#39;] result ## array([5.71199778e-01, 1.00000000e-04, nan, 9.79993121e-01, ## 3.46022280e+04, 3.16699140e+02]) y_hat1=air_model.predict(h=27) ## C:\\PROGRA~3\\ANACON~1\\Lib\\site-packages\\statsforecast\\core.py:492: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning. ## warnings.warn( yhat=y_hat1.reset_index(drop=True) forecast=pd.Series(pd.date_range(&quot;2006-01-01&quot;, freq=&quot;ME&quot;, periods=27)) forecast=pd.DataFrame(forecast) forecast.columns=[&quot;ds&quot;] forecast[&quot;hat&quot;]=yhat[&#39;AutoETS&#39;] forecast[&quot;unique_id&quot;]=&quot;1&quot; sns.lineplot(air_train,x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Train&quot;) sns.lineplot(air_test, x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Test&quot;) sns.lineplot(forecast,x=&quot;ds&quot;, y=&quot;hat&quot;, label=&quot;Forecast&quot;,) plt.show() Seasonal models in Python: ### Using Statsforecast ### Additive Seasonality air_holtw = StatsForecast(models = [AutoETS(model=[&quot;A&quot;,&quot;A&quot;,&quot;A&quot;], alias=&quot;AutoETS&quot;, season_length=12)], freq = &#39;ME&#39;) air_model = air_holtw.fit(df = air_train) result = air_holtw.fitted_[0,0].model_[&#39;par&#39;] result ## array([ 4.81222943e-01, 1.59916114e-02, 2.42485779e-01, 8.90792825e-01, ## 3.77019540e+04, 5.67094839e+02, -9.99248746e+02, -2.53068050e+03, ## 1.17586756e+03, -2.77023876e+03, 5.79529172e+03, 4.08571477e+03, ## 2.44858026e+03, -1.89481694e+02, -5.21476747e+02, 1.96822431e+03, ## -4.80880072e+03, -3.65375146e+03]) y_hat1=air_model.predict(h=27) ## C:\\PROGRA~3\\ANACON~1\\Lib\\site-packages\\statsforecast\\core.py:492: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning. ## warnings.warn( yhat=y_hat1.reset_index(drop=True) forecast=pd.Series(pd.date_range(&quot;2006-01-01&quot;, freq=&quot;ME&quot;, periods=27)) forecast=pd.DataFrame(forecast) forecast.columns=[&quot;ds&quot;] forecast[&quot;hat&quot;]=yhat[&#39;AutoETS&#39;] forecast[&quot;unique_id&quot;]=&quot;1&quot; sns.lineplot(air_train,x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Train&quot;) sns.lineplot(air_test, x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Test&quot;) sns.lineplot(forecast,x=&quot;ds&quot;, y=&quot;hat&quot;, label=&quot;Forecast&quot;,) plt.show() ### Multiplicative Seasonality air_holtw = StatsForecast(models = [AutoETS(model=[&quot;M&quot;,&quot;A&quot;,&quot;M&quot;], alias=&quot;AutoETS&quot;, season_length=12)], freq = &#39;ME&#39;) air_model = air_holtw.fit(df = air_train) result = air_holtw.fitted_[0,0].model_[&#39;par&#39;] result ## array([9.45573068e-02, 2.64219583e-03, 2.54391669e-02, nan, ## 3.72394856e+04, 1.57545432e+02, 9.69302717e-01, 9.46292042e-01, ## 9.96735467e-01, 9.23896361e-01, 1.13023141e+00, 1.12016781e+00, ## 1.07916358e+00, 1.02341081e+00, 1.00200872e+00, 1.04798340e+00, ## 8.69839578e-01, 8.90968112e-01]) y_hat1=air_model.predict(h=27) ## C:\\PROGRA~3\\ANACON~1\\Lib\\site-packages\\statsforecast\\core.py:492: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning. ## warnings.warn( yhat=y_hat1.reset_index(drop=True) forecast=pd.Series(pd.date_range(&quot;2006-01-01&quot;, freq=&quot;ME&quot;, periods=27)) forecast=pd.DataFrame(forecast) forecast.columns=[&quot;ds&quot;] forecast[&quot;hat&quot;]=yhat[&#39;AutoETS&#39;] forecast[&quot;unique_id&quot;]=&quot;1&quot; sns.lineplot(air_train,x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Train&quot;) sns.lineplot(air_test, x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Test&quot;) sns.lineplot(forecast,x=&quot;ds&quot;, y=&quot;hat&quot;, label=&quot;Forecast&quot;,) plt.show() ### AutoETS air_holtw = StatsForecast(models = [AutoETS(model=[&quot;Z&quot;,&quot;Z&quot;,&quot;Z&quot;], alias=&quot;AutoETS&quot;, season_length=12)], freq = &#39;ME&#39;) air_model = air_holtw.fit(df = air_train) result = air_holtw.fitted_[0,0].model_[&#39;par&#39;] result ## array([ 3.53928248e-01, nan, 2.91330703e-01, nan, ## 3.88348407e+04, -8.30637153e+02, -2.79366612e+03, 3.48960549e+02, ## -1.64504368e+03, 6.41938336e+03, 5.16183884e+03, 2.80318982e+03, ## 1.24549104e+02, -1.04962061e+03, 9.50906067e+02, -5.57370399e+03, ## -3.91615619e+03]) y_hat1=air_model.predict(h=27) ## C:\\PROGRA~3\\ANACON~1\\Lib\\site-packages\\statsforecast\\core.py:492: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning. ## warnings.warn( yhat=y_hat1.reset_index(drop=True) forecast=pd.Series(pd.date_range(&quot;2006-01-01&quot;, freq=&quot;ME&quot;, periods=27)) forecast=pd.DataFrame(forecast) forecast.columns=[&quot;ds&quot;] forecast[&quot;hat&quot;]=yhat[&#39;AutoETS&#39;] forecast[&quot;unique_id&quot;]=&quot;1&quot; sns.lineplot(air_train,x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Train&quot;) sns.lineplot(air_test, x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Test&quot;) sns.lineplot(forecast,x=&quot;ds&quot;, y=&quot;hat&quot;, label=&quot;Forecast&quot;,) plt.show() ### Comparing models: air_holtw = StatsForecast(models = [AutoETS(model=[&quot;A&quot;,&quot;A&quot;,&quot;A&quot;], alias = &quot;Additive&quot;, season_length=12), AutoETS(model=[&quot;M&quot;,&quot;A&quot;,&quot;M&quot;], alias = &quot;Multiplicative&quot;, season_length=12)], freq = &#39;ME&#39;) air_model = air_holtw.fit(df = air_train) y_hat1=air_model.predict(h=27) ## C:\\PROGRA~3\\ANACON~1\\Lib\\site-packages\\statsforecast\\core.py:492: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning. ## warnings.warn( y_hat1 ## ds Additive Multiplicative ## unique_id ## 1 2005-12-31 55463.046875 55276.296875 ## 1 2006-01-31 54885.386719 54177.058594 ## 1 2006-02-28 65846.859375 65606.875000 ## 1 2006-03-31 62177.195312 62759.925781 ## 1 2006-04-30 63248.167969 64265.078125 ## 1 2006-05-31 66142.429688 67983.453125 ## 1 2006-06-30 68683.671875 71003.070312 ## 1 2006-07-31 66152.406250 71539.562500 ## 1 2006-08-31 54251.707031 58415.742188 ## 1 2006-09-30 59907.390625 63461.949219 ## 1 2006-10-31 58180.179688 60389.761719 ## 1 2006-11-30 59900.917969 62076.480469 ## 1 2006-12-31 55417.457031 57164.699219 ## 1 2007-01-31 54844.781250 56022.656250 ## 1 2007-02-28 65810.687500 67835.507812 ## 1 2007-03-31 62144.972656 64885.832031 ## 1 2007-04-30 63219.460938 66435.843750 ## 1 2007-05-31 66116.859375 70273.367188 ## 1 2007-06-30 68660.890625 73388.007812 ## 1 2007-07-31 66132.117188 73935.812500 ## 1 2007-08-31 54233.632812 60366.957031 ## 1 2007-09-30 59891.289062 65575.835938 ## 1 2007-10-31 58165.839844 62395.746094 ## 1 2007-11-30 59888.144531 64132.800781 ## 1 2007-12-31 55406.078125 59053.101562 ## 1 2008-01-31 54834.640625 57868.250000 ## 1 2008-02-29 65801.656250 70064.148438 y_hat1=air_model.predict(h=27) ## C:\\PROGRA~3\\ANACON~1\\Lib\\site-packages\\statsforecast\\core.py:492: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning. ## warnings.warn( yhat=y_hat1.reset_index(drop=True) forecast=pd.Series(pd.date_range(&quot;2006-01-01&quot;, freq=&quot;ME&quot;, periods=27)) forecast=pd.DataFrame(forecast) forecast.columns=[&quot;ds&quot;] forecast[&quot;hat1&quot;]=yhat[&#39;Additive&#39;] forecast[&quot;hat2&quot;]=yhat[&#39;Multiplicative&#39;] forecast[&quot;unique_id&quot;]=&quot;1&quot; sns.lineplot(air_train,x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Train&quot;) sns.lineplot(air_test, x=&quot;ds&quot;, y=&quot;y&quot;, label=&quot;Test&quot;) sns.lineplot(forecast,x=&quot;ds&quot;, y=&quot;hat1&quot;, label=&quot;Additive&quot;) sns.lineplot(forecast,x=&quot;ds&quot;, y=&quot;hat2&quot;, label=&quot;Multiplicative&quot;) plt.show() 3.7 SAS Code for Exponential Smoothing Models The following code is for Exponential Smoothing models in SAS. Create library for data sets libname Time ‘Q:Drive - Time Series’; run; SIMPLE EXPONENTIAL SMOOTHING MODEL Create a simple exponential smoothing model proc esm data=Time.Steel print=all plot=all lead=24; forecast steelshp / model=simple; run; Create a simple exponential smoothing model with ID statement   proc esm data=Time.USAirlines print=all plot=all lead=24;   id date interval=month;   forecast Passengers / model=simple;   run;   LINEAR TREND FOR EXPONENTIAL SMOOTHING   Double exponential smoothing   proc esm data=Time.Steel print=all plot=all lead=24;   forecast steelshp / model=double;   run;   linear exponential smoothing   proc esm data=Time.Steel print=all plot=all lead=24;   forecast steelshp / model=linear;   run;   damped trend exponential smoothing   proc esm data=Time.Steel print=all plot=all lead=24;   forecast steelshp / model=damptrend;   run;   linear exponential smoothign with interval = month   proc esm data=Time.USAirlines print=all plot=all lead=24;   id date interval=month;   forecast Passengers / model=linear;   run;   SEASONAL EXPONENTIAL SMOOTHING MODEL   Additive seasonal exponential smoothing model   proc esm data=Time.USAirlines print=all plot=all   seasonality=12 lead=24 outfor=test1;   forecast Passengers / model=addseasonal;   run;   mulitplicative seasonal exponential smoothing model   proc esm data=Time.USAirlines print=all plot=all   seasonality=12 lead=24;   forecast Passengers / model=multseasonal;   run;   Winters additive exponential smoothing model (includes trend)   proc esm data=Time.USAirlines print=all plot=all   seasonality=12 lead=24;   forecast Passengers / model=addwinters;   run;   Winters multiplicative exponential smoothing model (includes trend) (Lead = 24)   proc esm data=Time.USAirlines print=all plot=all   seasonality=12 lead=24;   forecast Passengers / model=multwinters;   run;   Winters multiplicative exponential smoothing model (includes trend) Lead = 12   proc esm data=Time.USAirlines print=all plot=all lead=12   back=12 seasonality=12;   forecast Passengers / model=multwinters;   run;   EXPLORATION of SEASONAL EXPONENTIAL SMOOTHING MODEL   Winters multiplicative exponential smoothing model (includes trend) Lead = 12, uses outfor statement to output forecasts   proc esm data=Time.USAirlines print=all plot=all seasonality=12 lead=12 back=12 outfor=test;   forecast Passengers / model=multwinters;   run;   calculate |error|/|actual value|   data test2;   set test;   if TIMEID&gt;207;   abs_error=abs(error);   abs_err_obs=abs_error/abs(actual);   run;   mean of |error|/|actual value| for this forecast   proc means data=test2 mean;   var abs_error abs_err_obs;   run; "],["arima.html", "Chapter 4 ARIMA 4.1 Stationarity 4.2 Correlation Functions 4.3 AutoRegressive Models (AR) 4.4 Moving Average model (MA) 4.5 White noise 4.6 Examples 4.7 Forecasting 4.8 Trend 4.9 Python Code for ARMA/ARIMA models 4.10 SAS Code for ARMA/ARIMA", " Chapter 4 ARIMA We will not be getting into ARIMA. This chapter is fairly long and covers many different concepts in ARIMA. 4.1 Stationarity Before we can try to model the dependency structure (the AR and MA terms), we must first have a stationary series! The ADF test is one of the most well-known and accepted test for testing stationarity. However, others have also been proposed. Within this document, we will be using the KPSS test. Quotes.ts&lt;-Quotes |&gt; mutate(date = seq(ymd(&#39;2002-01-01&#39;),ymd(&#39;2005-04-01&#39;),by=&#39;months&#39;)) |&gt; mutate(month=yearmonth(date)) |&gt; as_tsibble(index=month) Quotes_train&lt;-Quotes.ts %&gt;% filter(year(date)&lt;2005) autoplot(Quotes_train,Quotes)+labs(title=&quot;Time Series of Monthly Stock quotes&quot;, x=&quot;Time&quot;, y=&quot;Quotes&quot;) The following code looks into stationarity. # Perform the KPSS test Quotes_train |&gt; features(Quotes, unitroot_kpss) ## # A tibble: 1 × 2 ## kpss_stat kpss_pvalue ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.197 0.1 Quotes_train |&gt; features(Quotes, unitroot_ndiffs) ## # A tibble: 1 × 1 ## ndiffs ## &lt;int&gt; ## 1 0 4.2 Correlation Functions The Acf and the Pacf in R will calculate the autocorrelation (up to the lag you specify) and the partial autocorrelation, respectively. ggAcf(Quotes_train$Quotes,lag=10) ggPacf(Quotes_train$Quotes,lag=10) Since the Hurricane data set needs a difference to be stationary, we will first create the difference column and explore the correlations in that variable. Hurricane.ts&lt;- hurricane %&gt;% as_tsibble(index=Year) Hurricane_train &lt;-Hurricane.ts %&gt;% filter(Year &lt;2000) autoplot(Hurricane_train,MeanVMax)+labs(title=&quot;Time Series of Yearly Mean Velocity for Hurricanes&quot;, x=&quot;Time&quot;, y=&quot;MPH&quot;) Hurricane_train %&gt;% features(MeanVMax,unitroot_ndiffs) ## # A tibble: 1 × 1 ## ndiffs ## &lt;int&gt; ## 1 1 Hurricane_train &lt;- Hurricane_train %&gt;% mutate(mean_diff=difference(MeanVMax)) Hurricane_train %&gt;% features(mean_diff,unitroot_ndiffs) ## # A tibble: 1 × 1 ## ndiffs ## &lt;int&gt; ## 1 0 autoplot(Hurricane_train,mean_diff)+labs(title=&quot;Differenced Mean Max Velocity&quot;, x=&quot;Time&quot;, y=&quot;Difference&quot;) ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_line()`). 4.3 AutoRegressive Models (AR) AutoRegressive (AR) models involve modeling the lags of Y. We can write an autoregressive model as \\[ Y_{t} = c + \\phi_{1}Y_{t-1}+\\phi_{2}Y_{t-2}+...\\phi_{p}Y_{t-p}+\\epsilon_{t} \\] Where there are p lags of Y. Below is the code to fit an AR(2) model. The order in the Arima function needs the p,d,q values (p=# of AR terms, d=how many differences should be taken and q=# of MA terms). ggAcf(Y[1:731,]) ggPacf(Y[1:731,]) Y.1 &lt;-data.frame(Y) Y.ts&lt;-Y.1 %&gt;% mutate(date = seq(ymd(&#39;2000-01-01&#39;),ymd(&#39;2002-09-26&#39;),by=&#39;day&#39;)) %&gt;% as_tsibble(index=date) Y_train &lt;- Y.ts %&gt;% filter(year(date)&lt;2002) autoplot(Y_train,Y)+labs(title=&quot;Time Series of Simulated Daily series&quot;, x=&quot;Time&quot;, y=&quot;Values&quot;) Y.ARIMA &lt;- Y_train %&gt;% model(ARIMA(Y~pdq(2,0,0)+PDQ(0,0,0))) report(Y.ARIMA) ## Series: Y ## Model: ARIMA(2,0,0) ## ## Coefficients: ## ar1 ar2 ## 0.6399 -0.3838 ## s.e. 0.0342 0.0342 ## ## sigma^2 estimated as 93.75: log likelihood=-2696.14 ## AIC=5398.28 AICc=5398.32 BIC=5412.07 Y.ARIMA %&gt;% residuals() %&gt;% ggAcf() Y.ARIMA %&gt;% residuals() %&gt;% ggPacf() 4.4 Moving Average model (MA) Moving average (MA) models involve modeling the lags of the error. We can write a moving average model as \\[ Y_{t} = c - \\theta_{1}\\epsilon_{t-1}-\\theta_{2}\\epsilon_{t-2}-...\\theta_{q}\\epsilon_{t-q}+\\epsilon_{t} \\] Where there are q lags of \\(\\epsilon\\). Below is code to fit an MA(2) model. ggAcf(x[1:74,]) ggPacf(x[1:74,]) x.1 &lt;-data.frame(x) x.ts&lt;-x.1 %&gt;% mutate(date = seq(ymd(&#39;2000-01-01&#39;),ymd(&#39;2000-4-9&#39;),by=&#39;day&#39;)) %&gt;% as_tsibble(index=date) x_train &lt;- x.ts %&gt;% filter(date &lt; &#39;2000-3-15&#39;) autoplot(x_train,x)+labs(title=&quot;Time Series of Simulated Daily series&quot;, x=&quot;Time&quot;, y=&quot;Values&quot;) x.ARIMA &lt;- x_train %&gt;% model(ARIMA(x~pdq(0,0,2)+PDQ(0,0,0))) report(x.ARIMA) ## Series: x ## Model: ARIMA(0,0,2) ## ## Coefficients: ## ma1 ma2 ## -0.2585 0.4874 ## s.e. 0.1031 0.1063 ## ## sigma^2 estimated as 0.2299: log likelihood=-49.88 ## AIC=105.77 AICc=106.11 BIC=112.68 x.ARIMA %&gt;% residuals() %&gt;% ggAcf() x.ARIMA %&gt;% residuals() %&gt;% ggPacf() 4.5 White noise For residuals to exhibit white noise, they must be “independent” and normally distributed with mean 0 and constant variance. You already know how to assess normality and constant variance, however, we need to focus on assessing “independence”. We can assess if there is significant dependence through the Ljung-Box test (or graphically through ACF and PACF plots). The hypotheses being tested are \\[H_{0}:No\\quad significant\\quad autocorrelation\\\\ H_{A}:Significant\\qquad autocorrletion \\] This should be assessed on a stationary time series. Looking at a stationary time series, going back 10 lags should be sufficient (this will be different when we get to seasonal models). Keep in mind that sample size does matter when assessing significance (adjust significance level accordingly). ### Before fitting model: ljung_box(Y[1:731,], lag = 10, dof=0) ## lb_stat lb_pvalue ## 217.3408 0.0000 ## Note: Y is a vector ### After fitting model: augment(Y.ARIMA) %&gt;% features(.innov,ljung_box, lag=10, dof = 2) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ARIMA(Y ~ pdq(2, 0, 0) + PDQ(0, 0, 0)) 8.50 0.386 ## Note that dof has changed!! 4.6 Examples We will now demonstrate these ideas on two different examples: First example is the Quotes data set: Quotes.ts&lt;-Quotes |&gt; mutate(date = seq(ymd(&#39;2002-01-01&#39;),ymd(&#39;2005-04-21&#39;),by=&#39;months&#39;)) |&gt; mutate(month=yearmonth(date)) |&gt; as_tsibble(index=month) Quotes_train&lt;-Quotes.ts %&gt;% filter(year(date)&lt;2005) autoplot(Quotes_train,Quotes)+labs(title=&quot;Time Series of Monthly Stock quotes&quot;, x=&quot;Time&quot;, y=&quot;Quotes&quot;) Quotes_train %&gt;% gg_tsdisplay(Quotes,plot_type = &#39;partial&#39;) We will try AR(1), MA(1) and perform two automatic searches: quotes_model &lt;-Quotes_train %&gt;% model(ar1 = ARIMA(Quotes ~ pdq(1,0,0) + PDQ(0,0,0)), ma1 = ARIMA(Quotes ~ pdq(0,0,1) + PDQ(0,0,0)), search1 = ARIMA(Quotes), search2 = ARIMA(Quotes,stepwise = F)) quotes_model2&lt;-as.data.frame(quotes_model) t(quotes_model2) ## [,1] ## ar1 ARIMA(1,0,0) w/ mean ## ma1 ARIMA(0,0,1) w/ mean ## search1 ARIMA(1,0,1) w/ mean ## search2 ARIMA(1,0,1) w/ mean glance(quotes_model) %&gt;% arrange(AICc) %&gt;% select(.model:BIC) ## # A tibble: 4 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 search1 2.54 -66.8 142. 143. 148. ## 2 search2 2.54 -66.8 142. 143. 148. ## 3 ma1 2.75 -68.7 143. 144. 148. ## 4 ar1 2.79 -68.8 144. 144. 148. Choosing the search1 model, we will look at residuals for white noise (no spikes in correlation plots and not significant for the Ljung-Box test): quotes_model %&gt;% select(search1) %&gt;% residuals() %&gt;% ggAcf() quotes_model %&gt;% select(search1) %&gt;% residuals() %&gt;% ggPacf() quotes_model %&gt;% select(search1) %&gt;% gg_tsresiduals() augment(quotes_model) %&gt;% filter(.model==&#39;search1&#39;) %&gt;% features(.innov,ljung_box, lag=10, dof = 2) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 search1 3.60 0.892 Second example is the Hurricane data set (mean Maximum velocity): Hurricane.ts&lt;- hurricane %&gt;% as_tsibble(index=Year) Hurricane_train &lt;-Hurricane.ts %&gt;% filter(Year &lt;2000) autoplot(Hurricane_train,MeanVMax)+labs(title=&quot;Time Series of Yearly Mean Velocity for Hurricanes&quot;, x=&quot;Time&quot;, y=&quot;MPH&quot;) Hurricane_train %&gt;% features(MeanVMax,unitroot_ndiffs) ## # A tibble: 1 × 1 ## ndiffs ## &lt;int&gt; ## 1 1 Hurricane_train &lt;- Hurricane_train %&gt;% mutate(mean_diff=difference(MeanVMax)) Hurricane_train %&gt;% gg_tsdisplay(mean_diff,plot_type = &#39;partial&#39;) ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_line()`). ## Warning: Removed 5 rows containing missing values or values outside the scale range (`geom_point()`). hurr_model &lt;-Hurricane_train %&gt;% model(ar3 = ARIMA(MeanVMax ~ 0 + pdq(3,1,0) + PDQ(0,0,0)), ma2 = ARIMA(MeanVMax ~ 0 + pdq(0,1,2) + PDQ(0,0,0)), arima32 = ARIMA(MeanVMax~0 + pdq(3,1,2) + PDQ(0,0,0)), search1 = ARIMA(MeanVMax), search2 = ARIMA(MeanVMax,stepwise = F)) hurr_model2&lt;-as.data.frame(hurr_model) t(hurr_model2) ## [,1] ## ar3 ARIMA(3,1,0) ## ma2 ARIMA(0,1,2) ## arima32 ARIMA(3,1,2) ## search1 ARIMA(1,0,1) w/ mean ## search2 ARIMA(2,0,3) w/ mean glance(hurr_model) %&gt;% arrange(AICc) %&gt;% select(.model:BIC) ## # A tibble: 5 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 arima32 94.6 -539. 1090. 1090. 1108. ## 2 ma2 97.4 -542. 1091. 1091. 1100. ## 3 search2 93.1 -540. 1094. 1095. 1115. ## 4 search1 96.3 -544. 1096. 1096. 1108. ## 5 ar3 108. -549. 1106. 1106. 1118. Looking at the ACF and PACF on the residuals of MA(2) model. hurr_model %&gt;% select(ma2) %&gt;% residuals() %&gt;% ggAcf() hurr_model %&gt;% select(ma2) %&gt;% residuals() %&gt;% ggPacf() hurr_model %&gt;% select(ma2) %&gt;% gg_tsresiduals() ## Warning: Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). ## Warning: Removed 2 rows containing non-finite outside the scale range (`stat_bin()`). Looking at the ACF and PACF on the ARIMA(3,1,2) model and Ljung-Box test. hurr_model %&gt;% select(arima32) %&gt;% residuals() %&gt;% ggAcf() hurr_model %&gt;% select(arima32) %&gt;% residuals() %&gt;% ggPacf() hurr_model %&gt;% select(arima32) %&gt;% gg_tsresiduals() ## Warning: Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). ## Warning: Removed 2 rows containing non-finite outside the scale range (`stat_bin()`). augment(hurr_model) %&gt;% filter(.model==&#39;arima32&#39;) %&gt;% features(.innov,ljung_box, lag=10, dof = 5) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 arima32 5.15 0.398 4.7 Forecasting Now let’s forecast each of our models: quotes_model %&gt;% select(search1) %&gt;% fabletools::forecast(h=4) %&gt;% autoplot(Quotes_train) quotes_for&lt;-quotes_model %&gt;% select(search1) %&gt;% fabletools::forecast(h=4) quotes_resid&lt;-Quotes$Quotes[37:40]-quotes_for$.mean MAPE&lt;-mean(abs(quotes_resid/Quotes$Quotes[37:40])) MAE&lt;-mean(abs(quotes_resid)) MAPE ## [1] 0.2330841 MAE ## [1] 3.952702 hurr_model %&gt;% select(arima32) %&gt;% fabletools::forecast(h=8) %&gt;% autoplot(Hurricane_train) hurr_for&lt;-hurr_model %&gt;% select(arima32) %&gt;% fabletools::forecast(h=8) hurr_resid&lt;-hurricane$MeanVMax[150:157]-hurr_for$.mean MAPE&lt;-mean(abs(hurr_resid/hurricane$MeanVMax[150:157])) MAE&lt;-mean(abs(hurr_resid)) MAPE ## [1] 0.06067555 MAE ## [1] 5.986266 4.8 Trend We will now take a look at trending time series. We will use the Consume and Raleigh Housing prices index as two examples. Consumer example using differences for trend: consume.ts&lt;- consume |&gt; mutate(date2=my(date))|&gt; mutate(month=yearmonth(date2)) |&gt; as_tsibble(index=month) consume_train&lt;-consume.ts %&gt;% filter(year(date2)&lt;1990) autoplot(consume_train,Disposable_income)+labs(title=&quot;Time Series of Monthly Disposable Income&quot;, x=&quot;Time&quot;, y=&quot;Thousands of Dollars&quot;) ndiffs(consume_train$Disposable_income) ## [1] 1 consume_train&lt;- consume_train %&gt;% mutate(income_diff = difference(Disposable_income)) autoplot(consume_train,income_diff)+labs(title=&quot;Time Series of Differenced Monthly Disposable Income&quot;, x=&quot;Time&quot;, y=&quot;Differences&quot;) ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_line()`). consume_train %&gt;% gg_tsdisplay(income_diff,plot_type = &#39;partial&#39;) ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_line()`). ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_point()`). Looking at the ACF and PACF, we will try a few models and also use an automatic search. consume_model &lt;-consume_train %&gt;% model(ar1 = ARIMA(Disposable_income ~ pdq(1,1,0) + PDQ(0,0,0)), ma1 = ARIMA(Disposable_income ~ pdq(0,1,1) + PDQ(0,0,0)), ar6 = ARIMA(Disposable_income ~ pdq(6,1,0) + PDQ(0,0,0)), ma6 = ARIMA(Disposable_income ~ pdq(0,1,6) + PDQ(0,0,0)), search1 = ARIMA(Disposable_income), search2 = ARIMA(Disposable_income,stepwise = F)) consume_model2&lt;-as.data.frame(consume_model) t(consume_model2) ## [,1] ## ar1 ARIMA(1,1,0) w/ drift ## ma1 ARIMA(0,1,1) w/ drift ## ar6 ARIMA(6,1,0) w/ drift ## ma6 ARIMA(0,1,6) ## search1 ARIMA(0,1,1) w/ drift ## search2 ARIMA(0,1,1) w/ drift glance(consume_model) %&gt;% arrange(AICc) %&gt;% select(.model:BIC) ## # A tibble: 6 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ma1 546. -397. 799. 800. 807. ## 2 search1 546. -397. 799. 800. 807. ## 3 search2 546. -397. 799. 800. 807. ## 4 ar6 520. -392. 800. 802. 820. ## 5 ar1 571. -399. 803. 803. 811. ## 6 ma6 610. -400. 814. 815. 831. Selecting the ARIMA(6,1,0) model. White noise looks good and forecast is good. consume_model %&gt;% select(ar6) %&gt;% residuals() %&gt;% ggAcf(lag.max = 10) consume_model %&gt;% select(ar6) %&gt;% residuals() %&gt;% ggPacf(lag.max = 10) consume_model %&gt;% select(ar6) %&gt;% gg_tsresiduals() augment(consume_model) %&gt;% filter(.model==&#39;ar6&#39;) %&gt;% features(.innov,ljung_box, lag=10, dof = 6) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ar6 3.21 0.523 pred_ar6 &lt;- consume_model %&gt;% select(ar6) %&gt;% fabletools::forecast(h=6) error_ar6 &lt;- consume$Disposable_income[89:94] - pred_ar6$.mean MAPE_ar6 &lt;-mean(abs(error_ar6/consume$Disposable_income[89:94])) MAE_ar6 &lt;- mean(abs(error_ar6)) consume_model %&gt;% select(ar6) %&gt;% fabletools::forecast(h=6) %&gt;% autoplot(consume_train) pred_ma1 &lt;- consume_model %&gt;% select(ma1) %&gt;% fabletools::forecast(h=6) error_ma1 &lt;- consume$Disposable_income[89:94] - pred_ma1$.mean MAPE_ma1 &lt;-mean(abs(error_ma1/consume$Disposable_income[89:94])) MAE_ma1 &lt;- mean(abs(error_ma1)) consume_model %&gt;% select(ma1) %&gt;% fabletools::forecast(h=6) %&gt;% autoplot(consume_train) Raleigh example using differences: Raleigh.ts&lt;- Raleigh %&gt;% mutate(quarter=yearquarter(DATE)) %&gt;% as_tsibble(index=quarter) Raleigh_train &lt;-Raleigh.ts %&gt;% filter(quarter &lt;yearquarter(&quot;2023 Q1&quot;)) autoplot(Raleigh_train,price_index)+labs(title=&quot;Time Series of Quarterly Housing price Index for Raleigh-Cary&quot;, x=&quot;Time&quot;, y=&quot;Index&quot;) Raleigh_train %&gt;% features(price_index,unitroot_ndiffs) ## # A tibble: 1 × 1 ## ndiffs ## &lt;int&gt; ## 1 2 Raleigh_train &lt;- Raleigh_train %&gt;% mutate(diff_price=difference(difference(price_index))) Raleigh_train %&gt;% gg_tsdisplay(diff_price,plot_type = &#39;partial&#39;) ## Warning: Removed 2 rows containing missing values or values outside the scale range (`geom_line()`). ## Warning: Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). Now we will try a few models: Raleigh_model &lt;-Raleigh_train %&gt;% model( ma5 = ARIMA(price_index ~ pdq(0,2,5)+ PDQ(0,0,0)+0), ar2 = ARIMA(price_index ~ pdq(2,2,0)+ PDQ(0,0,0)+0), ma2 = ARIMA(price_index ~ pdq(0,2,2)+ PDQ(0,0,0)+0), search1 = ARIMA(price_index~PDQ(0,0,0)), search2 = ARIMA(price_index,stepwise = FALSE) ) Raleigh_model2&lt;-as.data.frame(Raleigh_model) t(Raleigh_model2) ## [,1] ## ma5 ARIMA(0,2,5) ## ar2 ARIMA(2,2,0) ## ma2 ARIMA(0,2,2) ## search1 ARIMA(1,2,3) ## search2 ARIMA(0,2,5) glance(Raleigh_model) %&gt;% arrange(AICc) %&gt;% select(.model:BIC) ## # A tibble: 5 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ma5 7.65 -219. 451. 452. 466. ## 2 search2 7.65 -219. 451. 452. 466. ## 3 search1 8.36 -223. 455. 456. 468. ## 4 ar2 9.50 -229. 464. 464. 471. ## 5 ma2 10.1 -231. 468. 469. 476. Looking at residuals: Raleigh_model %&gt;% select(search1) %&gt;% residuals() %&gt;% ggAcf(lag.max = 10) Raleigh_model %&gt;% select(search1) %&gt;% residuals() %&gt;% ggPacf(lag.max = 10) Raleigh_model %&gt;% select(search1) %&gt;% gg_tsresiduals() augment(Raleigh_model) %&gt;% filter(.model==&#39;search1&#39;) %&gt;% features(.innov,ljung_box, lag=10, dof = 4) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 search1 7.76 0.257 Forecasting chosen model onto the validatoin data set: pred_arima123 &lt;- Raleigh_model %&gt;% select(search1) %&gt;% fabletools::forecast(h=5) error_arima123 &lt;- Raleigh.ts$price_index[93:97] - pred_arima123$.mean MAPE_arima123 &lt;-mean(abs(error_arima123/Raleigh.ts$price_index[93:97])) MAE_arima123 &lt;- mean(abs(error_arima123)) Raleigh_model %&gt;% select(search1) %&gt;% fabletools::forecast(h=5) %&gt;% autoplot(Raleigh_train) pred_data &lt;- tibble( quarter = yearquarter(seq.Date(from = as.Date(&quot;2023-01-01&quot;), to = as.Date(&quot;2024-01-01&quot;), by = &quot;quarter&quot;)), value = pred_arima123$.mean ) test_data &lt;- tibble( quarter = yearquarter(seq.Date(from = as.Date(&quot;2023-01-01&quot;), to = as.Date(&quot;2024-01-01&quot;), by = &quot;quarter&quot;)), value = Raleigh.ts$price_index[93:97] ) ggplot() + geom_line(data = test_data, aes(x = quarter, y = value), color = &quot;blue&quot;, linetype = &quot;solid&quot;) + geom_line(data = pred_data, aes(x = quarter, y = value), color = &quot;orange&quot;, linetype = &quot;dashed&quot;) + labs(title = &quot;Predicted versus Actual values&quot;, x = &quot;Quarter&quot;, y = &quot;Price Index&quot;) + theme_minimal() Using the consumer data set and fitting a linear trend line: consume_linear &lt;-consume_train %&gt;% model(trend1 = ARIMA(Disposable_income~ trend() + pdq(0,0,0) + PDQ(0,0,0)+1) ) report(consume_linear) ## Series: Disposable_income ## Model: LM w/ ARIMA(0,0,0) errors ## ## Coefficients: ## trend() intercept ## 7.8743 2368.4594 ## s.e. 0.1168 5.9841 ## ## sigma^2 estimated as 792.5: log likelihood=-417.56 ## AIC=841.12 AICc=841.41 BIC=848.56 fitted_values &lt;- fitted(consume_linear) # Plot the original data and fitted values autoplot(consume_train, Disposable_income) + autolayer(fitted_values, .fitted, color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + labs(title = &quot;Fitted Values from Linear Regression Model for Disposable Income&quot;, x = &quot;Time&quot;, y = &quot;Dollars (000)&quot;) + theme_minimal() consume_linear %&gt;% residuals() %&gt;% ggAcf(lag.max = 12) consume_linear %&gt;% residuals() %&gt;% ggPacf(lag.max = 12) consume_linear &lt;-consume_train %&gt;% model(trend1 = ARIMA(Disposable_income~ trend() + pdq(6,0,0) + PDQ(0,0,0)+1), trend2 = ARIMA(Disposable_income ~ trend() + PDQ(0,0,0) +1) ) consume_linear2&lt;-as.data.frame(consume_linear) t(consume_linear2) ## [,1] ## trend1 LM w/ ARIMA(6,0,0) errors ## trend2 LM w/ ARIMA(1,0,0) errors glance(consume_linear) %&gt;% arrange(AICc) %&gt;% select(.model:BIC) ## # A tibble: 2 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 trend2 504. -397. 803. 803. 813. ## 2 trend1 486. -393. 805. 807. 827. Comparing the random walk with drift to the linear trend time series model for the consumer data set: consume_linear %&gt;% select(trend1) %&gt;% residuals() %&gt;% ggAcf(lag.max = 10) consume_linear %&gt;% select(trend1) %&gt;% residuals() %&gt;% ggPacf(lag.max = 10) consume_linear %&gt;% select(trend1) %&gt;% gg_tsresiduals() augment(consume_linear) %&gt;% filter(.model==&#39;trend1&#39;) %&gt;% features(.innov,ljung_box, lag=10, dof = 6) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 trend1 3.20 0.524 pred_lm &lt;- consume_linear %&gt;% select(trend1) %&gt;% fabletools::forecast(h=6) consume_linear %&gt;% select(trend1) %&gt;% fabletools::forecast(h=6) %&gt;% autoplot(consume_train) error_lm &lt;- consume$Disposable_income[89:94] - pred_lm$.mean MAPE_lm &lt;-mean(abs(error_lm/consume$Disposable_income[89:94])) MAE_lm &lt;- mean(abs(error_lm)) pred_rw &lt;- tibble( month = yearmonth(seq.Date(from = as.Date(&quot;1990-01-01&quot;), to = as.Date(&quot;1990-06-01&quot;), by = &quot;month&quot;)), value = pred_ar6$.mean ) pred_lm2 &lt;- tibble( month = yearmonth(seq.Date(from = as.Date(&quot;1990-01-01&quot;), to = as.Date(&quot;1990-06-01&quot;), by = &quot;month&quot;)), value = pred_lm$.mean ) test_data &lt;- tibble( month = yearmonth(seq.Date(from = as.Date(&quot;1990-01-01&quot;), to = as.Date(&quot;1990-06-01&quot;), by = &quot;month&quot;)), value = consume.ts$Disposable_income[89:94] ) combined_data &lt;- bind_rows( test_data %&gt;% mutate(Line = &quot;Actual&quot;), pred_rw %&gt;% mutate(Line = &quot;Random Walk&quot;), pred_lm2 %&gt;% mutate(Line = &quot;Linear Model&quot;) ) # Plot the data with a legend ggplot(combined_data, aes(x = month, y = value, color = Line)) + geom_line(linetype = &quot;solid&quot;) + labs(title = &quot;Predicted versus Actual values&quot;, x = &quot;Date&quot;, y = &quot;Disposable Income (000)&quot;, color = &quot;Legend&quot;) + theme_minimal() Fitting a trend line to the Raleigh data set: Raleigh_linear &lt;-Raleigh_train %&gt;% model(trend1 = ARIMA(price_index~ trend() + pdq(0,0,0) + PDQ(0,0,0)+1) ) Raleigh_linear %&gt;% residuals() %&gt;% ggAcf(lag.max = 10) Raleigh_linear %&gt;% residuals() %&gt;% ggPacf(lag.max = 10) Raleigh_linear &lt;-Raleigh_train %&gt;% model(trend1 = ARIMA(price_index~ trend() + pdq(2,0,0) + PDQ(0,0,0)+1), trend2 = ARIMA(price_index ~ trend() + PDQ(0,0,0) + 1), trend3 = ARIMA(price_index ~ trend() + PDQ(0,0,0) + 1,stepwise = FALSE) ) Raleigh_linear2&lt;-as.data.frame(Raleigh_linear) t(Raleigh_linear2) ## [,1] ## trend1 LM w/ ARIMA(2,0,0) errors ## trend2 LM w/ ARIMA(2,0,2) errors ## trend3 LM w/ ARIMA(1,0,4) errors glance(Raleigh_linear) %&gt;% arrange(AICc) %&gt;% select(.model:BIC) ## # A tibble: 3 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 trend3 7.45 -224. 465. 466. 485. ## 2 trend2 8.53 -230. 474. 475. 491. ## 3 trend1 10.5 -239. 488. 489. 501. Raleigh_linear %&gt;% select(trend2) %&gt;% residuals() %&gt;% ggAcf(lag.max = 10) Raleigh_linear %&gt;% select(trend2) %&gt;%residuals() %&gt;% ggPacf(lag.max = 10) augment(Raleigh_linear) %&gt;% filter(.model==&#39;trend2&#39;) %&gt;% features(.innov,ljung_box, lag=10, dof = 4) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 trend2 9.50 0.147 Comparing the Raleigh data set using a random walk with drift to the trend model: pred_lm &lt;- Raleigh_linear %&gt;% select(trend2) %&gt;% fabletools::forecast(h=5) error_lm &lt;- Raleigh.ts$price_index[93:97] - pred_lm$.mean MAPE_lm &lt;-mean(abs(error_lm/Raleigh.ts$price_index[93:97])) MAE_lm &lt;- mean(abs(error_lm)) pred_rw &lt;- tibble( quarter = yearquarter(seq.Date(from = as.Date(&quot;2023-01-01&quot;), to = as.Date(&quot;2024-01-01&quot;), by = &quot;quarter&quot;)), value = pred_arima123$.mean ) pred_lm2 &lt;- tibble( quarter = yearquarter(seq.Date(from = as.Date(&quot;2023-01-01&quot;), to = as.Date(&quot;2024-01-01&quot;), by = &quot;quarter&quot;)), value = pred_lm$.mean ) test_data &lt;- tibble( quarter = yearquarter(seq.Date(from = as.Date(&quot;2023-01-01&quot;), to = as.Date(&quot;2024-01-01&quot;), by = &quot;quarter&quot;)), value = Raleigh.ts$price_index[93:97] ) combined_data &lt;- bind_rows( test_data %&gt;% mutate(Line = &quot;Actual&quot;), pred_rw %&gt;% mutate(Line = &quot;Random Walk&quot;), pred_lm2 %&gt;% mutate(Line = &quot;Linear Model&quot;) ) # Plot the data with a legend ggplot(combined_data, aes(x = quarter, y = value, color = Line)) + geom_line(linetype = &quot;solid&quot;) + labs(title = &quot;Predicted versus Actual values&quot;, x = &quot;Date&quot;, y = &quot;Raleigh Price Index&quot;, color = &quot;Legend&quot;) + theme_minimal() #### Fitting a Quadratic ARIMAX ### First we need to create the x-variables ### I called them time and time2 (time is just a sequence from 1 to n and time2 is time squared) Raleigh2.ts &lt;- Raleigh.ts %&gt;% mutate(time = 1:n(), time2= time^2) ## Create training data set Raleigh_train2 &lt;-Raleigh2.ts %&gt;% filter(quarter &lt;yearquarter(&quot;2023 Q1&quot;)) ##Create linear model lm.raleigh&lt;-lm(price_index~time+time2,data=Raleigh_train2) ## Make sure residuals are stationary ndiffs(resid(lm.raleigh)) ## [1] 0 ### Look for AR and MA terms on the residuals ggAcf(resid(lm.raleigh)) ggPacf(resid(lm.raleigh)) ## Put it all together in a model..notice that I have time and time2 in regression fit &lt;- Raleigh_train2 %&gt;% model(ARIMA(price_index~ time +time2 +pdq(2,0,2)+PDQ(0,0,0)+1)) ### Do I have white noise for the residuals of this model? fit %&gt;% residuals() %&gt;% ggAcf(lag.max = 10) fit %&gt;% residuals() %&gt;% ggPacf(lag.max = 10) ## To forecast, we need to create a new data set with time and time2 ## The training data ended at observation 92 Raleigh_future&lt;- new_data(Raleigh_train2,5) %&gt;% mutate(time=seq(93,97), time2=time^2) ## Now we can forecast and plot fabletools::forecast(fit,new_data=Raleigh_future) %&gt;% autoplot(Raleigh_train2) ## Save forecasts fit.quad&lt;-fabletools::forecast(fit,new_data=Raleigh_future) ##Compare error and also look at plots error_lm &lt;- Raleigh.ts$price_index[93:97] - pred_lm$.mean MAPE_lm &lt;-mean(abs(error_lm/Raleigh.ts$price_index[93:97])) MAE_lm &lt;- mean(abs(error_lm)) error_quad &lt;-Raleigh.ts$price_index[93:97] - fit.quad$.mean MAPE_quad &lt;-mean(abs(error_quad/Raleigh.ts$price_index[93:97])) MAE_quad &lt;- mean(abs(error_quad)) MAPE_lm ## [1] 0.05431984 MAPE_quad ## [1] 0.03425431 MAE_lm ## [1] 18.82757 MAE_quad ## [1] 11.84002 pred_quad &lt;- tibble( quarter = yearquarter(seq.Date(from = as.Date(&quot;2023-01-01&quot;), to = as.Date(&quot;2024-01-01&quot;), by = &quot;quarter&quot;)), value = fit.quad$.mean ) combined_data &lt;- bind_rows( test_data %&gt;% mutate(Line = &quot;Actual&quot;), pred_rw %&gt;% mutate(Line = &quot;Random Walk&quot;), pred_lm2 %&gt;% mutate(Line = &quot;Linear Model&quot;), pred_quad %&gt;% mutate(Line = &quot;Quadratic&quot;) ) ggplot(combined_data, aes(x = quarter, y = value, color = Line)) + geom_line(linetype = &quot;solid&quot;) + labs(title = &quot;Predicted versus Actual values&quot;, x = &quot;Date&quot;, y = &quot;Raleigh Price Index&quot;, color = &quot;Legend&quot;) + theme_minimal() 4.9 Python Code for ARMA/ARIMA models import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import pyplot from pandas import DataFrame from statsmodels.tsa.stattools import adfuller from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf from statsmodels.tsa.arima.model import ARIMA import statsmodels.api as sm quotes=pd.read_csv(&quot;Q:\\\\My Drive\\\\Fall 2017 - Time Series\\\\DataR\\\\fpp_insurance.csv&quot;) df = pd.date_range(start=&#39;2002-01-01&#39;, end=&#39;2005-05-01&#39;, freq=&#39;ME&#39;) quotes.index=pd.to_datetime(df) y=pd.read_csv(&quot;Q:\\\\My Drive\\\\Fall 2017 - Time Series\\\\DataR\\\\ar2.csv&quot;) quotes_train = quotes.head(36) y_train = y.head(731) result=adfuller(quotes_train[&quot;Quotes&quot;]) print(f&#39;ADF p-value: {result[1]}&#39;) ## ADF p-value: 0.04236093395330452 plot_acf(quotes_train[&quot;Quotes&quot;],lags=12) pyplot.show() plot_pacf(quotes_train[&quot;Quotes&quot;],lags=12) pyplot.show() ### Using statsmodel...the older way of doing this.. model = ARIMA(y_train, order=(2,0,0)) model_fit = model.fit() print(model_fit.summary()) ## SARIMAX Results ## ============================================================================== ## Dep. Variable: Y No. Observations: 731 ## Model: ARIMA(2, 0, 0) Log Likelihood -2696.123 ## Date: Thu, 30 Oct 2025 AIC 5400.247 ## Time: 08:46:57 BIC 5418.625 ## Sample: 0 HQIC 5407.337 ## - 731 ## Covariance Type: opg ## ============================================================================== ## coef std err z P&gt;|z| [0.025 0.975] ## ------------------------------------------------------------------------------ ## const -0.0960 0.481 -0.199 0.842 -1.039 0.847 ## ar.L1 0.6398 0.036 17.605 0.000 0.569 0.711 ## ar.L2 -0.3838 0.035 -11.007 0.000 -0.452 -0.315 ## sigma2 93.4961 4.803 19.466 0.000 84.083 102.910 ## =================================================================================== ## Ljung-Box (L1) (Q): 0.00 Jarque-Bera (JB): 0.27 ## Prob(Q): 0.95 Prob(JB): 0.88 ## Heteroskedasticity (H): 0.93 Skew: -0.03 ## Prob(H) (two-sided): 0.58 Kurtosis: 3.08 ## =================================================================================== ## ## Warnings: ## [1] Covariance matrix calculated using the outer product of gradients (complex-step). residuals = DataFrame(model_fit.resid) residuals.plot() pyplot.show() print(residuals.describe()) ## 0 ## count 731.000000 ## mean 0.001103 ## std 9.685433 ## min -33.427139 ## 25% -6.456285 ## 50% 0.257085 ## 75% 6.333976 ## max 30.312938 plot_acf(residuals,lags=12) pyplot.show() plot_pacf(residuals,lags=12) pyplot.show() Using ndiffs to see if we need to take a difference: from pmdarima.arima import ndiffs hurr = pd.read_csv(&quot;https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/hurrican.csv&quot;) hurr2 = hurr.dropna(axis=0) hurricane_train =hurr2.head(149) hurricane_train ## Date Hurricanes MinVMax MaxVMax MeanVMax Year ## 0 1851 3 80.0 100.0 86.7 1851 ## 1 1852 5 70.0 100.0 82.0 1852 ## 2 1853 4 70.0 130.0 97.5 1853 ## 3 1854 3 70.0 110.0 90.0 1854 ## 4 1855 4 70.0 110.0 90.0 1855 ## .. ... ... ... ... ... ... ## 146 1997 3 65.0 110.0 81.7 1997 ## 147 1998 10 65.0 155.0 96.5 1998 ## 148 1999 8 85.0 135.0 114.4 1999 ## 149 2000 8 70.0 120.0 91.3 2000 ## 150 2001 9 65.0 125.0 91.7 2001 ## ## [149 rows x 6 columns] n_diffs = ndiffs(hurricane_train[&quot;MeanVMax&quot;]) print(n_diffs) ## 1 Checking for white noise: The first value in the Ljung-Box test is the test statistic and the second value is the p-value. import statsmodels.stats.diagnostic as diag model = ARIMA(y_train, order=(2,0,0)) model_fit = model.fit() print(model_fit.summary()) ## SARIMAX Results ## ============================================================================== ## Dep. Variable: Y No. Observations: 731 ## Model: ARIMA(2, 0, 0) Log Likelihood -2696.123 ## Date: Thu, 30 Oct 2025 AIC 5400.247 ## Time: 08:46:58 BIC 5418.625 ## Sample: 0 HQIC 5407.337 ## - 731 ## Covariance Type: opg ## ============================================================================== ## coef std err z P&gt;|z| [0.025 0.975] ## ------------------------------------------------------------------------------ ## const -0.0960 0.481 -0.199 0.842 -1.039 0.847 ## ar.L1 0.6398 0.036 17.605 0.000 0.569 0.711 ## ar.L2 -0.3838 0.035 -11.007 0.000 -0.452 -0.315 ## sigma2 93.4961 4.803 19.466 0.000 84.083 102.910 ## =================================================================================== ## Ljung-Box (L1) (Q): 0.00 Jarque-Bera (JB): 0.27 ## Prob(Q): 0.95 Prob(JB): 0.88 ## Heteroskedasticity (H): 0.93 Skew: -0.03 ## Prob(H) (two-sided): 0.58 Kurtosis: 3.08 ## =================================================================================== ## ## Warnings: ## [1] Covariance matrix calculated using the outer product of gradients (complex-step). # Perform the Ljung-Box test lb_test = diag.acorr_ljungbox(model_fit.resid, lags=[10], model_df=2) print(lb_test) ## lb_stat lb_pvalue ## 10 8.542081 0.382384 Fitting ARIMA models. ## Fit AR(2) model to AR2 data set from statsforecast import StatsForecast from statsforecast.models import ARIMA y=pd.read_csv(&quot;Q:\\\\My Drive\\\\Fall 2017 - Time Series\\\\DataR\\\\ar2.csv&quot;) df = pd.date_range(start=&#39;2000-01-01&#39;, end=&#39;2002-09-26&#39;, freq=&#39;D&#39;) y.index=pd.to_datetime(df) d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: y.index, &#39;y&#39;: y[&#39;Y&#39;]} y_sf = pd.DataFrame(data = d) y_train = y_sf.head(731) y_test = y_sf.tail(69) model_SD_ARIMA = StatsForecast(models = [ARIMA(order=(2, 0, 0))], freq = &#39;D&#39;) model_SD_ARIMA.fit(df = y_train) ## StatsForecast(models=[ARIMA]) model_SD_ARIMA.fitted_[0][0].model_.get(&quot;arma&quot;) ## (2, 0, 0, 0, 1, 0, 0) model_SD_ARIMA.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: 0.639764958452907, &#39;ar2&#39;: -0.38382217434951516, &#39;intercept&#39;: -0.09343729897766202} model_SD_ARIMA.fitted_[0][0].model_.get(&quot;aic&quot;) ## 5400.246891713973 ### Fit MA(2) model to x x=pd.read_csv(&quot;Q:\\\\My Drive\\\\Fall 2017 - Time Series\\\\DataR\\\\MA2.csv&quot;) df = pd.date_range(start=&#39;2000-01-01&#39;, end=&#39;2000-04-09&#39;, freq=&#39;D&#39;) x.index=pd.to_datetime(df) d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: x.index, &#39;y&#39;: x[&#39;x&#39;]} x_sf = pd.DataFrame(data = d) x_train = x_sf.head(74) x_test = x_sf.tail(26) model_MA2 = StatsForecast(models = [ARIMA(order=(0, 0, 2))], freq = &#39;D&#39;) model_MA2.fit(df = x_train) ## StatsForecast(models=[ARIMA]) #### Note: when you get the &quot;ARMA&quot; values for the model, it is listed as: ### p q P Q seasonlength d D ### when it is NOT considering seasonality, season length is set to 1. model_MA2.fitted_[0][0].model_.get(&quot;arma&quot;) ## (0, 2, 0, 0, 1, 0, 0) model_MA2.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ma1&#39;: -0.2619703028351055, &#39;ma2&#39;: 0.49786050912412566, &#39;intercept&#39;: 0.0011885482107510217} model_MA2.fitted_[0][0].model_.get(&quot;aic&quot;) ## 107.89060294675616 resid=model_MA2.fitted_[0][0].model_.get(&quot;residuals&quot;) plot_acf(resid,lags=12) pyplot.show() plot_pacf(resid,lags=12) pyplot.show() Using the Quotes data set for different models in ARIMA: ## Compare two different models plus automatic search on Quotes data set: from statsforecast.models import AutoARIMA from statsforecast.arima import arima_string d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: quotes.index, &#39;y&#39;: quotes[&#39;Quotes&#39;]} quotes_sf = pd.DataFrame(data = d) quotes_train = quotes_sf.head(36) quotes_test = quotes_sf.tail(4) plot_acf(quotes_train[&quot;y&quot;],lags=12) pyplot.show() plot_pacf(quotes_train[&quot;y&quot;],lags=12) pyplot.show() ## AR(1) model model_Quotes = StatsForecast(models = [ARIMA(order=(1, 0, 0), include_mean=True)], freq = &#39;ME&#39;) model_Quotes.fit(df = quotes_train) ## StatsForecast(models=[ARIMA]) model_Quotes.fitted_[0][0].model_.get(&quot;arma&quot;) ## (1, 0, 0, 0, 1, 0, 0) model_Quotes.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: 0.6712076661935175, &#39;intercept&#39;: 13.18990536452543} model_Quotes.fitted_[0][0].model_.get(&quot;aic&quot;) ## 143.5893107658691 ## MA(1) model model_Quotes2 = StatsForecast(models = [ARIMA(order=(0, 0, 1), include_mean=True)], freq = &#39;ME&#39;) model_Quotes2.fit(df = quotes_train) ## StatsForecast(models=[ARIMA]) model_Quotes2.fitted_[0][0].model_.get(&quot;arma&quot;) ## (0, 1, 0, 0, 1, 0, 0) model_Quotes2.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ma1&#39;: 0.7507681467542602, &#39;intercept&#39;: 13.236625919936786} model_Quotes2.fitted_[0][0].model_.get(&quot;aic&quot;) ## 143.6147938154081 ### Now the automatic search....choose the AR(1) model model_Quotes = StatsForecast(models = [AutoARIMA(seasonal=False)], freq = &#39;ME&#39;) model_Quotes.fit(df = quotes_train) ## StatsForecast(models=[AutoARIMA]) model_Quotes.fitted_[0][0].model_.get(&quot;arma&quot;) ## (1, 0, 0, 0, 1, 0, 0) model_Quotes.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: 0.9900652806700364} model_Quotes.fitted_[0][0].model_.get(&quot;aic&quot;) ## 151.64675255955572 ### Even though the automatic search choose AR(1), it did NOT include intercept. ### Made AR term close to 1 (very close to a random walk). ### Refitting the AR(1) model: model_Quotes = StatsForecast(models = [ARIMA(order=(1, 0, 0), include_mean=True)], freq = &#39;ME&#39;) model_Quotes.fit(df = quotes_train) ## StatsForecast(models=[ARIMA]) resid=model_Quotes.fitted_[0][0].model_.get(&quot;residuals&quot;) plot_acf(resid,lags=12) pyplot.show() plot_pacf(resid,lags=12) pyplot.show() # Perform the Ljung-Box test lb_test = diag.acorr_ljungbox(resid, lags=[10], model_df=1) print(lb_test) ## lb_stat lb_pvalue ## 10 7.200837 0.616218 Now for using the hurricane data set: ###Note that hurr2 is being redefined here. df = pd.date_range(start=&#39;1851-01-01&#39;, end=&#39;2008-01-01&#39;, freq=&#39;YE&#39;) hurr.index=pd.to_datetime(df) hurr2 = hurr.dropna(axis=0) d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: hurr2.index, &#39;y&#39;: hurr2[&#39;MeanVMax&#39;]} hurr_sf = pd.DataFrame(data = d) hurr_train = hurr_sf.head(149) hurr_test = hurr_sf.tail(9) ###Looks like we need to take differences: n_diffs=ndiffs(hurr_train[&quot;y&quot;]) print(n_diffs) ## 1 hurr2[&#39;MVM_diff&#39;] = hurr2[&#39;MeanVMax&#39;].diff() ## &lt;string&gt;:2: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy hurr3 = hurr2.dropna(axis=0) df = pd.date_range(start=&#39;1854-01-01&#39;, end=&#39;2008-01-01&#39;, freq=&#39;YE&#39;) hurr3.index=pd.to_datetime(df) d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: hurr3.index, &#39;y&#39;: hurr3[&#39;MVM_diff&#39;]} hurr_sf = pd.DataFrame(data = d) hurr_train = hurr_sf.head(146) plot_acf(hurr_train[&#39;y&#39;],lags=12) pyplot.show() plot_pacf(hurr_train[&#39;y&#39;],lags=12) pyplot.show() ### Now to fit models model_hurr = StatsForecast(models = [ARIMA(order=(3, 1, 0))], freq = &#39;YE&#39;) model_hurr.fit(df = hurr_train) ## StatsForecast(models=[ARIMA]) model_hurr.fitted_[0][0].model_.get(&quot;arma&quot;) ## (3, 0, 0, 0, 1, 1, 0) model_hurr.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: -1.206803756212731, &#39;ar2&#39;: -0.9921577329431666, &#39;ar3&#39;: -0.45513268425713543} model_hurr.fitted_[0][0].model_.get(&quot;aic&quot;) ## 1170.7081439455412 model_hurr = StatsForecast(models = [ARIMA(order=(0, 1, 2))], freq = &#39;YE&#39;) model_hurr.fit(df = hurr_train) ## StatsForecast(models=[ARIMA]) model_hurr.fitted_[0][0].model_.get(&quot;arma&quot;) ## (0, 2, 0, 0, 1, 1, 0) model_hurr.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ma1&#39;: -1.7415599367901724, &#39;ma2&#39;: 0.7832219253808886} model_hurr.fitted_[0][0].model_.get(&quot;aic&quot;) ## 1106.8880415246215 model_hurr1 = StatsForecast(models = [ARIMA(order=(3, 1, 2))], freq = &#39;YE&#39;) model_hurr1.fit(df = hurr_train) ## StatsForecast(models=[ARIMA]) model_hurr1.fitted_[0][0].model_.get(&quot;arma&quot;) ## (3, 2, 0, 0, 1, 1, 0) model_hurr1.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: -0.023320010896224146, &#39;ar2&#39;: -0.1099021803224285, &#39;ar3&#39;: 0.08526901780966276, &#39;ma1&#39;: -1.8828872133155077, &#39;ma2&#39;: 0.8828872198953689} model_hurr1.fitted_[0][0].model_.get(&quot;aic&quot;) ## 1095.8026951945421 model_hurr = StatsForecast(models = [AutoARIMA(seasonal=False)], freq = &#39;YE&#39;) model_hurr.fit(df = quotes_train) ## StatsForecast(models=[AutoARIMA]) model_hurr.fitted_[0][0].model_.get(&quot;arma&quot;) ## (1, 0, 0, 0, 1, 0, 0) model_hurr.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: 0.9900652806700364} model_hurr.fitted_[0][0].model_.get(&quot;aic&quot;) ## 151.64675255955572 resid=model_hurr1.fitted_[0][0].model_.get(&quot;residuals&quot;) plot_acf(resid,lags=12) pyplot.show() plot_pacf(resid,lags=12) pyplot.show() lb_test = diag.acorr_ljungbox(resid, lags=[10], model_df=5) print(lb_test) ## lb_stat lb_pvalue ## 10 2.30648 0.805315 Get some measures of accuracy on validation data: ##Hurricane data y_hat1=model_hurr1.predict(h=8) ## C:\\PROGRA~3\\ANACON~1\\Lib\\site-packages\\statsforecast\\core.py:492: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning. ## warnings.warn( yhat=y_hat1.reset_index(drop=True) test = hurr_test.reset_index(drop=True) abs_error= np.absolute(test[&#39;y&#39;]-yhat[&quot;ARIMA&quot;]) MAE = np.mean(abs_error) MAE ## 100.95193915786221 MAPE = np.mean(abs_error/np.absolute(test[&#39;y&#39;])) MAPE ## 1.022498446993838 An example with trend (fitting it with both differencing and linear regression) consume = pd.read_csv(&quot;https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/consume1982.csv&quot;) df = pd.date_range(start=&#39;1982-09-01&#39;, end=&#39;1990-07-01&#39;, freq=&#39;ME&#39;) consume.index=pd.to_datetime(df) consume2 =consume d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: consume.index, &#39;y&#39;: consume[&#39;Disposable_income&#39;]} consume_sf = pd.DataFrame(data = d) consume_train = consume_sf.head(88) consume_test = consume_sf.tail(6) n_diffs=ndiffs(consume_train[&quot;y&quot;]) print(n_diffs) ## 1 consume2[&#39;income_diff&#39;] = consume[&#39;Disposable_income&#39;].diff() consume3 = consume2.dropna(axis=0) d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: consume3.index, &#39;y&#39;: consume3[&#39;income_diff&#39;]} consume_sf = pd.DataFrame(data = d) consume_train_diff = consume_sf.head(87) consume_test_diff = consume_sf.tail(6) plot_acf(consume_train_diff[&#39;y&#39;],lags=12) pyplot.show() plot_pacf(consume_train_diff[&#39;y&#39;],lags=12) pyplot.show() model_consume = StatsForecast(models = [ARIMA(order=(1, 1, 0), alias=&quot;AR1&quot;,include_drift=True), ARIMA(order=(0, 1, 1), alias=&quot;MA1&quot;,include_drift=True), ARIMA(order=(6, 1, 0), alias=&quot;AR6&quot;,include_drift=True), ARIMA(order=(0, 1, 6), alias=&quot;MA6&quot;,include_drift=True)], freq = &#39;ME&#39;) model_consume.fit(df = consume_train) ## StatsForecast(models=[AR1,MA1,AR6,MA6]) model_consume.fitted_[0][0].model_.get(&quot;arma&quot;) ## (1, 0, 0, 0, 1, 1, 0) model_consume.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: -0.25896653323215896, &#39;drift&#39;: 8.022216007742262} model_consume.fitted_[0][0].model_.get(&quot;aic&quot;) ## 803.1326810471955 model_consume.fitted_[0][1].model_.get(&quot;arma&quot;) ## (0, 1, 0, 0, 1, 1, 0) model_consume.fitted_[0][1].model_.get(&quot;coef&quot;) ## {&#39;ma1&#39;: -0.41438549614590336, &#39;drift&#39;: 7.98102069783255} model_consume.fitted_[0][1].model_.get(&quot;aic&quot;) ## 799.3233434797908 model_consume.fitted_[0][2].model_.get(&quot;arma&quot;) ## (6, 0, 0, 0, 1, 1, 0) model_consume.fitted_[0][2].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: -0.4010611168449245, &#39;ar2&#39;: -0.31654068903784843, &#39;ar3&#39;: -0.16283506529385414, &#39;ar4&#39;: -0.1061553402707993, &#39;ar5&#39;: -0.010770449307681035, &#39;ar6&#39;: -0.22726549262306284, &#39;drift&#39;: 7.7466345838326145} model_consume.fitted_[0][2].model_.get(&quot;aic&quot;) ## 801.9784335671225 model_consume.fitted_[0][3].model_.get(&quot;arma&quot;) ## (0, 6, 0, 0, 1, 1, 0) model_consume.fitted_[0][3].model_.get(&quot;coef&quot;) ## {&#39;ma1&#39;: -0.31833320917990643, &#39;ma2&#39;: -0.2699449416974281, &#39;ma3&#39;: 0.04769403066216973, &#39;ma4&#39;: 0.022689040735286884, &#39;ma5&#39;: 0.1251498387227853, &#39;ma6&#39;: -0.370928091243984, &#39;drift&#39;: 8.089050997814592} model_consume.fitted_[0][3].model_.get(&quot;aic&quot;) ## 795.8092927997254 ### Now for looking at trend: consume = pd.read_csv(&quot;https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/consume1982.csv&quot;) df = pd.date_range(start=&#39;1982-09-01&#39;, end=&#39;1990-07-01&#39;, freq=&#39;ME&#39;) consume.index=pd.to_datetime(df) #consume2 =consume d = {&#39;unique_id&#39;: 1, &#39;ds&#39;: consume.index, &#39;y&#39;: consume[&#39;Disposable_income&#39;]} consume_sf = pd.DataFrame(data = d) consume_sf[&#39;x&#39;] = range(1, len(df) + 1) consume_train = consume_sf.head(88) consume_test = consume_sf.tail(6) 4.10 SAS Code for ARMA/ARIMA AUGMENTED DICKEY-FULLER TESTING proc arima data=Time.fpp_insurance plot=all;   identify var=quotes nlag=10 stationarity=(adf=2);   identify var=quotes(1) nlag=10 stationarity=(adf=2);   run; quit; CORRELATION FUNCTIONS Notice no model statement!   proc arima data=Time.ar2 plot(unpack)=all;   identify var=y nlag=10 outcov=Corr;   estimate method=ML;   run; quit; BUILDING AN AUTOREGRESSIVE MODEL Fit an AR2 model   proc arima data=Time.AR2 plot=all;   identify var=y nlag=10;   estimate p=2 method=ML;   run; quit;   Add another estimate statement proc arima data=Time.AR2 plot=all; identify var=y nlag=10; estimate p=(2) method=ML; estimate p=(1,2,4) method=ML; run; quit;   BUILDING A MOVING AVERAGE MODEL proc arima data=Time.ma2;   identify var=x;   estimate q=2 method=ML;   run; quit;   Need to check for how to take care of trend proc arima data=Time.Ebay9899 plot=all;   identify var=DailyHigh nlag=10 stationarity=(adf=2);   run; quit; It is a random walk!! The way to model a random walk is by using differences   proc arima data=Time.Ebay9899 plot=all;   identify var=DailyHigh(1) nlag=10 stationarity=(adf=2);   run; quit; BUILDING AN AUTOREGRESSIVE MOVING AVERAGE MODEL   (AUTOMATIC SELECTION TECHNIQUES)   Fit an ARIMA model   proc arima data=Time.Hurricanes plot=all;   identify var=MeanVMax nlag=12 stationarity=(adf=2);   run; quit;   Model identification with minimum information criterion (MINIC)   proc arima data=Time.Hurricanes plot=all;   identify var=MeanVMax nlag=12 minic P=(0:12) Q=(0:12);   run; quit;   Model identification with smallest canonical correlation (SCAN);   proc arima data=Time.Hurricanes plot=all;   identify var=MeanVMax nlag=12 scan P=(0:12) Q=(0:12);   run; quit;   Model identificaiton with extended sample autocorrelation function (ESACF)   proc arima data=Time.Hurricanes plot=all;   identify var=MeanVMax nlag=12 esacf P=(0:12) Q=(0:12);   run; quit;   Create estimates with our ARIMA model p=2, q=3   proc arima data=Time.Hurricanes plot=all;   identify var=MeanVMax nlag=12;   estimate p=2 q=3 method=ML;   run; quit; FORECASTING   proc arima data=Time.Hurricanes plot=all;   identify var=MeanVMax nlag=10 ;   estimate p=2 q=3 method=ML;   forecast lead=10;   run; quit; "],["seasonal-arima.html", "Chapter 5 Seasonal ARIMA 5.1 Seasonality 5.2 Seasonal “Unit-Root” Testing 5.3 Deterministic Solution 5.4 Fourier Transformations 5.5 Other Predictor Variables 5.6 Stochastic Solution 5.7 Seasonal ARIMA Modeling 5.8 Multiplicative vs. Additive", " Chapter 5 Seasonal ARIMA 5.1 Seasonality Seasonality is the component of time series that represents the effects of seasonal variation. The seasonal component of a time series is the repeated pattern over a fixed period of time. For example, the airlines dataset we’ve been working through (shown below) shows seasonality every 12 observations (12 months) where we see different patterns in summer and holiday times of the year as compared to other times of the year. Seasonality occurs over a fixed and known period of time. autoplot(USAirlines_ts, Passengers) + labs(title=&quot;Time Series plot for Passengers&quot;, x=&quot;Date&quot;,y=&quot;Passengers&quot;) The length of the seasonal periods is denoted as S. Therefore, seasonal factors repeat every S units of time. As mentioned in the stationarity section, seasonal models are not stationary by default because the time series doesn’t revert to a long term mean. When your data has both trend and seasonality, the seasonality should be the first problem you try and correct to make stationary. We do this because making the data stationary by accounting for the seasonality might also account for trend. This is not true for trend. Our process for building ARIMA models is the following: Similar to when we accounted for trend, seasonality can be solved with a deterministic or stochastic solution. Deterministic - Seasonal dummy variables, Fourier transformations, predictor variables Stochastic - Seasonal differences Once data is made stationary, we can model with traditional and seasonal ARIMA approaches. When we forecast our data we account for the deterministic or stochastic solution above and build that into our forecasts. 5.2 Seasonal “Unit-Root” Testing Similar to trend, we can perform statistical tests to try and evaluate whether we need to take a seasonal difference or perform more deterministic solutions. However, seasonal unit root tests have problems with large seasonal frequencies - values of S that are larger than 12. To counter these problems with seasonal unit roots we use measures of seasonal strength instead: \\[ F_S = \\max{(0, 1 - \\frac{Var(E_t)}{Var(S_t + E_t)})} \\] where \\(E_t\\) is the error component and \\(S_t\\) is the seasonal component. These components both come from a time series decomposition of the data. If \\(F_S &lt; 0.64\\) then no seasonal differencing would be recommended, while if \\(F_S \\ge 0.64\\) then take one seasonal difference. The only downside of this approach is that if your data has any real amount of seasonality it would just recommend taking the differences without any notion of deterministic solutions. Therefore, this really isn’t an approach to compare whether you should use deterministic or stochastic solution for your specific problem. In practice, we typically model our data with both deterministic and stochastic solutions and compare the forecasts to see which was more accurate. Let’s see how to evaluate this in each of our softwares! 5.2.1 R R has this functionality built in with its unitroot_nsdiffs option inside of the features function. We just need to specify the variable we want to calculate this test on, which for us is the variable Passengers. The output reports the number of seasonal differences it suggests (either 1 or 0) based on the cut-off described above. train %&gt;% features(Passengers, unitroot_nsdiffs) ## # A tibble: 1 × 1 ## nsdiffs ## &lt;int&gt; ## 1 1 5.2.2 Python Python doesn’t have a built in function at the time of writing this to calculate this. However, we can easily calculate this ourselves from the time series decomposition we have done in previous sections. Inside of our StatsForecast function we use the MSTL function with a frequency set to monthly data. This MSTL function calculates the STL version of the decomposition. We then use the fit function with the df = option to specify our training data we want to decompose. We can then use the .fitted_[0,0].model_ functions to print out each of the above components for each observation in our training data. from statsforecast import StatsForecast from statsforecast.models import MSTL dcmp = StatsForecast(models = [MSTL(season_length = 12)], freq = &#39;M&#39;) dcmp.fit(df = train_sf) ## StatsForecast(models=[MSTL]) result = dcmp.fitted_[0,0].model_ result ## data trend seasonal remainder ## 0 34348.0 39019.258306 -4656.823649 -14.434657 ## 1 33536.0 38883.031147 -5824.358016 477.326868 ## 2 40578.0 38749.491951 1252.474783 576.033266 ## 3 38267.0 38618.251802 -554.512182 203.260380 ## 4 38249.0 38488.211305 388.631112 -627.842417 ## .. ... ... ... ... ## 202 60341.0 62515.681097 -2946.195752 771.514655 ## 203 60791.0 62606.870904 -1594.438482 -221.432422 ## 204 57233.0 62699.163218 -6078.039540 611.876322 ## 205 54140.0 62792.107128 -7499.634204 -1152.472924 ## 206 67342.0 62885.168552 4003.645894 453.185554 ## ## [207 rows x 4 columns] Now that we have our components, we can calculate \\(F_S\\) by hand. We just use the var function from numpy to calculate the variance. import numpy as np FS = 1 - (np.var(result[&#39;remainder&#39;])/(np.var(result[&#39;remainder&#39;]) + np.var(result[&#39;seasonal&#39;]))) FS ## 0.9140123862305709 5.2.3 SAS SAS doesn’t have a built in function at the time of writing this to calculate this. However, we can easily calculate this ourselves from the time series decomposition we have done in previous sections. By default SAS will provide the classical decomposition. In the PROC TIMESERIES procedure, we use the data =option to provide our training data and the plots = (series decomp) option to ask SAS for a plot of the original data series and a plot of the decomposition. The outdecomp option saves the dataset with teh decomposition of our time series. The ID statement is where we provide our date variable and specify our monthly seasonality with the interval = month option. Next, in the VAR statement we provide our target variable, Passengers. proc timeseries data = work.train plots = (series decomp) outdecomp = decomp; id date interval = month; var Passengers; run; Now that we have our components, we can calculate \\(F_S\\) by hand. First, we need to combine our seasonal and error components represented in our data as SC and IC respectively. Once we combine these together in a DATA STEP we can use the PROC MEANS procedure to calculate the variance of each of them. We just use the var option in the PROC MEANS statement and name the variables in the VAR statement. From there we can use another DATA STEP to calculate the value of \\(F_S\\) and print it out with the PROC PRINT procedure. data decomp; set decomp; S_E = SC + IC; run; proc means data = decomp var; var IC S_E; run; data calculation; FS = 1 - (0.0011888/0.0084828); run; proc print data = calculation; run; The tests above suggest taking a seasonal difference. However, let’s solve our problem with both a deterministic approach and stochastic approach to see which is better for our data. 5.3 Deterministic Solution Unlike trend, there are many different approaches to accounting for seasonality deterministically. The approaches we will discuss are seasonal dummy variables, Fourier transforms, and predictor variables. Seasonal Dummy Variables Seasonal dummy variables are exactly what their name implies. We will use a set of dummy variables to account for the seasonal effects in our dataset. For a time series with S periods within a season, there will be S-1 dummy variables - one for each period (and one accounted for with the intercept). Since our US airlines dataset is monthly, we will build a dummy variable for each month and pick one to not include. For example, we could build the following model: \\[ Y_t = \\beta_0 + \\beta_1JAN + \\beta_2FEB + \\cdots + \\beta_{11}NOV + e_t \\] In the above model, we chose to leave out the month of December’s dummy variable since our model has an intercept. In fact, the intercept \\(\\beta_0\\) is the average effect of December. Each of the coefficients on the respective variables measures the average difference in effect from that month and December (our reference level). Let’s see how to do this in each of our softwares! 5.3.1 R Adding external variables to an ARIMA model in R is quite straightforward. By using the formula structure in the ARIMA function in the model function (from the fable package), we can add these dummy variables to our model as well as build out an automatic ARIMA model. We specify that the Month variable in our dataset is a categorical variable with the factor function. To prevent the model from automatically trying to take a seasonal difference we use the PDQ function where we specify the D = 0 option. model_SD_ARIMA &lt;- train %&gt;% model(ARIMA(Passengers ~ factor(Month) + PDQ(D = 0))) report(model_SD_ARIMA) ## Series: Passengers ## Model: LM w/ ARIMA(1,1,1)(0,0,2)[12] errors ## ## Coefficients: ## ar1 ma1 sma1 sma2 factor(Month)2 factor(Month)3 factor(Month)4 factor(Month)5 ## 0.4437 -0.7884 0.2291 0.1213 -1128.4319 8309.2204 5714.8890 6702.4628 ## s.e. 0.1350 0.0973 0.0684 0.0697 601.3847 733.8228 802.3666 834.1741 ## factor(Month)6 factor(Month)7 factor(Month)8 factor(Month)9 factor(Month)10 ## 9450.040 11955.9752 11632.3364 1024.3541 5202.9495 ## s.e. 849.269 854.2442 850.4283 836.4706 807.6305 ## factor(Month)11 factor(Month)12 intercept ## 2791.6907 3925.0336 114.7323 ## s.e. 748.6175 615.1594 64.3365 ## ## sigma^2 estimated as 3535534: log likelihood=-1837.63 ## AIC=3709.26 AICc=3712.52 BIC=3765.84 R found the “best” model based on automatic selection using seasonal dummy variables as the ARIMA model with one autoregressive (AR) term, one regular difference, one moving average (MA) term, and two seasonal moving average (SMA) terms. Seasonal AR and MA terms are discussed below in the seasonal ARIMA section. 5.3.2 Python Adding external variables to an ARIMA model in Python is quite straightforward. First, we use the get_dummies function from pandas to make our Month variable into dummy variables. By using the merge function on our training dataset we can add this new variable to our training data. d_X = {&#39;unique_id&#39;: 1, &#39;ds&#39;: train.index, &#39;Month&#39;: usair[&#39;Month&#39;].head(n = 207)} X_sf = pd.DataFrame(data = d_X) X_sf = pd.get_dummies(X_sf, columns = [&#39;Month&#39;]) train_sf_X = train_sf.merge(X_sf, how = &#39;left&#39;, on = [&#39;unique_id&#39;, &#39;ds&#39;]) Now, in the AutoARIMA function, we can add these seasonal dummy variables to our model as well as build out an automatic ARIMA by just putting the training dataset with these new columns in the .fit function. This means we have to be careful about which datasets we use since Python automatically assumes any additional column (beyond unique_id, ds, and y) are predictor variables. We use the .fitted_[0][0].model_.get() and function to evaluate our model by asking for the order of the ARIMA model (arma), the coefficients in the ARIMA model (coef) and the AIC (aic). To prevent the model from automatically trying to take a seasonal difference we use the D = 0 option. To make sure it knows this data has possible seasonal correlations we also use the season_length = 12 option. from statsforecast.models import AutoARIMA, ARIMA model_SD_ARIMA = StatsForecast(models = [AutoARIMA(D = 0, season_length = 12)], freq = &#39;M&#39;) model_SD_ARIMA.fit(df = train_sf_X) ## StatsForecast(models=[AutoARIMA]) model_SD_ARIMA.fitted_[0][0].model_.get(&quot;arma&quot;) ## (1, 1, 0, 2, 12, 1, 0) model_SD_ARIMA.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: 0.37132542906435295, &#39;ma1&#39;: -0.7274857656037033, &#39;sma1&#39;: 0.24669635086070002, &#39;sma2&#39;: 0.14076605299826497, &#39;ex_1&#39;: -5545.23095703125, &#39;ex_2&#39;: -6631.3447265625, &#39;ex_3&#39;: 2788.095458984375, &#39;ex_4&#39;: 234.20970153808594, &#39;ex_5&#39;: 1250.2080078125, &#39;ex_6&#39;: 4022.0859375, &#39;ex_7&#39;: 6561.49462890625, &#39;ex_8&#39;: 6233.07861328125, &#39;ex_9&#39;: -4427.6884765625, &#39;ex_10&#39;: -241.98623657226562, &#39;ex_11&#39;: -2695.10693359375, &#39;ex_12&#39;: -1547.816162109375} model_SD_ARIMA.fitted_[0][0].model_.get(&quot;aic&quot;) ## 3712.0228454869493 Python found the “best” model based on automatic selection using seasonal dummy variables as the ARIMA model with one autoregressive (AR) term, one regular difference, one moving average (MA) term, and two seasonal moving average (SMA) terms. Seasonal AR and MA terms are discussed below in the seasonal ARIMA section. 5.3.3 SAS Adding external variables to an ARIMA model in SAS is quite straightforward. First, we use the DATA STEP to make our Month variable into dummy variables. From there we use the PROC ARIMA procedure with this newly created dataset. With the IDENTIFY statement we give our target variable with the VAR option. We also include the CROSSCORR option with our all of our dummy variables we just calculated. Unfortunately, we cannot use the automatic selection techniques here since they will just be on the original target variable. We then use the ESTIMATE statement to define our model with only the seasonal dummy variables. The INPUT option is where we put our dummy variables into the actual model. We finish with the FORECAST statement with the LEAD option set to 0 to just give us fitted values from our training dataset. The OUT option at the very beginning of the procedure will save the model predictions and residuals which we will use in the next PROC ARIMA. Since the PROC ARIMA procedure can only do automatic selection techniques on the variables in the IDENTIFY which don’t include the dummy variables yet, we have to use the residuals from our seasonal dummy variables model to put in another PROC ARIMA procedure to get an automatic selection for our model. First, we run the residuals through a Dickey-Fuller test using the STATIONARITY option with the ADF option inside of it to see if we need to difference. After looking at the results below we see we need to take a difference so we rerun the PROC ARIMA procedure with the additional IDENTIFY statement on the first difference of the residuals using the (1) notation. Then we can use the MINIC to do automatic selection. data train_sd; set train; if month=1 then Jan = 1; else Jan=0; if month=2 then Feb = 1; else Feb=0; if month=3 then Mar = 1; else Mar=0; if month=4 then Apr = 1; else Apr=0; if month=5 then May = 1; else May=0; if month=6 then Jun = 1; else Jun=0; if month=7 then Jul = 1; else Jul=0; if month=8 then Aug = 1; else Aug=0; if month=9 then Sep = 1; else Sep=0; if month=10 then Oct = 1; else Oct=0; if month=11 then Nov = 1; else Nov=0; if month=12 then Dec = 1; else Dec=0; run; proc arima data = train_sd out = train_sd_f ; identify var = Passengers nlag = 36 crosscorr = (JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV); estimate input = (JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV); forecast lead = 0; run; proc arima data = train_sd_f; identify var = residual nlag = 36 stationarity=(adf = 5); identify var = residual(1) nlag = 36 minic P=(1:24) Q=(1:24); run; SAS output The results from the MINIC selection gave us only an ARIMA (1,1,1) model. However, there appears to be spikes in our correlation plots below at seasonal lags (explained in the section below on seasonal ARIMA). SAS output Once we decide what model we would like to build, we can go back to the original PROC ARIMA procedure and add our autoregressive (AR) lags with the P option and our moving average (MA) terms with the Q option. We have a model with lags on the seasonal lags as well which is explained in the section below. proc arima data = train_sd out = train_sd_f ; identify var = Passengers(1) nlag = 36 crosscorr = (JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV); estimate input = (JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV) p = (1)(12) q = (1)(12); run; SAS output SAS output There are some advantages and disadvantages to the seasonal dummy variable approach. The advantages are that the model’s seasonal dummy variables have some nice interpretability. They tells us the average impact each seasonal component has on the target variable. It is also a rather straight-forward approach to implement. The main disadvantages however are that if your seasonal is especially long or complex, seasonal dummy variables are burdensome. Also, the constant effect of each season that is assumed may not be appropriate. 5.4 Fourier Transformations Harmonic regression using Fourier transforms is another approach to account for seasonality deterministically. Fourier showed that a series of sine and cosine terms of the right frequencies approximate periodic patterns in a data series. To do this, we add Fourier variables to a regression model to account for the seasonal pattern. The odd terms \\(k=1,3,5\\) etc. are accounted for with sine variables: \\[ X_{k, t} = \\sin(k \\times \\frac{2\\pi t}{S}) \\] The even terms \\(k = 2, 4, 6\\) etc. are accounted for with cosine variables: \\[ X_{k, t} = \\cos(k \\times \\frac{2\\pi t}{S}) \\] The goal is to find the right combination of sine and cosine terms that when blended together mimic the unique pattern in our data. The idea of adding these terms together to get a new unique wave is show below: SAS output In fact, if you add the same number of Fourier variables as you have seasonal dummy variables, you will get the same predictions. However, we typically do not need all of the Fourier variables, especially with large values of S. We need to decide how many of these values we need. One way to approach this is to build many models, while increasing the number of Fourier terms in the models and compare them all via a metric like BIC. Let’s see how to do this in each of our softwares! 5.4.1 R Adding external variables to an ARIMA model in R is quite straightforward. By using the formula structure in the ARIMA function in the model function, we can add these fourier variables to our model as well as build out an automatic ARIMA model. The fourier function in our formula will add the Fourier variables in pairs of sine and cosine functions with the K = option defining how many pairs of the Fourier variables to add. To prevent the model from automatically trying to take a seasonal difference we use the PDQ function where we specify the D = 0 option. We can use the glance function to quickly compare all these models. model_F_ARIMA &lt;- train %&gt;% model( `K = 1` = ARIMA(Passengers ~ fourier(K=1) + PDQ(D = 0)), `K = 2` = ARIMA(Passengers ~ fourier(K=2) + PDQ(D = 0)), `K = 3` = ARIMA(Passengers ~ fourier(K=3) + PDQ(D = 0)), `K = 4` = ARIMA(Passengers ~ fourier(K=4) + PDQ(D = 0)), `K = 5` = ARIMA(Passengers ~ fourier(K=5) + PDQ(D = 0)), `K = 6` = ARIMA(Passengers ~ fourier(K=6) + PDQ(D = 0)) ) glance(model_F_ARIMA) ## # A tibble: 6 × 8 ## .model sigma2 log_lik AIC AICc BIC ar_roots ma_roots ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 K = 1 5041124. -1887. 3785. 3786. 3805. &lt;cpl [13]&gt; &lt;cpl [1]&gt; ## 2 K = 2 5086926. -1887. 3789. 3790. 3816. &lt;cpl [13]&gt; &lt;cpl [1]&gt; ## 3 K = 3 4296120. -1866. 3753. 3754. 3786. &lt;cpl [24]&gt; &lt;cpl [1]&gt; ## 4 K = 4 4327984. -1866. 3756. 3757. 3796. &lt;cpl [24]&gt; &lt;cpl [1]&gt; ## 5 K = 5 3689835. -1843. 3718. 3720. 3771. &lt;cpl [0]&gt; &lt;cpl [27]&gt; ## 6 K = 6 3535544. -1838. 3709. 3713. 3766. &lt;cpl [1]&gt; &lt;cpl [25]&gt; Based on the results above, the model with all of the Fourier terms has the lowest values of AIC, AICc, and BIC. This model is the same as the model that we had with the seasonal dummy variables. We can see a plot of these different models below. model_F_ARIMA %&gt;% fabletools::forecast(h = 12) %&gt;% autoplot(train, level = 95) + facet_wrap(vars(.model), ncol = 2) + guides(colour = &quot;none&quot;, fill = &quot;none&quot;, level = &quot;none&quot;) + geom_label( aes(x = yearmonth(&quot;2007 Jan&quot;), y = 4250, label = paste0(&quot;AICc = &quot;, format(AICc))), data = glance(model_F_ARIMA) ) + labs(title= &quot;Comparison of Different Models&quot;, y=&quot;Passengers&quot;) 5.4.2 Python Adding external variables to an ARIMA model in Python is quite straightforward. First, we use the fourier function from the utils.feature_engineering package to make our Fourier variables. We need to specify the length of the season as well as how many pairs of sine and cosine terms to be created with the k = option. from utilsforecast.feature_engineering import fourier train_sf_f, test_sf_f = fourier(train_sf, freq=&#39;ME&#39;, season_length = 12, k = 6, h = 12) Now that we have created our training data we can use a for loop to try out the six different models with up to six pairs of Fourier variables. We can compare these models based on AIC. aic = np.array([]) for i in range(7): model_F_ARIMA = StatsForecast(models = [AutoARIMA(season_length = 12, D = 0)], freq = &#39;M&#39;) model_F_ARIMA.fit(df = train_sf_f.iloc[:, 0:3 + 2*i]) aic = np.append(aic, model_F_ARIMA.fitted_[0][0].model_.get(&quot;aic&quot;)) ## StatsForecast(models=[AutoARIMA]) ## StatsForecast(models=[AutoARIMA]) ## StatsForecast(models=[AutoARIMA]) ## StatsForecast(models=[AutoARIMA]) ## StatsForecast(models=[AutoARIMA]) ## StatsForecast(models=[AutoARIMA]) ## StatsForecast(models=[AutoARIMA]) print(aic) ## [3811.43504347 3815.00601709 3794.76746678 3780.58680421 3741.98970672 ## 3720.41333752 3711.34605279] Based on the results above, the model with all of the Fourier terms has the lowest values of AIC, AICc, and BIC. This model is the same as the model that we had with the seasonal dummy variables. We can build this final model below to see what the automatic ARIMA model is with these Fourier terms. model_F_ARIMA = StatsForecast(models = [AutoARIMA(season_length = 12, D = 0)], freq = &#39;M&#39;) model_F_ARIMA.fit(df = train_sf_f) ## StatsForecast(models=[AutoARIMA]) model_F_ARIMA.fitted_[0][0].model_.get(&quot;arma&quot;) ## (1, 1, 0, 2, 12, 1, 0) model_F_ARIMA.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: 0.3732468363982339, &#39;ma1&#39;: -0.7307059972352768, &#39;sma1&#39;: 0.24990579074901723, &#39;sma2&#39;: 0.1340925791781142, &#39;ex_1&#39;: -1257.8986624024692, &#39;ex_2&#39;: 304.52100293908734, &#39;ex_3&#39;: -2568.038612436736, &#39;ex_4&#39;: 385.81835392443463, &#39;ex_5&#39;: 2282.5552591547603, &#39;ex_6&#39;: -6944313.76512648, &#39;ex_7&#39;: -4356.104750276369, &#39;ex_8&#39;: 663.2591446039733, &#39;ex_9&#39;: 1280.8474981794134, &#39;ex_10&#39;: 202.15878511196934, &#39;ex_11&#39;: 283.18975372909335, &#39;ex_12&#39;: 415.4550326420687} model_F_ARIMA.fitted_[0][0].model_.get(&quot;aic&quot;) ## 3711.3460527885513 5.4.3 SAS Adding external variables to an ARIMA model in SAS is quite straightforward. First, we use the DATA STEP to make our Fourier variables by calculating them by hand. From there we use the PROC ARIMA procedure with this newly created dataset. With the IDENTIFY statement we give our target variable with the VAR option. We also include the CROSSCORR option with our all of our Fourier variables we just calculated. We then use the ESTIMATE statement to define our model with the Fourier variables in six different combinations where we add a new pair of sine and cosine functions with each new model. The INPUT option is where we put our Fourier variables into the actual model. We can add our autoregressive (AR) lags with the P option and our moving average (MA) terms with the Q option. We have a model with lags on the seasonal lags as well which is explained in the section below. data train_f; set train; pi = constant(&#39;PI&#39;); x1=cos(2*pi*1*_n_/12); x2=sin(2*pi*1*_n_/12); x3=cos(2*pi*2*_n_/12); x4=sin(2*pi*2*_n_/12); x5=cos(2*pi*3*_n_/12); x6=sin(2*pi*3*_n_/12); x7=cos(2*pi*4*_n_/12); x8=sin(2*pi*4*_n_/12); x9=cos(2*pi*5*_n_/12); x10=sin(2*pi*5*_n_/12); x11=cos(2*pi*6*_n_/12); x12=sin(2*pi*6*_n_/12); run; proc arima data = train_f; identify var = Passengers nlag = 36 crosscorr = (x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12); estimate input = (x1 x2) p = (1)(12) q = (1)(12); estimate input = (x1 x2 x3 x4) p = (1)(12) q = (1)(12); estimate input = (x1 x2 x3 x4 x5 x6) p = (1)(12) q = (1)(12); estimate input = (x1 x2 x3 x4 x5 x6 x7 x8) p = (1)(12) q = (1)(12); estimate input = (x1 x2 x3 x4 x5 x6 x7 x8 x9 x10) p = (1)(12) q = (1)(12); estimate input = (x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12) p = (1)(12) q = (1)(12); run; quit; SAS output The results shown is the just from the model with the best AIC value. Based on the results above, the model with all of the Fourier terms has the lowest value of AIC. This model is the same as the model that we had with the seasonal dummy variables. There are some advantages and disadvantages of the Fourier term approach to accounting for seasonality. The main advantage is that Fourier terms can handle long and complex seasonality. In fact, for multiple seasons in a dataset, we can add more Fourier variables at different frequencies to account for this. The disadvantages of the Fourier approach are that the Fourier terms themselves are not really interpretable and that we need to use trial and error to find the “right” amount of variables to use. 5.5 Other Predictor Variables The last common approach to accounting for seasonality in data is to use other predictor variables that have a matching seasonality to use them in a model. Modeling these variables against the target variable might account for the seasonality in our dataset. An example of this might be outdoor temperature modeling energy usage because they have the same 24 hour cycle. This cycle might not match during certain times of the year as well as others which might need to be accounted for in modeling the seasonality. There are some advantages and disadvantages of the external predictor variable approach to accounting for seasonality. The main advantage is that these variables can handle long and complex seasonality. In fact, for multiple seasons in a dataset, we can add more predictor variables to potentially account for this. There is also the potential of nice interpretability for these variables in the model. The disadvantages of the external predictor variable approach are that we need to use trial and error to find the “right” variables to use if there are even variables we can use in this context. 5.6 Stochastic Solution When a stochastic solution is best to solve seasonality, we need to take seasonal differences to account for the seasonality. A difference on a season is when we look at the difference between the current point and the same point in the previous season: \\(Y_t - Y_{t-S}\\). For our dataset, it can be thought of as the year over year change in our data. Let’s look at a plot of the original data vs. the seasonally differenced data. There are some limitations to differencing. Differencing is hard to evaluate for long and complex seasons due to the statistical tests for stochastic differencing typically ended at a season length of 24. Therefore, long and/or complex seasons are typically best approached with deterministic solutions. In fact, it is hard to imagine a difference being very long in terms of time points logically. For example, if you had daily data and thought you had an annual season, it is hard to imagine there is an actual impact of noon on Jan 26 from last year on noon on Jan 26 of this year. Prepare for Modeling After we remove the seasonal effect through either deterministic or stochastic approaches we need to check to see if we have stationary data for modeling. Just because we remove the seasonality doesn’t mean the data is fully stationary. For this we would use the same techniques as we saw in the section of stationarity. For this example we will just use the differenced data from our stochastic solution, but the following should be done if you want to check the deterministic solutions as well. Let’s see how to do this in our softwares! 5.6.1 R R has this functionality built in with its unitroot_ndiffs option inside of the features function. We just need to specify the variable we want to calculate this test on, which for us is the seasonal difference on the variable Passengers. To get this variable we use the mutate function to create our seasonal difference of 12 lags. The output reports the number of differences it suggests based on the unit root test. train %&gt;% mutate(Pass_diff = difference(Passengers, lag = 12)) %&gt;% features(Pass_diff, unitroot_ndiffs) ## # A tibble: 1 × 1 ## ndiffs ## &lt;int&gt; ## 1 0 5.6.2 Python Python has this functionality built in with its adfuller function inside of the statsmodels.tsa.stattools package. We just need to specify the variable we want to calculate this test on, which for us is the seasonal difference on the variable Passengers. To get this variable we use the shift function to create our seasonal difference of 12 lags. from statsmodels.tsa.stattools import adfuller train[&quot;Passengers_diff&quot;] = train[&quot;Passengers&quot;] - train[&quot;Passengers&quot;].shift(12) ## &lt;string&gt;:2: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame. ## Try using .loc[row_indexer,col_indexer] = value instead ## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy result = adfuller(train.iloc[12:207][&quot;Passengers_diff&quot;]) result ## (-3.1193806884272663, 0.0251517817351501, 15, 179, {&#39;1%&#39;: -3.4674201432469816, &#39;5%&#39;: -2.877826051844538, &#39;10%&#39;: -2.575452082332012}, 3265.477674996173) 5.6.3 SAS SAS has this functionality built in with its STATIONARITY option inside of the IDENTIFY statement from the PROC ARIMA procedure. We just need to specify the variable we want to calculate this test on, which for us is the seasonal difference on the variable Passengers. To get this variable we use the (12) specification to create our seasonal difference of 12 lags. proc arima data = train; identify var = Passengers(12) stationarity = (adf = 5); run; SAS output Based on the results above, our differenced data is stationary. Once our data is made stationary by either deterministic or stochastic approaches, the remaining data or differences are modeled with seasonal ARIMA models. The key to these new seasonal ARIMA models is that there still might be effects at seasonal lags in the dataset, even though the main seasonality is accounted for. 5.7 Seasonal ARIMA Modeling When extending the ARIMA model framework to the seasonal ARIMA framework, we add another set of terms - P, D, Q, and S. Notice how these terms are capitalized. \\[ ARIMA(p, d, q)(P, D, Q)_S \\] The terms represent the number of seasonal AR terms (\\(P\\)), the number of seasonal MA terms (\\(Q\\)), and the number of seasonal differences (\\(D\\)). The length of the season is still defined as \\(S\\). Imagine we have an \\(ARIMA(1,0,1)(2,1,0)_{12}\\) model. This would be the structure: \\[ Y_{t} - Y_{t-12} = W_t \\] \\[ W_t = \\omega + \\phi_1 W_{t-1} + \\phi_2 W_{t-12} + \\phi_3 W_{t-24} + \\theta_1 e_{t-1} + e_t \\] where \\(W_t\\) is the seasonal difference from \\(D = 1\\). The regular AR term of \\(p = 1\\) is represented by the first lag \\(W_{t-1}\\). The seasonal AR term of \\(P = 2\\) is represented by the first two seasonal lags \\(W_{t-12}\\) and \\(W_{t-24}\\). The regular MA term of \\(q=1\\) is represented as the first error lag \\(e_{t-1}\\). Seasonal ARIMA models have the same structure and approach as typical ARIMA models with AR and MA patterns in the ACF and PACF. The main difference is that the pattern is just on the seasonal lag instead of the individual lags. For example, if you had an \\(ARIMA(0,0,0)(1,0,0)_{12}\\) model, then the ACF would have an exponentially decreasing pattern every 12 lags while the PACF only had a single spike at lag 12. SAS output The pattern is the same, but it is only seen on the season since we are dealing with the seasonal lag. The opposite would be true for an \\(ARIMA(0,0,0)(0,0,1)_{12}\\) model. The PACF would have an exponentially decreasing pattern every 12 lags while the ACF only had a single spike at lag 12. SAS output The pattern is the same, but it is only seen on the season since we are dealing with the seasonal lag. For our US airlines data we can observe the lag structure on the differenced data below. ## Warning: Removed 12 rows containing missing values or values outside the scale range (`geom_line()`). ## Warning: Removed 12 rows containing missing values or values outside the scale range ## (`geom_point()`). From the above ACF and PACF plots we see a large spike in the PACF at lag 1 and an exponential decrease in the ACF within the first season. This would lend itself to a \\(p = 1\\) (AR term at lag 1) with \\(q = 0\\) (no MA term within a season). On the seasonal lags we see spikes at most of the seasons (12, 24, 36, etc.) in both plots. This kind of complicated pattern would lend itself to both a \\(P =1\\) (seasonal AR term at lag 12) and \\(Q = 1\\) (seasonal MA term at lag 12). Overall, for our data we can try the \\(ARIMA(1,0,0)(1,1,1)_{12}\\) model based on these findings. Let’s build this seasonal ARIMA model in each of our softwares! 5.7.1 R In R we can build both a specific ARIMA model and automatically select an ARIMA model using the same ARIMA function inside of the model function. To specify the specific ARIMA model we want we use the pdq and PDQ functions in the formula framework to build the regular and seasonal lags respectively. In the pdq function we specify the \\(p=1\\) piece along with the \\(d=0\\) and \\(q=0\\). In the PDQ function we are specifying the \\(P = 1\\), \\(D = 1\\), and \\(Q = 1\\) terms. To let R build its own ARIMA model through automatic selection we just use the ARIMA function on the Passengers variable without specifying the pdq and PDQ functions. model_SARIMA &lt;- train %&gt;% model( hand = ARIMA(Passengers ~ pdq(1,0,0) + PDQ(1,1,1)), auto = ARIMA(Passengers) ) With these models built we can investigate them one at a time. Below we use the select function to first select the model by built by hand and then view a summary of this model using the report function. We can isolate the same model and also use the gg_tsresiduals function to look at the residuals from this model up to lag 36 in the code below. Lastly, we look at the Ljung-Box test for white noise using the filter function to get the model we want and the features function with the ljung_box option. We have 3 degrees of freedom (dof = 3) because we have 3 terms we are estimating in our seasonal ARIMA model - \\(p = 1\\), \\(P = 1\\), and \\(Q = 1\\). model_SARIMA %&gt;% select(hand) %&gt;% report ## Series: Passengers ## Model: ARIMA(1,0,0)(1,1,1)[12] w/ drift ## ## Coefficients: ## ar1 sar1 sma1 constant ## 0.7444 0.1721 -0.7755 319.7503 ## s.e. 0.0487 0.1040 0.0753 38.5404 ## ## sigma^2 estimated as 3745598: log likelihood=-1754.8 ## AIC=3519.61 AICc=3519.93 BIC=3535.97 model_SARIMA %&gt;% select(hand) %&gt;% gg_tsresiduals(lag = 36) augment(model_SARIMA) %&gt;% filter(.model == &quot;hand&quot;) %&gt;% features(.innov, ljung_box, lag = 36, dof = 3) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hand 37.4 0.275 From the output above we can see that the estimation of the coefficients from our model summary and the residuals from the plot. We also have statistical white noise with our residuals based on the results from the Ljung-Box test. Let’s use the same code as above to view the results from R’s automatically selected seasonal ARIMA model. model_SARIMA %&gt;% select(auto) %&gt;% report ## Series: Passengers ## Model: ARIMA(1,0,1)(0,1,1)[12] w/ drift ## ## Coefficients: ## ar1 ma1 sma1 constant ## 0.8801 -0.2962 -0.6785 179.8722 ## s.e. 0.0454 0.0950 0.0600 34.0147 ## ## sigma^2 estimated as 3639496: log likelihood=-1751.67 ## AIC=3513.34 AICc=3513.66 BIC=3529.7 model_SARIMA %&gt;% select(auto) %&gt;% gg_tsresiduals(lag = 36) augment(model_SARIMA) %&gt;% filter(.model == &quot;auto&quot;) %&gt;% features(.innov, ljung_box, lag = 36, dof = 3) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 auto 29.4 0.648 From the output above we can see that the estimation of the coefficients from our model summary as well as what model R selected automatically - \\(ARIMA(1,0,1)(0,1,1)_{12}\\). We also have statistical white noise with our residuals based on the results from the Ljung-Box test. Let’s forecast both of these models using the the forecast function with the h = 12 option to forecast 12 months into the future. We also plot these two models’ forecasts against each other. model_SARIMA_for &lt;- fabletools::forecast(model_SARIMA, h = 12) model_SARIMA_for %&gt;% autoplot(train) + autolayer(fitted(model_SARIMA), col = &quot;blue&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = as_date(&quot;2007-03-15&quot;),color=&quot;orange&quot;,linetype=&quot;longdash&quot;) ## Plot variable not specified, automatically selected `.vars = .fitted` Let’s take both of these models to the test dataset to see which forecasts better since both of these were rather comparable on the training dataset. For this we use the accuracy function from the fabletools library. fabletools::accuracy(model_SARIMA_for, test) ## # A tibble: 2 × 10 ## .model .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 auto Test 807. 1339. 1229. 1.20 1.89 NaN NaN 0.446 ## 2 hand Test 752. 1280. 1162. 1.11 1.78 NaN NaN 0.494 Based on the output above it appears that the model built based on the ACF and PACF plots forecasts better than the automatically selected model. This isn’t always the case, but still a good example to show that automatic selection techniques are good starting points, but you should not be afraid to adjust them to see if you can get better models. This model was not as good as the previously built Holt-Winters exponentially smoothing model on the testing dataset, but again, this is not always the case. 5.7.2 Python In Python we can build both a specific ARIMA model and automatically select an ARIMA model using the same StatsForecast function from the statsforecast package. To specify the specific ARIMA model we want we use the order and seasonal_order functions in the ARIMA function to build the regular and seasonal lags respectively. In the order function we specify the \\(p=1\\) piece along with the \\(d=0\\) and \\(q=0\\). In the seasonal_order function we are specifying the \\(P = 1\\), \\(D = 1\\), and \\(Q = 1\\) terms along with the season length of 12 months (season_length = 12). We also include a drift or intercept terms in our model. To let Python build its own ARIMA model through automatic selection we just use the AutoARIMA function with only specifying the season length and drift. Then we have to tell the model we have a monthly frequency to our dataset with the freq = ‘ME’ option. Lastly, we just use the .fit function on our training dataset. model_SARIMA = StatsForecast(models = [ARIMA(order = (1, 0, 0), season_length = 12, seasonal_order = (1,1,1), include_drift = True), AutoARIMA(season_length = 12, allowdrift = True)], freq = &#39;ME&#39;) model_SARIMA.fit(df = train_sf) ## StatsForecast(models=[ARIMA,AutoARIMA]) With these models built we can investigate them one at a time. Below we use the .fitted_[0][0] function to first select the model by built by hand since it was the first model built. We then use the .model_.get() function along with different inputs to view a summary of this model. Specifically we ask for the ARIMA structure (arma), coefficients (coef), and AICc (aicc) from the model. Lastly, we look at the Ljung-Box test for white noise using the residuals option to get the residuals we want and the acorr_ljungbox function for the test itself. We have 3 degrees of freedom (model_df = 3) because we have 3 terms we are estimating in our seasonal ARIMA model - \\(p = 1\\), \\(P = 1\\), and \\(Q = 1\\). model_SARIMA.fitted_[0][0].model_.get(&quot;arma&quot;) ## (1, 0, 1, 1, 12, 0, 1) model_SARIMA.fitted_[0][0].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: 0.6391833439388491, &#39;sar1&#39;: 0.030766527668189745, &#39;sma1&#39;: -0.6771949561412464, &#39;drift&#39;: 123.67179487179487} model_SARIMA.fitted_[0][0].model_.get(&quot;aicc&quot;) ## 3528.358903437652 sm.stats.acorr_ljungbox(model_SARIMA.fitted_[0][0].model_.get(&quot;residuals&quot;), lags = [36], model_df = 3) ## lb_stat lb_pvalue ## 36 72.651681 0.000083 From the output above we can see that the estimation of the coefficients from our model summary. We also have statistical white noise with our residuals based on the results from the Ljung-Box test. Let’s use the same code as above to view the results from Python’s automatically selected seasonal ARIMA model represented with the .fitted_[0][1] model. model_SARIMA.fitted_[0][1].model_.get(&quot;arma&quot;) ## (1, 0, 0, 2, 12, 0, 1) model_SARIMA.fitted_[0][1].model_.get(&quot;coef&quot;) ## {&#39;ar1&#39;: 0.7259352846043337, &#39;sma1&#39;: -0.612998668446738, &#39;sma2&#39;: -0.06770550890976594} model_SARIMA.fitted_[0][1].model_.get(&quot;aicc&quot;) ## 3575.825387284918 sm.stats.acorr_ljungbox(model_SARIMA.fitted_[0][1].model_.get(&quot;residuals&quot;), lags = [36], model_df = 3) ## lb_stat lb_pvalue ## 36 52.813322 0.015734 From the output above we can see that the estimation of the coefficients from our model summary as well as what model Python selected automatically - \\(ARIMA(1,0,0)(0,1,2)_{12}\\). We also do not have statistical white noise with our residuals based on the results from the Ljung-Box test. Let’s forecast both of these models using the the forecast function with the h = 12 option to forecast 12 months into the future. We also plot these two models’ forecasts against each other. model_SARIMA_for = model_SARIMA.forecast(df = train_sf, h = 12, level = [95]) model_SARIMA.plot(train_sf, model_SARIMA_for) Let’s take both of these models to the test dataset to see which forecasts better since both of these were rather comparable on the training dataset. error = np.array(test[&#39;Passengers&#39;]) - np.array(model_SARIMA_for[&#39;AutoARIMA&#39;]) MAPE = np.mean(abs(error)/test[&#39;Passengers&#39;])*100 print(&quot;Auto. MAE =&quot;, np.mean(abs(error)), &quot;\\nAuto. MAPE =&quot;, MAPE) ## Auto. MAE = 4475.8896484375 ## Auto. MAPE = 6.97691719464673 error = np.array(test[&#39;Passengers&#39;]) - np.array(model_SARIMA_for[&#39;ARIMA&#39;]) MAPE = np.mean(abs(error)/test[&#39;Passengers&#39;])*100 print(&quot;Hand MAE =&quot;, np.mean(abs(error)), &quot;\\nHand MAPE =&quot;, MAPE) ## Hand MAE = 1249.953125 ## Hand MAPE = 1.9164337911544131 Based on the output above it appears that the model built based on the ACF and PACF plots forecasts better than the automatically selected model. This isn’t always the case, but still a good example to show that automatic selection techniques are good starting points, but you should not be afraid to adjust them to see if you can get better models. This model was not as good as the previously built Holt-Winters exponentially smoothing model on the testing dataset, but again, this is not always the case. 5.7.3 SAS In SAS we can build ARIMA models in the PROC ARIMA procedure. We can look at the IDENTIFY statement to examine the target variable. Here we are specifying the seasonal difference of the target variable using the (12) at the end of the Passengers variable in the VAR option. The MINIC option will have SAS recommend a model based on lowest BIC. The P and Q options tell SAS the bounds on the models to test with the MINIC. When running the procedure we see SAS recommend an AR term of 1 and an MA term of 12. Next, we use the ESTIMATE statements to build our models. The first estimate statement is the model we are developing by hand from looking at the ACF and PACF plots. In the P option we specify the \\(p=1\\) and \\(P = 1\\) lags by specifying the lags as (1)(12). In the Q option we specify the \\(Q = 1\\) by specifying the lags as (12). The AR terms are specified in multiplicative format and not additive format ((1, 12)) as that is what is fit by default in R and Python. The difference between this is discussed in the section below. After the ESTIMATE statement we put a FORECAST statement with LEAD equaling 12 to forecast the next 12 months as well as the OUT option to save these forecasts. The second estimate statement is the model we are developing automatically from the MINIC. In the P option we specify the \\(p=1\\) lag by specifying the lag as (1). In the Q option we specify the \\(Q = 1\\) by specifying the lags as (12). After the ESTIMATE statement we put a FORECAST statement with LEAD equaling 12 to forecast the next 12 months as well as the OUT option to save these forecasts. proc arima data = train; identify var = Passengers(12) nlag = 36 minic P = (0:12) Q = (0:12); estimate p = (1)(12) q = (12); forecast lead = 12 id = date interval = month out = work.SARIMA_hand_f; estimate p = 1 q = (12); forecast lead = 12 id = date interval = month out = work.SARIMA_auto_f; run; quit; SAS output SAS output SAS output SAS output From the output above we can see that the estimation of the coefficients from our model summaries. We also have statistical white noise with our residuals from both models based on the results from the Ljung-Box test plots above as well. We need to isolate just the forecasted observations from the code above. For that we use the DATA STEP to just look at the predicted values from the two previous models beyond the March, 2007 training observations with the WHERE statement. Next, we use the MERGE statement in another DATA STEP to merge the original test dataset with the predicted values from the above model. Lastly, we calculate the absolute error and absolute percentage error for each observation with our last DATA STEP. To get the average of these calculations, we throw our test dataset into the PROC MEANS procedure where we specify these variables with the VAR statement. data SARIMA_hand_f; set SARIMA_hand_f; where date &gt; &#39;1mar2007&#39;d; Predict_SARIMA_hand = Forecast; keep Predict_SARIMA_hand; run; data test; merge test SARIMA_hand_f; run; data SARIMA_auto_f; set SARIMA_auto_f; where date &gt; &#39;1mar2007&#39;d; Predict_SARIMA_auto = Forecast; keep Predict_SARIMA_auto; run; data test; merge test SARIMA_auto_f; run; data test; set test; AE_SARIMA_hand = abs(Passengers - Predict_SARIMA_hand); APE_SARIMA_hand = abs(Passengers - Predict_SARIMA_hand)/Passengers*100; AE_SARIMA_auto = abs(Passengers - Predict_SARIMA_auto); APE_SARIMA_auto = abs(Passengers - Predict_SARIMA_auto)/Passengers*100; run; proc means data = test; var AE_SARIMA_hand APE_SARIMA_hand AE_SARIMA_auto APE_SARIMA_auto; run; SAS output Based on the output above it appears that the model built based on the ACF and PACF plots forecasts better than the automatically selected model. This isn’t always the case, but still a good example to show that automatic selection techniques are good starting points, but you should not be afraid to adjust them to see if you can get better models. This model was not as good as the previously built Holt-Winters exponentially smoothing model on the testing dataset, but again, this is not always the case. 5.8 Multiplicative vs. Additive To look at the mathematical difference between multiplicative and additive models we need to first need to define the backshift operator, \\(B\\). The backshift operator on a variable shifts the variable back in time. For example, \\(B(Y_t) = Y_{t-1}\\). This can be extended to any number of lags. For example, a two lagged backshift is \\(B^2(Y_t) = B(Y_{t-1}) = Y_{t-2}\\). For additive seasonal terms the model is structured as the following: \\[ (1 - \\phi_1 B - \\phi_2B^{12})Y_t = e_t \\] This can be factored out in the following way: \\[ Y_t -\\phi_1 B(Y_t) - \\phi_2 B^{12}(Y_t) = e_t \\] \\[ Y_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-12} = e_t \\] \\[ Y_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-12} + e_t \\] However, the multiplicative seasonal terms the model is structured differently: \\[ (1 - \\phi_1B)(1 - \\phi_2 B^{12})Y_t = e_t \\] This can be factored out in the following way: \\[ (1 - \\phi_1 B - \\phi_2 B^{12} - \\phi_1 \\phi_2 B^{13})Y_t = e_t \\] \\[ Y_t -\\phi_1 B(Y_t) - \\phi_2 B^{12}(Y_t) - \\phi_1 \\phi_2 B^{13}(Y_t) = e_t \\] \\[ Y_t - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-12} - \\phi_1 \\phi_2 Y_{t-13}= e_t \\] \\[ Y_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-12} + \\phi_1 \\phi_2 Y_{t-13} + e_t \\] This effect on the \\(13^{th}\\) lag is the multiplied impact from the first and twelfth lag. In fact, we can see this effect when we plot the ACF and PACF plots. In a dataset with an additive seasonal effect we only see the spike on the lag. However, with a multiplicative seasonal effect we typically see what we call “counter spikes” on one or both sides of the seasonal spike as the plot shows below. SAS output This multiplicative seasonal effect is the default for R and Python, but must be specified in SAS to get. "]]
