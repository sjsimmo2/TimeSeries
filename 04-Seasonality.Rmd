# Seasonality

```{r, message=FALSE,echo=FALSE}
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
library(prophet)
file.dir = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/" 
input.file1 = "usairlines.csv"
input.file2 = "steel.csv"
input.file3 = "leadyear.csv"
input.file4 = "ebay9899.csv"
input.file5 = "fpp_insurance.csv" 
input.file6 = "ar2.csv"
input.file7 = "MA2.csv"
input.file8 = "hurrican.csv"

# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines = read.csv(paste(file.dir, input.file1,sep = ""))
Steel = read.csv(paste(file.dir, input.file2, sep = ""))
Lead.Year = read.csv(paste(file.dir, input.file3, sep = ""))
Ebay = read.csv(paste(file.dir, input.file4, sep = ""))
Quotes= read.csv(paste(file.dir, input.file5, sep = ""))
Y= read.csv(paste(file.dir, input.file6, sep = ""))
x=read.csv(paste(file.dir, input.file7, sep = ""))
hurricane=read.csv(paste(file.dir, input.file8, sep = ""))
SteelShp <- ts(Steel$steelshp, start = 1984, frequency = 12)
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)
library(reticulate)
#use_python("C:\\ProgramData\\Anaconda3\\python.exe")
use_python("C:\\ProgramData\\Anaconda3\\envs\\R_Env2\\python.exe")
```

Seasonality is the component of the time series that represents the effects of the seasonal variation within the dataset. This seasonal effect can be thought of as repetitive behavior that occurs every S time periods. Here, S is the seasonal period that gets repeated every S units of time. Remember though that seasonal data is **not** stationary. This is because the series itself does not revert to an overall, constant mean.

Let's explore the U.S. airline passenger data from 1990 to 2007.

First, we load the dataset into a time series object using the `ts` function. We use the `start =` option to define the starting point for our dataset, which is Janurary of 1990. The `frequency =` specifies the length of the seasonal period in our dataset. Our data is monthly with an annual seasonal pattern, so our frequency here is 12. From there we split our data into two pieces - training and testing - using the `subset` function. For the training set, we are subsetting the data to exclude the last 12 months. These last 12 months will be our test data set.

```{r}
# Load the Data
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)

autoplot(Passenger) + labs(title="Time Series plot for Passengers", x="Date",y="Passengers")
```

```{r}
# Create training set from overall Airlines Data
training <- subset(Passenger, end = length(Passenger)-12)

# Create test set from overall Airlines Data
test <- subset(Passenger, start = length(Passenger)-11)
```

Now, let's explore our training data by looking at a time series decomposition. This time series decomposition breaks down the data into three pieces - season, trend, and error.

```{r}
# Time Series Decomposition ...STL#
decomp_stl <- stl(Passenger, s.window = 7)

# Plot the individual components of the time series
plot(decomp_stl)
```

From the above plot we can see the annual season of our data as well as the upward trending pattern. We do have a shock to the system from the September 11 attacks. This impacted US airline travel for a number of months. We will have to account for this in our modeling as we go along. We can even see this shock in the error term of our decomposition as it resulted in large errors that were out of pattern of the rest of the data.

A great first place to start with any time series dataset is to build an exponential smoothing model. Exponential smoothing models are great first models as they are easy to build and relatively accurate. This allows us to set a baseline to try and beat with more sophisticated modeling approaches.

To build a Holt-Winters exponential smoothing model to account for both the trend and seasonality in our data, we use the `hw` function. With the `seasonal =` option we can specify whether our data has additive or multiplicative seasonality. The `initial =` option just specifies that the initial values of the HW model are optimized in the actual model building process as compared to just estimated with a simple calculation like the overall mean. The `h =` specifies that we want to forecast 12 time periods into the future on our training dataset.

```{r}
# Fit Holt-Winters ESM (multiplicative seasonality) on training data
HWES.USAir.train <- hw(training, seasonal = "multiplicative", initial='optimal', h=12)
```

Now that we have our model, let's plot the forecast as well as evaluate this forecast with our test dataset. The `mean` element in the ESM model object gives the 12 month forecast that we specified above. We can look at the difference between this forecast and our test dataset - what actually happened in those 12 months. From there we calculate the MAE and MAPE of this forecast. The `autoplot` function is used to visualize the forecast itself.

```{r}
# Calculate prediction errors from forecast
HW.error <- test - HWES.USAir.train$mean

# Calculate prediction error statistics (MAE and MAPE)
HW.MAE <- mean(abs(HW.error))
HW.MAPE <- mean(abs(HW.error)/abs(test))*100

HW.MAE
```

```{r}
HW.MAPE
```

```{r}
autoplot(HWES.USAir.train)+
  autolayer(fitted(HWES.USAir.train),series="Fitted")+ylab("Airlines Passengers")+ geom_vline(xintercept = 2007.25,color="orange",linetype="dashed")
```

From the output above, we see that we have an MAE of 1134.58 and a MAPE of 1.76%. This gives us a good baseline to try and beat with further modeling approaches.

## Seasonal Unit Root Testing

Exponential smoothing models do not require data to be stationary. However, ARIMA models do require stationary data for modeling. Since seasonal data is not stationary, we must account for this lack of stationarity in one of two broad categories of approaches - deterministic solutions and stochastic solutions.

Some examples of deterministic solutions to seasonality are seasonal dummy variables, Fourier transforms, and predictor variables. The stochastic solution to seasonality is taking a seasonal difference. Be careful, as the choice of these solutions will matter for modeling. This is why we need to evaluate which approach we need using seasonal unit root testing. Similar to trending data, unit root testing allows us to determine if differencing is the proper solution to make our seasonal data stationary. There are many tests for seasonal unit-root testing. Each test is a little different on what the null and alternative hypotheses are so we must be careful to know what we are looking it with results.

The `nsdiffs` function runs seasonal unit root tests and reports the number of seasonal differences that are needed for your dataset. If 0, that **does not** mean your data is stationary. It just means that your seasonality cannot be solved with differencing. If more than 0, then seasonal differencing is the solution to the seasonality problem of our data.

```{r}
training %>% nsdiffs()
```

```{r}
cbind("Airlines Passengers" = training,
      "Annual change in Passengers" = diff(training, 12)) %>%
  autoplot(facets=TRUE) +
  xlab("Time") + ylab("") +
  ggtitle("Comparison of Differenced Data to Original")
```

We can see from the output above that we need one seasonal difference to account for the seasonality in our dataset. That means we need to solve our seasonality stochastically. The plot shows both the original data as well as the differenced data. This is not the end of our unit root testing however. There might still be a "regular" unit root in our data. Our data does not appear to be trending, but it still might contain a unity root. Again, we will want to test for whether our data requires an additional difference or whether it is stationary.

The `ndiffs` function applied to the differenced data will help with this. The `diff` function is used with a `lag = 12` to take our seasonal difference.

```{r}
training %>% diff(lag = 12) %>% ndiffs()
```

From above we see that we require no more additional differencing. Since our data no longer appears to have a trend or season, we are ready to go to ARIMA modeling.

## Deterministic Solutions

If our dataset required a deterministic solution instead of a stochastic one, how would we go about solving this? Again, our dataset **does not** need a deterministic solution, but let's explore how we would solve seasonality deterministically for completion sake.

Unlike trend, there are many different approaches to accounting for seasonality deterministically. The approaches we will discuss are seasonal dummy variables, Fourier transforms, and predictor variables.

### Seasonal Dummy Variables

Seasonal dummy variables are exactly what their name implies. We will use a set of dummy variables to account for the seasonal effects in our dataset. For a time series with *S* periods within a season, there will be *S-1* dummy variables - one for each period (and one accounted for with the intercept). Since our dataset is monthly, we will build a dummy variable for each month and pick one to not include. For example, we could build the following model:

$$
Y_t = \beta_0 + \beta_1JAN + \beta_2FEB + \cdots + \beta_{11}NOV + e_t 
$$

In the above model, we chose to leave out the month of December's dummy variable since our model has an intercept. In fact, the intercept $\beta_0$ is the average effect of December. Each of the coefficients on the respective variables measures the average difference in effect from that month and December (our reference level).

There are a lot of ways to build month variables in a dataset. You could extract month from a time stamp variable for example. Here, we create our own month variable by repeating the values 1 to 12 across our dataset using the `rep` function. We then convert this to a factor with the `factor` function as well as set the reference level using the `relevel` function.

```{r}
# Seasonal Dummy Variables
Month <- rep(0, length(training))
Month <- Month + 1:12
```

```{r}
M <- factor(Month)
M <- relevel(M, ref="12")
```

By using an `lm` function we can see how building a linear regression with this factor variable would look. This is the exact same thing that is going on with the modeling in our data. If we were to use this factor variable in an ARIMA modeling function like `auto.arima`, the function will first build a linear regression and then evaluate which ARIMA model we need on the residuals from our model. Essentially, this "removes" the impact of the season from our dataset. The `auto.arima` function's `xreg =` option is how we incorporate any external variables (here our dummy variables) into the model. The `seasonal = FALSE` option tells the function to not try and account for season itself since we are accounting for it with seasonal dummy variables.

```{r}
Season.Lin <- lm(training ~ M)
summary(Season.Lin)
```

```{r}
M.Matrix <- model.matrix(~M)

Trend <- 1:length(training)

SD.ARIMA <- auto.arima(training, xreg = M.Matrix[,2:12], method="ML", seasonal = FALSE)
summary(SD.ARIMA)
```

There are some advantages and disadvantages to the seasonal dummy variable approach. The advantages are that the model's seasonal dummy variables have some nice interpretability. They tells us the average impact each seasonal component has on the target variable. It is also a rather straight-forward approach to implement. The main disadvantages however are that if your seasonal is especially long or complex, seasonal dummy variables are burdensome. Also, the constant effect of each season that is assumed may not be appropriate.

### Fourier Transforms

Harmonic regression using Fourier transforms is another approach to account for seasonality deterministically. Fourier showed that a series of sine and cosine terms of the right frequencies approximate periodic patterns in a data series. To do this, we add Fourier variables to a regression model to account for the seasonal pattern. The odd terms $k=1,3,5$ etc. are accounted for with sine variables:

$$
X_{k, t} = \sin(k \times \frac{2\pi t}{S})
$$

The even terms $k = 2, 4, 6$ etc. are accounted for with cosine variables:

$$
X_{k, t} = \cos(k \times \frac{2\pi t}{S})
$$

In fact, if you add the same number of Fourier variables as you have seasonal dummy variables, you will get the same predictions. However, we typically do not need all of the Fourier variables, especially with large values of *S*. We need to decide how many of these values we need. One way to approach this is to build many models, while increasing the number of Fourier terms in the models and compare them all via a metric like BIC.

Here, we loop through the first 6 Fourier terms to account for seasonality. We do this by adding the `fourier` function to the `auto.arima` function through the `xreg =` option. All of this is contained within a loop where we gradually increase the number of Fourier terms in the regression. The `K =` option in the `fourier` function does this. We loop through the first 6 Fourier terms (half of the seasonal length) and record the training BIC for each model. This BIC is reported on a plot with the 6 different forecasts (one from each model). The `gridExtra` package's `grid.arrange` function allows the plots to all be put together.

```{r}
plots <- list()
for (i in seq(6)) {
  fit <- auto.arima(training, xreg = fourier(training, K = i),
                    seasonal = FALSE, lambda = NULL)
  plots[[i]] <- autoplot(forecast::forecast(fit,
                                  xreg = fourier(training, K=i, h=12))) +
    xlab(paste("K=",i,"   BIC=",round(fit$bic,2))) +
    ylab("") + ylim(30000,80000)
}
gridExtra::grid.arrange(
  plots[[1]],plots[[2]],plots[[3]],
  plots[[4]],plots[[5]],plots[[6]], nrow=3)
```

From the above output we see that the best model had 6 Fourier terms. So we can build this model to account for the seasonality in our dataset.

```{r}
F.ARIMA <- auto.arima(training, xreg = fourier(training, K = 6), seasonal = FALSE)
summary(F.ARIMA)
```

There are some advantages and disadvantages of the Fourier term approach to accounting for seasonality. The main advantage is that Fourier terms can handle long and complex seasonality. In fact, for multiple seasons in a dataset, we can add more Fourier variables at different frequencies to account for this. The disadvantages of the Fourier approach are that the Fourier terms themselves are not really interpretable and that we need to use trial and error to find the "right" amount of variables to use.

After removing the seasonality through deterministic solutions, the remaining error term (residuals) are modeled with seasonal ARIMA models. The key to these new seasonal ARIMA models is that there still might be effects at seasonal lags in the dataset, even though the main seasonality is accounted for.

## Stochastic Solutions

The stochastic approach has been hinted at above. When a stochastic solution is best to solve seasonality, we need to take seasonal differences. A difference on a season is when we look at the difference between the current point and the same point in the previous season: $Y_t - Y_{t-S}$. For our dataset, it can be thought of as the year over year change in our data.

Here the `ggtsdisplay` function allows us to see the actual difference along with its correlation pattern with ACF and PACF plots. Notice, we are looking at these ACF and PACF plots with regards to the differences on the training data using the `diff` function with `lag = 12` specifying the length of the difference being the same as our season.

```{r}
training %>% diff(lag = 12) %>% ggtsdisplay()
```

After we remove the seasonal effect through stochastic approaches, the remaining differences are modeled with seasonal ARIMA models. The key to these new seasonal ARIMA models is that there still might be effects at seasonal lags in the dataset, even though the main seasonality is accounted for.

There are some limitations to differencing. Differencing is hard to evaluate for long and complex seasons due to the statistical tests for stochastic differencing typically ended at a season length of 24. Therefore, long and/or complex seasons are typically best approached with deterministic solutions. In fact, it is hard to imagine a difference being very long in terms of time points logically. For example, if you had daily data and thought you had an annual season, it is hard to imagine there is an actual impact of Jan 26 from last year on Jan 26 of this year.

## Seasonal ARIMA Modeling

When extending the ARIMA model framework to the seasonal ARIMA framework, we add another set of terms - P, D, Q, and S. Notice how these terms are capitalized.

$$
ARIMA(p, d, q)(P, D, Q)_S
$$

The terms represent the number of *seasonal* AR terms ($P$), the number of *seasonal* MA terms ($Q$), and the number of *seasonal* differences ($D$). The length of the season is still defined as $S$. Seasonal ARIMA models have the same structure and approach as typical ARIMA models with AR and MA patterns in the ACF and PACF. THe main difference is that the pattern is just on the seasonal lag instead of the individual lags. For example, if you had an $ARIMA(0,0,0)(1,0,0)_{12}$ model, then the ACF would have an exponentially decreasing pattern *every 12 lags* while the PACF only had a single spike *at lag 12*. The pattern is the same, but it is only seen on the season since we are dealing with the seasonal lag. The opposite would be true for an $ARIMA(0,0,0)(0,0,1)_{12}$ model. The PACF would have an exponentially decreasing pattern *every 12 lags* while the ACF only had a single spike *at lag 12*.

For our data we can try the $ARIMA(1,0,0)(1,1,1)_{12}$ model using the `Arima` function. With either the `ggtsdisplay` function on the residuals, or the `checkresiduals` function on the model object, we can see that the model still doesn't capture all of the signal and pattern in the dataset.

```{r}
# Seasonal ARIMA 
training %>% 
  Arima(order=c(1,0,0), seasonal=c(1,1,1)) %>%
  residuals() %>% ggtsdisplay()
```

```{r}
S.ARIMA <- Arima(training, order=c(1,0,0), seasonal=c(1,1,1))
summary(S.ARIMA)
```

```{r}
checkresiduals(S.ARIMA)
```

We can still use the `auto.arima` function to select the "best" starting point model for us as well. We can use the original data in this function along with the `seasonal = TRUE` option to allow the function to take the seasonal difference for us.

```{r}
S.ARIMA <- auto.arima(training, method="ML", seasonal = TRUE)
summary(S.ARIMA)
```

```{r}
checkresiduals(S.ARIMA)
```

Above we can see that the `auto.arima` function selected the $ARIMA(1,0,1)(0,1,1)_{12}$ model. By using the `checkresiduals` function we see that we are left with white noise after our modeling. We still have a large outlier that we will have to account for in the next section on dynamic regression.

We can use the `forecast` function with `h = 12` to forecast the next 12 observations in our dataset. Similar to the exponential smoothing model above, we plot the forecast as well as evaluate the MAE and MAPE from this model.

```{r}
forecast::forecast(S.ARIMA, h = 12)
```

```{r}
autoplot(forecast::forecast(S.ARIMA, h = 12)) + autolayer(fitted(S.ARIMA), series="Fitted") + 
  ylab("Airlines Passengers") + 
  geom_vline(xintercept = 2007.25,color="orange",linetype="dashed")
```

```{r}
S.ARIMA.error <- test - forecast::forecast(S.ARIMA, h = 12)$mean

# Calculate prediction error statistics (MAE and MAPE)
S.ARIMA.MAE <- mean(abs(S.ARIMA.error))
S.ARIMA.MAPE <- mean(abs(S.ARIMA.error)/abs(test))*100

S.ARIMA.MAE
```

```{r}
S.ARIMA.MAPE
```

From the above output we see that our seasonal ARIMA did not beat the Holt-Winters exponential smoothing model in terms of MAE or MAPE. That outlier might be impacting the estimation of our model, so we will have to address that.

The default for the `Arima` function in R (which is what `auto.arima` is built off of) uses the multiplicative approach to seasonal ARIMA models. To get an additive seasonal ARIMA model instead we need to use the `fixed =` option in the `Arima` function to specify exactly what terms we want to estimate instead as shown below. The zeroes are for terms we *do not* want to estimate, while the *NA* values will be estimated by the function.

```{r}
# Additive Seasonal ARIMA  
S.ARIMA <- Arima(training, order=c(1,0,13), seasonal=c(0,1,0),
                 fixed=c(NA,NA,0,0,0,0,0,0,0,0,0,0,NA,NA), method="ML",)
summary(S.ARIMA)
```

## Python Code for Seasonality

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.graphics import tsaplots
from statsmodels.graphics import tsaplots
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.arima_model import ARMA
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing
```

```{python}
usair = pd.read_csv("https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/usairlines.csv")

df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair.index=pd.to_datetime(df)

training = usair.head(207)
test = usair.tail(12)
```

```{python}
plt.plot(training["Passengers"])
plt.xlabel("Date")
plt.ylabel("Passengers")
plt.title("Airline Passengers")
plt.show()
```

```{python}
training['HWES_MUL'] = ExponentialSmoothing(training['Passengers'], trend='mul', seasonal='mul', seasonal_periods = 12).fit().fittedvalues

training[['Passengers', 'HWES_MUL']].plot(title='Holt Winters Exponential Smoothing: Additive and Multiplicative Seasonality')
plt.show()
```

```{python}
fit = ExponentialSmoothing(training['Passengers'],trend='mul',seasonal='mul',seasonal_periods=12).fit()

plt.cla()
fcast=fit.forecast(12)
ax = training["Passengers"].plot(figsize=(12,8))
fcast.plot(ax = ax,color = "orange")
plt.show()
```

```{python}
error=test['Passengers']-fcast
MAPE=np.mean(abs(error)/test['Passengers'])*100
MAPE
```

### Seasonality

```{python}
from statsmodels.tsa.stattools import adfuller

training["Passengers_diff"] = training["Passengers"] - training["Passengers"].shift(12)

plt.cla()
plt.plot(training["Passengers_diff"].dropna())
plt.xlabel("Date")
plt.ylabel("Passengers")
plt.title("Seasonal Difference in Airline Passengers")
plt.show()
```

```{python}
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import kpss

adfuller(training["Passengers_diff"].dropna())
kpss(training["Passengers_diff"].dropna())
```

### Deterministic Solutions

```{python}
results = sm.OLS.from_formula("Passengers ~ C(Month)",
                              data=training).fit()
print(results.summary())
```

```{python}
import pmdarima as pm

model = pm.auto_arima(results.resid.dropna(), start_p = 0, start_q = 0, max_p = 5, max_q = 5, seasonal = False)
model.summary()
```

```{python}
training['t'] = range(1, 208, 1)

training['x1'] = np.sin(2*np.pi*training['t']/12)
training['x2'] = np.cos(2*np.pi*training['t']/12)
training['x3'] = np.sin(4*np.pi*training['t']/12)
training['x4'] = np.cos(4*np.pi*training['t']/12)
training['x5'] = np.sin(6*np.pi*training['t']/12)
training['x6'] = np.cos(6*np.pi*training['t']/12)
training['x7'] = np.sin(8*np.pi*training['t']/12)
training['x8'] = np.sin(8*np.pi*training['t']/12)
training['x9'] = np.sin(10*np.pi*training['t']/12)
training['x10'] = np.sin(10*np.pi*training['t']/12)
training['x11'] = np.sin(12*np.pi*training['t']/12)
training['x12'] = np.sin(12*np.pi*training['t']/12)

results = sm.OLS.from_formula("Passengers ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x12",
                              data=training).fit()
print(results.summary())

model = pm.auto_arima(results.resid.dropna(), start_p = 0, start_q = 0, max_p = 5, max_q = 5, seasonal = False)
model.summary()
```

### Stochastic Solution

```{python}
df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair.index=pd.to_datetime(df)

training = usair.head(207)
test = usair.tail(12)

training["Passengers_diff"] = training["Passengers"] - training["Passengers"].shift(12)

plt.cla()
plt.plot(training["Passengers_diff"].dropna())
plt.xlabel("Date")
plt.ylabel("Passengers")
plt.title("Seasonal Difference in Airline Passengers")
plt.show()
```

```{python}
from matplotlib import pyplot
from statsmodels.graphics.tsaplots import plot_acf

plot_acf(training['Passengers_diff'].dropna(), lags = 36)
pyplot.show()
```

```{python}
from statsmodels.graphics.tsaplots import plot_pacf

plot_pacf(training['Passengers_diff'].dropna(), lags = 36)
pyplot.show()
```

### Seasonal ARIMA

```{python}
from statsmodels.tsa.statespace.sarimax import SARIMAX

model2 = SARIMAX(training['Passengers'], order = (1,0,0), seasonal_order = (1,1,1,12)).fit()
model2.summary()
```

```{python}
lag_test=[4,5,6,7,8,9,10,11,12,13]

## Not run
#for x in lag_test:
#  sm.stats.acorr_ljungbox(model.resid, lags=[x], model_df=2)
```

```{python}
plot_acf(model2.resid, lags = 36)
pyplot.show()
```

```{python}
plot_pacf(model2.resid, lags = 36)
pyplot.show()
```

```{python}
model3 = pm.auto_arima(training['Passengers'], start_p = 0, start_q = 0, max_p = 5, max_q = 5, seasonal = True, m = 12)
model3.summary()
```

```{python}
model3 = SARIMAX(training['Passengers'], order = (1,0,1), seasonal_order = (0,1,1,12), trend = 't').fit()
model3.summary()

plt.cla()
fcast2 = model3.forecast(12)
ax = training["Passengers"].plot(figsize=(12,8))
fcast2.plot(ax = ax,color = "orange")
plt.show()
```

```{python}
error2 = test['Passengers']-fcast2
MAPE2 = np.mean(abs(error2)/test['Passengers'])*100
MAPE2
```
