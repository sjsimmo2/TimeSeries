# ARIMA

We will not be getting into ARIMA.  This chapter is fairly long and covers many different concepts in ARIMA.

```{r, message=FALSE,echo=FALSE}
library(readr)
library(fpp3)
library(fable)
library(fabletools)
library(lubridate)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)

library(reticulate)

use_python("C:\\ProgramData\\Anaconda3\\python.exe")

file.dir = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/" 
input.file1 = "usairlines.csv"
input.file2 = "steel.csv"
input.file3 = "leadyear2.csv"
input.file4 = "ebay9899.csv"
input.file5 = "fpp_insurance.csv" 
input.file6 = "ar2.csv"
input.file7 = "MA2.csv"
input.file8 = "hurrican.csv"
input.file9 = "consume1982.csv"
input.file10 = "Raleighhouse.csv"

# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines = read_csv(paste(file.dir, input.file1,sep = ""),show_col_types = FALSE)
Steel = read_csv(paste(file.dir, input.file2, sep = ""),show_col_types = FALSE)
Lead.Year = read_csv(paste(file.dir, input.file3, sep = ""),show_col_types = FALSE)
Ebay = read_csv(paste(file.dir, input.file4, sep = ""),show_col_types = FALSE)
Quotes= read_csv(paste(file.dir, input.file5, sep = ""),show_col_types = FALSE)
Y= read_csv(paste(file.dir, input.file6, sep = ""),show_col_types = FALSE)
x=read_csv(paste(file.dir, input.file7, sep = ""),show_col_types = FALSE)
hurricane=read_csv(paste(file.dir, input.file8, sep = ""),show_col_types = FALSE)
temp<-read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\minntemp.csv", show_col_types = FALSE)
consume <- read_csv(paste(file.dir, input.file9, sep = ""),show_col_types = FALSE)
Raleigh <- read_csv(paste(file.dir, input.file10, sep = ""),show_col_types = FALSE)


```



## Stationarity

Before we can try to model the dependency structure (the AR and MA terms), we must first have a stationary series!  The ADF test is one of the most well-known and accepted test for testing stationarity.  However, others have also been proposed.  Within this document, we will be using the KPSS test.


```{r plot}

Quotes.ts<-Quotes |> mutate(date = seq(ymd('2002-01-01'),ymd('2005-04-01'),by='months')) |>
mutate(month=yearmonth(date)) |> as_tsibble(index=month)

Quotes_train<-Quotes.ts %>% filter(year(date)<2005)

autoplot(Quotes_train,Quotes)+labs(title="Time Series of Monthly Stock quotes", x="Time", y="Quotes")
```


The following code looks into stationarity.


```{r stationarity}


# Perform the KPSS test 
Quotes_train |>
 features(Quotes, unitroot_kpss)

Quotes_train |>
 features(Quotes, unitroot_ndiffs)
```


## Correlation Functions


The Acf and the Pacf in R will calculate the autocorrelation (up to the lag you specify) and the partial autocorrelation, respectively.  


```{r correlation functions}
ggAcf(Quotes_train$Quotes,lag=10)
ggPacf(Quotes_train$Quotes,lag=10)
```


Since the Hurricane data set needs a difference to be stationary, we will first create the difference column and explore the correlations in that variable.

```{r}
Hurricane.ts<- hurricane %>% as_tsibble(index=Year)

Hurricane_train <-Hurricane.ts %>% filter(Year <2000)

autoplot(Hurricane_train,MeanVMax)+labs(title="Time Series of Yearly Mean Velocity for Hurricanes", x="Time", y="MPH")

Hurricane_train %>% features(MeanVMax,unitroot_ndiffs)

Hurricane_train <- Hurricane_train %>% mutate(mean_diff=difference(MeanVMax))
Hurricane_train %>% features(mean_diff,unitroot_ndiffs)

autoplot(Hurricane_train,mean_diff)+labs(title="Differenced Mean Max Velocity", x="Time", y="Difference")

```



## AutoRegressive Models (AR)

AutoRegressive (AR) models involve modeling the lags of Y.  We can write an autoregressive model as

$$ Y_{t} = c + \phi_{1}Y_{t-1}+\phi_{2}Y_{t-2}+...\phi_{p}Y_{t-p}+\epsilon_{t} $$
Where there are *p* lags of *Y*.  Below is the code to fit an AR(2) model. The *order* in the *Arima* function needs the p,d,q values (p=# of AR terms, d=how many differences should be taken and q=# of MA terms).

```{r AR2}

ggAcf(Y[1:731,])
ggPacf(Y[1:731,])

Y.1 <-data.frame(Y)

Y.ts<-Y.1 %>% mutate(date = seq(ymd('2000-01-01'),ymd('2002-09-26'),by='day'))  %>% as_tsibble(index=date)

Y_train <- Y.ts %>% filter(year(date)<2002)

autoplot(Y_train,Y)+labs(title="Time Series of Simulated Daily series", x="Time", y="Values")

Y.ARIMA <- Y_train %>% model(ARIMA(Y~pdq(2,0,0)+PDQ(0,0,0)))
report(Y.ARIMA)

Y.ARIMA %>% residuals() %>% ggAcf()
Y.ARIMA %>% residuals() %>% ggPacf()

```


## Moving Average model (MA)

Moving average (MA) models involve modeling the lags of the error.  We can write a moving average model as

$$ Y_{t} = c - \theta_{1}\epsilon_{t-1}-\theta_{2}\epsilon_{t-2}-...\theta_{q}\epsilon_{t-q}+\epsilon_{t} $$
Where there are *q* lags of $\epsilon$.  Below is code to fit an MA(2) model.

```{r MA2}

ggAcf(x[1:74,])
ggPacf(x[1:74,])

x.1 <-data.frame(x)

x.ts<-x.1 %>% mutate(date = seq(ymd('2000-01-01'),ymd('2000-4-9'),by='day'))  %>% as_tsibble(index=date)

x_train <- x.ts %>% filter(date < '2000-3-15')

autoplot(x_train,x)+labs(title="Time Series of Simulated Daily series", x="Time", y="Values")

x.ARIMA <- x_train %>% model(ARIMA(x~pdq(0,0,2)+PDQ(0,0,0)))
report(x.ARIMA)

x.ARIMA %>% residuals() %>% ggAcf()
x.ARIMA %>% residuals() %>% ggPacf()


```


## White noise

For residuals to exhibit white noise, they must be "independent" and normally distributed with mean 0 and constant variance.  You already know how to assess normality and constant variance, however, we need to focus on assessing "independence".  We can assess if there is significant dependence through the Ljung-Box test (or graphically through ACF and PACF plots). The hypotheses being tested are

$$H_{0}:No\quad significant\quad autocorrelation\\
H_{A}:Significant\qquad autocorrletion $$

This should be assessed on a *stationary* time series.  Looking at a stationary time series, going back 10 lags should be sufficient (this will be different when we get to seasonal models).  Keep in mind that sample size does matter when assessing significance (adjust significance level accordingly).

```{r Ljung-Box test}

### Before fitting model:

ljung_box(Y[1:731,], lag = 10, dof=0)

## Note: Y is a vector

### After fitting model:

augment(Y.ARIMA) %>% features(.innov,ljung_box, lag=10, dof = 2)


## Note that dof has changed!!
```


## Examples

We will now demonstrate these ideas on two different examples: 



First example is the Quotes data set:

```{r}
Quotes.ts<-Quotes |> mutate(date = seq(ymd('2002-01-01'),ymd('2005-04-21'),by='months')) |>
mutate(month=yearmonth(date)) |> as_tsibble(index=month)

Quotes_train<-Quotes.ts %>% filter(year(date)<2005)

autoplot(Quotes_train,Quotes)+labs(title="Time Series of Monthly Stock quotes", x="Time", y="Quotes")

Quotes_train %>% gg_tsdisplay(Quotes,plot_type = 'partial')
```

We will try AR(1), MA(1) and perform two automatic searches:


```{r}
quotes_model <-Quotes_train %>%
  model(ar1 = ARIMA(Quotes ~ pdq(1,0,0) + PDQ(0,0,0)),
        ma1 = ARIMA(Quotes ~ pdq(0,0,1) + PDQ(0,0,0)),
        search1 = ARIMA(Quotes),
        search2 = ARIMA(Quotes,stepwise = F))

quotes_model2<-as.data.frame(quotes_model)
t(quotes_model2)

glance(quotes_model) %>% arrange(AICc) %>% select(.model:BIC)
```


Choosing the search1 model, we will look at residuals for white noise (no spikes in correlation plots and not significant for the Ljung-Box test):

```{r}
quotes_model %>% select(search1) %>% residuals() %>% ggAcf()
quotes_model %>% select(search1) %>% residuals() %>% ggPacf()
quotes_model %>% select(search1) %>% gg_tsresiduals()

augment(quotes_model) %>% filter(.model=='search1') %>% features(.innov,ljung_box, lag=10, dof = 2)
```



Second example is the Hurricane data set (mean Maximum velocity):

```{r}
Hurricane.ts<- hurricane %>% as_tsibble(index=Year)

Hurricane_train <-Hurricane.ts %>% filter(Year <2000)

autoplot(Hurricane_train,MeanVMax)+labs(title="Time Series of Yearly Mean Velocity for Hurricanes", x="Time", y="MPH")

Hurricane_train %>% features(MeanVMax,unitroot_ndiffs)
```



```{r}


Hurricane_train <- Hurricane_train %>% mutate(mean_diff=difference(MeanVMax))

Hurricane_train %>% gg_tsdisplay(mean_diff,plot_type = 'partial')
```


```{r}
hurr_model <-Hurricane_train %>%
  model(ar3 = ARIMA(MeanVMax ~ 0 + pdq(3,1,0) + PDQ(0,0,0)),
        ma2 = ARIMA(MeanVMax ~ 0 + pdq(0,1,2) + PDQ(0,0,0)),
        arima32 = ARIMA(MeanVMax~0 + pdq(3,1,2) + PDQ(0,0,0)),
        search1 = ARIMA(MeanVMax),
        search2 = ARIMA(MeanVMax,stepwise = F))

hurr_model2<-as.data.frame(hurr_model)
t(hurr_model2)

glance(hurr_model) %>% arrange(AICc) %>% select(.model:BIC)
```

Looking at the ACF and PACF on the residuals of MA(2) model.

```{r}
hurr_model %>% select(ma2) %>% residuals() %>% ggAcf()
hurr_model %>% select(ma2) %>% residuals() %>% ggPacf()
hurr_model %>% select(ma2) %>% gg_tsresiduals()


```

Looking at the ACF and PACF on the ARIMA(3,1,2) model and Ljung-Box test.

```{r}
hurr_model %>% select(arima32) %>% residuals() %>% ggAcf()
hurr_model %>% select(arima32) %>% residuals() %>% ggPacf()
hurr_model %>% select(arima32) %>% gg_tsresiduals()

augment(hurr_model) %>% filter(.model=='arima32') %>% features(.innov,ljung_box, lag=10, dof = 5)
```

## Forecasting

Now let's forecast each of our models:



```{r}

quotes_model %>% select(search1) %>% fabletools::forecast(h=4) %>% autoplot(Quotes_train)

quotes_for<-quotes_model %>% select(search1) %>% fabletools::forecast(h=4)

quotes_resid<-Quotes$Quotes[37:40]-quotes_for$.mean
MAPE<-mean(abs(quotes_resid/Quotes$Quotes[37:40]))
MAE<-mean(abs(quotes_resid))
MAPE
MAE

```




```{r}
hurr_model %>% select(arima32) %>% fabletools::forecast(h=8) %>% autoplot(Hurricane_train)

hurr_for<-hurr_model %>% select(arima32) %>% fabletools::forecast(h=8)

hurr_resid<-hurricane$MeanVMax[150:157]-hurr_for$.mean
MAPE<-mean(abs(hurr_resid/hurricane$MeanVMax[150:157]))
MAE<-mean(abs(hurr_resid))
MAPE
MAE
```

## Trend


We will now take a look at trending time series.  We will use the Consume and Raleigh Housing prices index as two examples.

Consumer example using differences for trend:

```{r}
consume.ts<- consume |> mutate(date2=my(date))|> mutate(month=yearmonth(date2)) |> as_tsibble(index=month)

consume_train<-consume.ts %>% filter(year(date2)<1990)

autoplot(consume_train,Disposable_income)+labs(title="Time Series of Monthly Disposable Income", x="Time", y="Thousands of Dollars")

ndiffs(consume_train$Disposable_income)

consume_train<- consume_train %>% 
  mutate(income_diff = difference(Disposable_income))

autoplot(consume_train,income_diff)+labs(title="Time Series of Differenced Monthly Disposable Income", x="Time", y="Differences")

consume_train %>% gg_tsdisplay(income_diff,plot_type = 'partial')
```

Looking at the ACF and PACF, we will try a few models and also use an automatic search.

```{r}
consume_model <-consume_train %>%
  model(ar1 = ARIMA(Disposable_income ~ pdq(1,1,0) + PDQ(0,0,0)),
        ma1 = ARIMA(Disposable_income ~ pdq(0,1,1) + PDQ(0,0,0)),
        ar6 = ARIMA(Disposable_income ~ pdq(6,1,0) + PDQ(0,0,0)),
        ma6 = ARIMA(Disposable_income ~ pdq(0,1,6) + PDQ(0,0,0)),
        search1 = ARIMA(Disposable_income),
        search2 = ARIMA(Disposable_income,stepwise = F))

consume_model2<-as.data.frame(consume_model)
t(consume_model2)

glance(consume_model) %>% arrange(AICc) %>% select(.model:BIC)


```


Selecting the ARIMA(6,1,0) model.  White noise looks good and forecast is good.

```{r}
consume_model %>% select(ar6) %>% residuals() %>% ggAcf(lag.max = 10)
consume_model %>% select(ar6) %>% residuals() %>% ggPacf(lag.max = 10)
consume_model %>% select(ar6) %>% gg_tsresiduals()

augment(consume_model) %>% filter(.model=='ar6') %>% features(.innov,ljung_box, lag=10, dof = 6)

pred_ar6 <- consume_model %>% select(ar6) %>%
  fabletools::forecast(h=6)

error_ar6 <- consume$Disposable_income[89:94] - pred_ar6$.mean
MAPE_ar6 <-mean(abs(error_ar6/consume$Disposable_income[89:94]))
MAE_ar6 <- mean(abs(error_ar6))

consume_model %>% select(ar6) %>% fabletools::forecast(h=6) %>% autoplot(consume_train)

pred_ma1 <- consume_model %>% select(ma1) %>%
  fabletools::forecast(h=6)

error_ma1 <- consume$Disposable_income[89:94] - pred_ma1$.mean
MAPE_ma1 <-mean(abs(error_ma1/consume$Disposable_income[89:94]))
MAE_ma1 <- mean(abs(error_ma1))

consume_model %>% select(ma1) %>% fabletools::forecast(h=6) %>% autoplot(consume_train)

```

Raleigh example using differences: 

```{r}

Raleigh.ts<- Raleigh %>% mutate(quarter=yearquarter(DATE)) %>% as_tsibble(index=quarter)

Raleigh_train <-Raleigh.ts %>% filter(quarter <yearquarter("2023 Q1"))

autoplot(Raleigh_train,price_index)+labs(title="Time Series of Quarterly Housing price Index for Raleigh-Cary", x="Time", y="Index")

Raleigh_train %>% features(price_index,unitroot_ndiffs)

Raleigh_train <- Raleigh_train %>% mutate(diff_price=difference(difference(price_index)))

Raleigh_train %>% gg_tsdisplay(diff_price,plot_type = 'partial')

```

Now we will try a few models:


```{r}
Raleigh_model <-Raleigh_train %>%
  model(
        ma5 = ARIMA(price_index ~ pdq(0,2,5)+ PDQ(0,0,0)+0),
        ar2 = ARIMA(price_index ~ pdq(2,2,0)+ PDQ(0,0,0)+0),
        ma2 = ARIMA(price_index ~ pdq(0,2,2)+ PDQ(0,0,0)+0),
        search1 = ARIMA(price_index~PDQ(0,0,0)),
        search2 = ARIMA(price_index,stepwise = FALSE)
        )

Raleigh_model2<-as.data.frame(Raleigh_model)
t(Raleigh_model2)

glance(Raleigh_model) %>% arrange(AICc) %>% select(.model:BIC)
```


Looking at residuals:


```{r}
Raleigh_model %>% select(search1) %>% residuals() %>% ggAcf(lag.max = 10)
Raleigh_model %>% select(search1) %>% residuals() %>% ggPacf(lag.max = 10)
Raleigh_model %>% select(search1) %>% gg_tsresiduals()

augment(Raleigh_model) %>% filter(.model=='search1') %>% features(.innov,ljung_box, lag=10, dof = 4)
```

Forecasting chosen model onto the validatoin data set:

```{r}
pred_arima123 <- Raleigh_model %>% select(search1) %>%
  fabletools::forecast(h=5)

error_arima123 <- Raleigh.ts$price_index[93:97] - pred_arima123$.mean
MAPE_arima123 <-mean(abs(error_arima123/Raleigh.ts$price_index[93:97]))
MAE_arima123 <- mean(abs(error_arima123))

Raleigh_model %>% select(search1) %>% fabletools::forecast(h=5) %>% autoplot(Raleigh_train)
```



```{r}
pred_data <- tibble(
  quarter = yearquarter(seq.Date(from = as.Date("2023-01-01"),
                                 to = as.Date("2024-01-01"),
                                 by = "quarter")),
  value = pred_arima123$.mean  
)

test_data <- tibble(
  quarter = yearquarter(seq.Date(from = as.Date("2023-01-01"),
                                 to = as.Date("2024-01-01"),
                                 by = "quarter")),
  value = Raleigh.ts$price_index[93:97]  
)

ggplot() +
  geom_line(data = test_data, aes(x = quarter, y = value), color = "blue", linetype = "solid") +
  geom_line(data = pred_data, aes(x = quarter, y = value), color = "orange", linetype = "dashed") +
  labs(title = "Predicted versus Actual values",
       x = "Quarter", y = "Price Index") +
  theme_minimal()
```


Using the consumer data set and fitting a linear trend line:

```{r}

consume_linear <-consume_train %>%
  model(trend1 = ARIMA(Disposable_income~ trend() + pdq(0,0,0) + PDQ(0,0,0)+1)
        )
report(consume_linear)

fitted_values <- fitted(consume_linear)

# Plot the original data and fitted values
autoplot(consume_train, Disposable_income) +
  autolayer(fitted_values, .fitted, color = "blue", linetype = "dashed") +
  labs(title = "Fitted Values from Linear Regression Model for Disposable Income",
       x = "Time", y = "Dollars (000)") +
  theme_minimal()

consume_linear %>% residuals() %>% ggAcf(lag.max = 10)
consume_linear %>% residuals() %>% ggPacf(lag.max = 10)


```





```{r}

consume_linear <-consume_train %>%
  model(trend1 = ARIMA(Disposable_income~ trend() + pdq(6,0,0) + PDQ(0,0,0)+1),
        trend2 = ARIMA(Disposable_income ~ trend() + PDQ(0,0,0) +1)
        )
consume_linear2<-as.data.frame(consume_linear)
t(consume_linear2)

glance(consume_linear) %>% arrange(AICc) %>% select(.model:BIC)

```

Comparing the random walk with drift to the linear trend time series model for the consumer data set:

```{r}
consume_linear %>% select(trend1) %>% residuals() %>% ggAcf(lag.max = 10)
consume_linear %>% select(trend1) %>% residuals() %>% ggPacf(lag.max = 10)
consume_linear %>% select(trend1) %>% gg_tsresiduals()

augment(consume_linear) %>% filter(.model=='trend1') %>% features(.innov,ljung_box, lag=10, dof = 6)

pred_lm <- consume_linear %>% select(trend1) %>%
  fabletools::forecast(h=6)

consume_linear %>% select(trend1) %>% fabletools::forecast(h=6) %>% autoplot(consume_train)

error_lm <- consume$Disposable_income[89:94] - pred_lm$.mean
MAPE_lm <-mean(abs(error_lm/consume$Disposable_income[89:94]))
MAE_lm <- mean(abs(error_lm))


pred_rw <- tibble(
  month = yearmonth(seq.Date(from = as.Date("1990-01-01"),
                                 to = as.Date("1990-06-01"),
                                 by = "month")),
  value = pred_ar6$.mean  
)

pred_lm2 <- tibble(
  month = yearmonth(seq.Date(from = as.Date("1990-01-01"),
                                 to = as.Date("1990-06-01"),
                                 by = "month")),
  value = pred_lm$.mean  
)

test_data <- tibble(
  month = yearmonth(seq.Date(from = as.Date("1990-01-01"),
                                 to = as.Date("1990-06-01"),
                                 by = "month")),
  value = consume.ts$Disposable_income[89:94]  
)

combined_data <- bind_rows(
  test_data %>% mutate(Line = "Actual"),
  pred_rw %>% mutate(Line = "Random Walk"),
  pred_lm2 %>% mutate(Line = "Linear Model")
)

# Plot the data with a legend
ggplot(combined_data, aes(x = month, y = value, color = Line)) +
  geom_line(linetype = "solid") +
  labs(title = "Predicted versus Actual values",
       x = "Date", y = "Disposable Income (000)",
       color = "Legend") +
  theme_minimal()



```

Fitting a trend line to the Raleigh data set:


```{r}

Raleigh_linear <-Raleigh_train %>%
  model(trend1 = ARIMA(price_index~ trend() + pdq(0,0,0) + PDQ(0,0,0)+1)
    )
Raleigh_linear %>% residuals() %>% ggAcf(lag.max = 10)
Raleigh_linear %>% residuals() %>% ggPacf(lag.max = 10)


Raleigh_linear <-Raleigh_train %>%
  model(trend1 = ARIMA(price_index~ trend() + pdq(2,0,0) + PDQ(0,0,0)+1),
        trend2 = ARIMA(price_index ~ trend() + PDQ(0,0,0) + 1),
        trend3 = ARIMA(price_index ~ trend() + PDQ(0,0,0) + 1,stepwise = FALSE)
    )
Raleigh_linear2<-as.data.frame(Raleigh_linear)
t(Raleigh_linear2)

glance(Raleigh_linear) %>% arrange(AICc) %>% select(.model:BIC)

Raleigh_linear %>% select(trend2) %>% residuals() %>% ggAcf(lag.max = 10)
Raleigh_linear %>% select(trend2) %>%residuals() %>% ggPacf(lag.max = 10)


augment(Raleigh_linear) %>% filter(.model=='trend2') %>% features(.innov,ljung_box, lag=10, dof = 4)



```


Comparing the Raleigh data set using a random walk with drift to the trend model:

```{r}


pred_lm <- Raleigh_linear %>% select(trend2) %>%
  fabletools::forecast(h=5)

error_lm <- Raleigh.ts$price_index[93:97] - pred_lm$.mean
MAPE_lm <-mean(abs(error_lm/Raleigh.ts$price_index[93:97]))
MAE_lm <- mean(abs(error_lm))


pred_rw <- tibble(
  quarter = yearquarter(seq.Date(from = as.Date("2023-01-01"),
                                 to = as.Date("2024-01-01"),
                                 by = "quarter")),
  value = pred_arima123$.mean  
)

pred_lm2 <- tibble(
  quarter = yearquarter(seq.Date(from = as.Date("2023-01-01"),
                                 to = as.Date("2024-01-01"),
                                 by = "quarter")),
  value = pred_lm$.mean  
)

test_data <- tibble(
  quarter = yearquarter(seq.Date(from = as.Date("2023-01-01"),
                                 to = as.Date("2024-01-01"),
                                 by = "quarter")),
  value = Raleigh.ts$price_index[93:97]  
)

combined_data <- bind_rows(
  test_data %>% mutate(Line = "Actual"),
  pred_rw %>% mutate(Line = "Random Walk"),
  pred_lm2 %>% mutate(Line = "Linear Model")
)

# Plot the data with a legend
ggplot(combined_data, aes(x = quarter, y = value, color = Line)) +
  geom_line(linetype = "solid") +
  labs(title = "Predicted versus Actual values",
       x = "Date", y = "Raleigh Price Index",
       color = "Legend") +
  theme_minimal()


```





## Python Code for ARMA/ARIMA models


```{python ADF, correlation plots and ARIMA}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from pandas import DataFrame
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA

quotes=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\fpp_insurance.csv")
df = pd.date_range(start='2002-01-01', end='2005-05-01', freq='ME')
quotes.index=pd.to_datetime(df)
y=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\ar2.csv")
quotes_train = quotes.head(36)
y_train = y.head(731)

result=adfuller(quotes_train["Quotes"])
print(f'ADF p-value: {result[1]}')

plot_acf(quotes_train["Quotes"],lags=12)
pyplot.show()
plot_pacf(quotes_train["Quotes"],lags=12)
pyplot.show()

### Using statsmodel...the older way of doing this..

model = ARIMA(y_train, order=(2,0,0))
model_fit = model.fit()
print(model_fit.summary())

residuals = DataFrame(model_fit.resid)
residuals.plot()
pyplot.show()
print(residuals.describe())

plot_acf(residuals,lags=12)
pyplot.show()
plot_pacf(residuals,lags=12)
pyplot.show()
```

Using ndiffs to see if we need to take a difference:

```{python}
from pmdarima.arima import ndiffs

hurr = pd.read_csv("https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/hurrican.csv")
hurr2 = hurr.dropna(axis=0)
hurricane_train =hurr2.head(149)
hurricane_train
n_diffs = ndiffs(hurricane_train["MeanVMax"])
print(n_diffs)
```


Checking for white noise:
The first value in the Ljung-Box test is the test statistic and the second value is the p-value.

```{python white noise with Ljung-Box}

import statsmodels.stats.diagnostic as diag


model = ARIMA(y_train, order=(2,0,0))
model_fit = model.fit()
print(model_fit.summary())

# Perform the Ljung-Box test
lb_test = diag.acorr_ljungbox(model_fit.resid, lags=[10], model_df=2)
print(lb_test)


  
```



Fitting ARIMA models.



```{python}

## Fit AR(2) model to AR2 data set

from statsforecast import StatsForecast
from statsforecast.models import ARIMA

y=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\ar2.csv")
df = pd.date_range(start='2000-01-01', end='2002-09-26', freq='D')
y.index=pd.to_datetime(df)

d = {'unique_id': 1, 'ds': y.index, 'y': y['Y']}
y_sf = pd.DataFrame(data = d)
y_train = y_sf.head(731)
y_test = y_sf.tail(69)



model_SD_ARIMA = StatsForecast(models = [ARIMA(order=(2, 0, 0),season_length=12)], freq = 'D')
model_SD_ARIMA.fit(df = y_train)



model_SD_ARIMA.fitted_[0][0].model_.get("arma")
model_SD_ARIMA.fitted_[0][0].model_.get("coef")
model_SD_ARIMA.fitted_[0][0].model_.get("aic")

### Fit MA(2) model to x

x=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\MA2.csv")
df = pd.date_range(start='2000-01-01', end='2000-04-09', freq='D')
x.index=pd.to_datetime(df)

d = {'unique_id': 1, 'ds': x.index, 'y': x['x']}
x_sf = pd.DataFrame(data = d)
x_train = x_sf.head(74)
x_test = x_sf.tail(26)

model_MA2 = StatsForecast(models = [ARIMA(order=(0, 0, 2))], freq = 'D')
model_MA2.fit(df = x_train)



model_MA2.fitted_[0][0].model_.get("arma")
model_MA2.fitted_[0][0].model_.get("coef")
model_MA2.fitted_[0][0].model_.get("aic")

resid=model_MA2.fitted_[0][0].model_.get("residuals")

plot_acf(resid,lags=12)
pyplot.show()
plot_pacf(resid,lags=12)
pyplot.show()


```

Using the Quotes data set for different models in ARIMA:

```{python}

## Compare two different models plus automatic search on Quotes data set:


from statsforecast.models import AutoARIMA
from statsforecast.arima import arima_string


d = {'unique_id': 1, 'ds': quotes.index, 'y': quotes['Quotes']}
quotes_sf = pd.DataFrame(data = d)
quotes_train = quotes_sf.head(36)
quotes_test = quotes_sf.tail(4)


plot_acf(quotes_train["y"],lags=12)
pyplot.show()
plot_pacf(quotes_train["y"],lags=12)
pyplot.show()


## AR(1) model

model_Quotes = StatsForecast(models = [ARIMA(order=(1, 0, 0), include_mean=True)], freq = 'ME')
model_Quotes.fit(df = quotes_train)



model_Quotes.fitted_[0][0].model_.get("arma")
model_Quotes.fitted_[0][0].model_.get("coef")
model_Quotes.fitted_[0][0].model_.get("aic")

## MA(1) model

model_Quotes2 = StatsForecast(models = [ARIMA(order=(0, 0, 1), include_mean=True)], freq = 'ME')
model_Quotes2.fit(df = quotes_train)



model_Quotes2.fitted_[0][0].model_.get("arma")
model_Quotes2.fitted_[0][0].model_.get("coef")
model_Quotes2.fitted_[0][0].model_.get("aic")

###  Now the automatic search....choose the AR(1) model

model_Quotes = StatsForecast(models = [AutoARIMA(seasonal=False)], freq = 'ME')
model_Quotes.fit(df = quotes_train)

model_Quotes.fitted_[0][0].model_.get("arma")
model_Quotes.fitted_[0][0].model_.get("coef")
model_Quotes.fitted_[0][0].model_.get("aic")

### Even though the automatic search choose AR(1), it did NOT include intercept.
### Made AR term close to 1 (very close to a random walk).
### Refitting the AR(1) model:

model_Quotes = StatsForecast(models = [ARIMA(order=(1, 0, 0), include_mean=True)], freq = 'ME')
model_Quotes.fit(df = quotes_train)

resid=model_Quotes.fitted_[0][0].model_.get("residuals")

plot_acf(resid,lags=12)
pyplot.show()
plot_pacf(resid,lags=12)
pyplot.show()



# Perform the Ljung-Box test
lb_test = diag.acorr_ljungbox(resid, lags=[10], model_df=1)
print(lb_test)

```


Now for using the hurricane data set:

```{python}

###Note that hurr2 is being redefined here.


df = pd.date_range(start='1851-01-01', end='2008-01-01', freq='YE')
hurr.index=pd.to_datetime(df)
hurr2 = hurr.dropna(axis=0)

d = {'unique_id': 1, 'ds': hurr2.index, 'y': hurr2['MeanVMax']}
hurr_sf = pd.DataFrame(data = d)
hurr_train = hurr_sf.head(149)
hurr_test = hurr_sf.tail(9)

###Looks like we need to take differences:
n_diffs=ndiffs(hurr_train["y"])
print(n_diffs)

hurr2['MVM_diff'] = hurr2['MeanVMax'].diff()
hurr3 = hurr2.dropna(axis=0)

df = pd.date_range(start='1854-01-01', end='2008-01-01', freq='YE')
hurr3.index=pd.to_datetime(df)


d = {'unique_id': 1, 'ds': hurr3.index, 'y': hurr3['MVM_diff']}
hurr_sf = pd.DataFrame(data = d)
hurr_train = hurr_sf.head(146)


plot_acf(hurr_train['y'],lags=12)
pyplot.show()
plot_pacf(hurr_train['y'],lags=12)
pyplot.show()

### Now to fit models

model_hurr = StatsForecast(models = [ARIMA(order=(3, 1, 0))], freq = 'YE')
model_hurr.fit(df = hurr_train)



model_hurr.fitted_[0][0].model_.get("arma")
model_hurr.fitted_[0][0].model_.get("coef")
model_hurr.fitted_[0][0].model_.get("aic")

model_hurr = StatsForecast(models = [ARIMA(order=(0, 1, 2))], freq = 'YE')
model_hurr.fit(df = hurr_train)



model_hurr.fitted_[0][0].model_.get("arma")
model_hurr.fitted_[0][0].model_.get("coef")
model_hurr.fitted_[0][0].model_.get("aic")

model_hurr1 = StatsForecast(models = [ARIMA(order=(3, 1, 2))], freq = 'YE')
model_hurr1.fit(df = hurr_train)



model_hurr1.fitted_[0][0].model_.get("arma")
model_hurr1.fitted_[0][0].model_.get("coef")
model_hurr1.fitted_[0][0].model_.get("aic")


model_hurr = StatsForecast(models = [AutoARIMA(seasonal=False)], freq = 'YE')
model_hurr.fit(df = quotes_train)

model_hurr.fitted_[0][0].model_.get("arma")
model_hurr.fitted_[0][0].model_.get("coef")
model_hurr.fitted_[0][0].model_.get("aic")

resid=model_hurr1.fitted_[0][0].model_.get("residuals")
plot_acf(resid,lags=12)
pyplot.show()
plot_pacf(resid,lags=12)
pyplot.show()
lb_test = diag.acorr_ljungbox(resid, lags=[10], model_df=5)
print(lb_test)

```

Get some measures of accuracy on validation data:

```{python}
##Hurricane data

y_hat1=model_hurr1.predict(h=8)
yhat=y_hat1.reset_index(drop=True)
test = hurr_test.reset_index(drop=True)
abs_error= np.absolute(test['y']-yhat["ARIMA"])
MAE = np.mean(abs_error)
MAE
MAPE = np.mean(abs_error/np.absolute(test['y']))
MAPE

```


An example with trend (fitting it with both differencing and linear regression)

```{python}
consume = pd.read_csv("https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/consume1982.csv")


df = pd.date_range(start='1982-09-01', end='1990-07-01', freq='ME')
consume.index=pd.to_datetime(df)

consume2 =consume


d = {'unique_id': 1, 'ds': consume.index, 'y': consume['Disposable_income']}
consume_sf = pd.DataFrame(data = d)
consume_train = consume_sf.head(88)
consume_test = consume_sf.tail(8)

n_diffs=ndiffs(consume_train["y"])
print(n_diffs)

consume2['income_diff'] = consume['Disposable_income'].diff()
consume3 = consume2.dropna(axis=0)
d = {'unique_id': 1, 'ds': consume3.index, 'y': consume3['income_diff']}
consume_sf = pd.DataFrame(data = d)
consume_train_diff = consume_sf.head(87)
consume_test_diff = consume_sf.tail(8)

plot_acf(consume_train_diff['y'],lags=12)
pyplot.show()
plot_pacf(consume_train_diff['y'],lags=12)
pyplot.show()

model_consume = StatsForecast(models = [ARIMA(order=(1, 1, 0), alias="AR1"),
                                       ARIMA(order=(0, 1, 1), alias="MA1"),
                                       ARIMA(order=(6, 1, 0), alias="AR6"),
                                       ARIMA(order=(0, 1, 6), alias="MA6")], freq = 'ME')
model_consume.fit(df = consume_train)



model_consume.fitted_[0][0].model_.get("arma")
model_consume.fitted_[0][0].model_.get("coef")
model_consume.fitted_[0][0].model_.get("aic")

```


## SAS Code for ARMA/ARIMA


_AUGMENTED DICKEY-FULLER TESTING_

proc arima data=Time.fpp_insurance plot=all; \  
	identify var=quotes nlag=10 stationarity=(adf=2); \  
		identify var=quotes(1) nlag=10 stationarity=(adf=2); \  
		run;
quit;

_CORRELATION FUNCTIONS_ \

_Notice no model statement!_ \  

proc arima data=Time.ar2 plot(unpack)=all; \  
	identify var=y nlag=10 outcov=Corr; \  
		estimate method=ML; \  
		run;
quit;

_BUILDING AN AUTOREGRESSIVE MODEL_

_Fit an AR2 model_ \  

proc arima data=Time.AR2 plot=all; \  
	identify var=y nlag=10; \  
		estimate p=2 method=ML; \  
		run;
quit; \  


_Add another estimate statement_ 
proc arima data=Time.AR2 plot=all;
	identify var=y nlag=10;
	estimate p=(2) method=ML;
	estimate p=(1,2,4) method=ML;
run;
quit; \  


_BUILDING A MOVING AVERAGE MODEL_

proc arima data=Time.ma2; \  
	identify var=x; \  
		estimate q=2 method=ML; \  
		run;
quit; \  

_Need to check for how to take care of trend_

proc arima data=Time.Ebay9899 plot=all; \  
	identify var=DailyHigh nlag=10 stationarity=(adf=2); \  
	run;
quit;


_It is a random walk!!  The way to model a random walk is by using differences_ \  


proc arima data=Time.Ebay9899 plot=all; \  
	identify var=DailyHigh(1) nlag=10 stationarity=(adf=2); \  
	run;
quit;




_BUILDING AN AUTOREGRESSIVE MOVING AVERAGE MODEL_ \  
   _(AUTOMATIC SELECTION TECHNIQUES)_ \  
   


_Fit an ARIMA model_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 stationarity=(adf=2); \  
	run;
quit; \  


_Model identification with minimum information criterion (MINIC)_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 minic P=(0:12) Q=(0:12); \  
	run;
quit; \  


_Model identification with smallest canonical correlation (SCAN);_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 scan P=(0:12) Q=(0:12); \  
	run;
quit; \  


_Model identificaiton with extended sample autocorrelation function (ESACF)_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 esacf P=(0:12) Q=(0:12); \  
	run;
quit; \  


_Create estimates with our ARIMA model p=2, q=3_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12; \  
		estimate p=2 q=3 method=ML; \  
		run;
quit;

_FORECASTING_ \ 

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=10 ; \  
		estimate p=2 q=3 method=ML; \  
		forecast lead=10; \  
		run;
quit;


