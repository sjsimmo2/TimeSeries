# ARIMA


```{r libraries, message=FALSE,echo=FALSE}
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
file.dir = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/" 
input.file1 = "usairlines.csv"
input.file2 = "steel.csv"
input.file3 = "leadyear.csv"
input.file4 = "ebay9899.csv"
input.file5 = "fpp_insurance.csv" 
input.file6 = "ar2.csv"
input.file7 = "MA2.csv"
input.file8 = "hurrican.csv"

# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines = read.csv(paste(file.dir, input.file1,sep = ""))
Steel = read.csv(paste(file.dir, input.file2, sep = ""))
Lead.Year = read.csv(paste(file.dir, input.file3, sep = ""))
Ebay = read.csv(paste(file.dir, input.file4, sep = ""))
Quotes= read.csv(paste(file.dir, input.file5, sep = ""))
Y= read.csv(paste(file.dir, input.file6, sep = ""))
x=read.csv(paste(file.dir, input.file7, sep = ""))
hurricane=read.csv(paste(file.dir, input.file8, sep = ""))
SteelShp <- ts(Steel$steelshp, start = 1984, frequency = 12)
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)
library(reticulate)
use_python("C:\\ProgramData\\Anaconda3\\python.exe")
```

We will now be switching over to doing ARMA/ARIMA models!!  There are a number of different concepts you will need in order to do this type of modeling.

## Stationarity

Before we can try to model the dependency structure (the AR and MA terms), we must first have a stationarity!  The ADF test is one of the most well-known and accepted test for testing stationarity.  There are several packages that will do this for you, however, below, I am focusing on the ADF test within the package aTSA.

```{r plot}

Quotes.ts<-ts(Quotes$Quotes,start=2002,frequency=12)
autoplot(Quotes.ts)+labs(title="Time Series of Daily Stock quotes", x="Time", y="Quotes")
```


The following code produces output similar to the output seen in SAS (under the tau test).


```{r stationarity}


# Perform the ADF test (k=0)
aTSA::adf.test(Quotes.ts)

```


## Correlation Functions


The Acf and the Pacf in R will calculate the autocorrelation (up to the lag you specify) and the partial autocorrelation, respectively.  You can either output these values to look at them or plot them (see code below).

```{r correlation functions}
acf1=Acf(Y, lag=10)$acf
pacf1=Pacf(Y, lag=10)$acf

index1=seq(1,length(pacf1))

all.dat=data.frame(cbind(acf1[2:11],pacf1,index1))
colnames(all.dat)=c("acf","pacf","index")

ggplot(all.dat,aes(x=factor(index),y=acf))+geom_col()+labs(x="Lags")
```


## AutoRegressive Models (AR)

AutoRegressive (AR) models involve modeling the lags of Y.  We can write an autoregressive model as

$$ Y_{t} = c + \phi_{1}Y_{t-1}+\phi_{2}Y_{t-2}+...\phi_{p}Y_{t-p}+\epsilon_{t} $$
Where there are *p* lags of *Y*.  Below is the code to fit an AR(2) model. The *order* in the *Arima* function needs the p,d,q values (p=# of AR terms, d=how many differences should be taken and q=# of MA terms).

```{r AR2}

ggAcf(Y)
ggPacf(Y)

Y.ts <- ts(Y)
Y.ARIMA <- Arima(Y.ts, order=c(2,0,0))

ggAcf(Y.ARIMA$residuals)
ggPacf(Y.ARIMA$residuals)

```


## Moving Average model (MA)

Moving average (MA) models involve modeling the lags of the error.  We can write a moving average model as

$$ Y_{t} = c - \theta_{1}\epsilon_{t-1}-\theta_{2}\epsilon_{t-2}-...\theta_{q}\epsilon_{t-q}+\epsilon_{t} $$
Where there are *q* lags of $\epsilon$.  Below is code to fit an MA(2) model.

```{r MA2}

ggAcf(x)
ggPacf(x)

x.ts <- ts(x)
x.ARIMA <- Arima(x.ts, order=c(0,0,2))
summary(x.ARIMA)
ggAcf(x.ARIMA$residuals)
ggPacf(x.ARIMA$residuals)

```


## White noise

For residuals to exhibit white noise, they must be "independent" and normally distributed with mean 0 and constant variance.  You already know how to assess normality and constant variance, however, we need to focus on assessing "independence".  We can assess if there is significant dependence through the Ljung-Box test. The hypotheses being tested are

$$H_{0}:No\quad significant\quad autocorrelation\\
H_{A}:Significant\qquad autocorrletion $$

This should be assessed on a stationary time series.  Looking at a stationary time series, going back 10 lags should be sufficient (this will be different when we get to seasonal models).  We would like for all of the p-values (for lags 1-10) to be insignificant (large p-values).  However, keep in mind that sample size will matter when assessing significance.

```{r Ljung-Box test}

White.LB <- rep(NA, 10)
for(i in 1:10){
  White.LB[i] <- Box.test(Y, lag=i, type="Ljung-Box", fitdf = 0)$p.value
}

white.dat=data.frame(cbind(White.LB,index1))
colnames(white.dat)=c("pvalues","Lag")

ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Ljung-Box test p-values",x="Lags",y="p-values")+coord_cartesian(ylim = c(0, 0.025))


####Fit appropriate model
Y.ARIMA=Arima(Y,order=c(2,0,0))
White.LB <- rep(NA, 10)
for(i in 3:10){
  White.LB[i] <- Box.test(Y.ARIMA$residuals, lag=i, type="Ljung-Box", fitdf = 2)$p.value
}

white.dat=data.frame(cbind(White.LB[3:10],index1[3:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Ljung-Box test when there is white noise",x="Lags",y="p-values")

```


## Trending Series

If the series is trending then it is NOT stationary.  You will NEED to do something to the series in order to make it stationary!!  You will either fit a linear regression line (then use the residuals to model dependencies) or take differences and use the differenced series to model dependencies.  We will be using the Ebay stock data of "Daily High" (however, this data has missing values!!).  If you do not impute missing values, it simply ignores the missing values (could create an issue).  I would recommend FIRST imputing those values BEFORE doing the ADF test (in the na_interpolation algorithm, missing values are imputed).

```{r adf test with missing values}
Daily.High <- ts(Ebay$DailyHigh)
###NOT appropriate since there are missing values!!
aTSA::adf.test(Daily.High)

ggplot_na_distribution(Daily.High)+labs(y="Stock prices for Ebay")

# Interpolate the missing observations in this data set
Daily.High<-Daily.High %>% na_interpolation(option = "spline")

autoplot(Daily.High)+labs(title="Daily high stock quotes",x="Time",y="Quotes")

# Perform an ADF test
aTSA::adf.test(Daily.High)
### Definitely a Random Walk!!
```


How do we fit each situation?  If the series is stationary about the line, we need to fit a line (and then model the AR and MA terms on the residuals).  If series is a random walk with drift, then need to take differences.  Here is the R code for each situation.

```{r ARIMA with trend}

###Fitting a regression line...

time.high=seq(1,length(Daily.High))
ARIMA.line=Arima(Daily.High,order=c(0,0,0),xreg=time.high)
summary(ARIMA.line)
#### Now model residuals (ARIMA.line$residuals) with AR and MA terms...can also send residuals through an automatic procedure to help!

####Fitting a random walk with drift

ARIMA.RW=Arima(Daily.High,order=c(0,1,0))
summary(ARIMA.RW)
####  automatic procedure will determine if you need differencing or not!

```


## Fitting ARIMA models

We can use an automatic procedure to help us find a model. For this illustration, we will be using the mean of the maximum velocity in the hurricane data set.  This data also has some missing values which we need to look into first.

```{r visualize missing values}

max.velocity=hurricane$MeanVMax

ggplot_na_distribution(max.velocity)+labs(y="Mean Max Velocity")

```

This is yearly data and the reason those values are missing is because there were no hurricanes recorded for that year.  Since there is no trend (nor seasonality), I am going to remove these NA values and then run the Dickey-Fuller test.

```{r hurrican ADF test}

max.velocity=na.omit(max.velocity)
hurrican.ts=ts(max.velocity)
aTSA::adf.test(hurrican.ts)

```

Series is stationary!! Let's see if there is any significant dependecies here...

```{r Ljung-Box for Hurricane data}
index1=seq(1,10)
White.LB <- rep(NA, 10)
for(i in 1:10){
  White.LB[i] <- Box.test(hurrican.ts, lag=i, type="Ljung-Box", fitdf = 0)$p.value
}

white.dat=data.frame(cbind(White.LB[1:10],index1[1:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Hurricane data",x="Lags",y="p-values")

```

There is definitely something to be modeled here!!  Let's try an automated search first...

```{r auto.arima for Hurricane data}

model1=auto.arima(hurrican.ts)
model2=auto.arima(hurrican.ts,d=0)

```

Let's take a look at ACF and PACF plots and see how well we do manually..

```{r Correlation plots hurricane data}

ggAcf(hurrican.ts)
ggPacf(hurrican.ts)

```


Using the graphs and some trial and error, here was the model I chose...



```{r by hand hurricane model}

model3=Arima(hurrican.ts,order=c(2,0,3))
summary(model3)

```


Comparing the ACF and PACF plots for these models:

```{r comparing ACF and PACF}

ggAcf(model1$residuals,lag.max = 10)
ggPacf(model1$residuals,lag.max = 10)
ggAcf(model2$residuals,lag.max = 10)
ggPacf(model2$residuals,lag.max = 10)
ggAcf(model3$residuals,lag.max = 10)
ggPacf(model3$residuals,lag.max = 10)

```


Let's take a look at white noise for each model:


```{r white noise}

index1=seq(1,10)
White.LB <- rep(NA, 10)
for(i in 2:10){
  White.LB[i] <- Box.test(model1$residuals, lag=i, type="Ljung-Box", fitdf = 1)$p.value
}

white.dat=data.frame(cbind(White.LB[2:10],index1[2:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Model 1",x="Lags",y="p-values")

White.LB <- rep(NA, 10)
for(i in 3:10){
  White.LB[i] <- Box.test(model2$residuals, lag=i, type="Ljung-Box", fitdf = 2)$p.value
}

white.dat=data.frame(cbind(White.LB[3:10],index1[3:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Model 2",x="Lags",y="p-values")

White.LB <- rep(NA, 10)
for(i in 6:10){
  White.LB[i] <- Box.test(model3$residuals, lag=i, type="Ljung-Box", fitdf = 5)$p.value
}

white.dat=data.frame(cbind(White.LB[6:10],index1[6:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Model 3",x="Lags",y="p-values")

ggplot(data =hurrican.ts, aes(x = model1$residuals)) +
    geom_histogram() +
    labs(title = 'Histogram of Residuals for Model 1', x = 'Residuals', y = 'Frequency')
ggplot(data =hurrican.ts, aes(x = model2$residuals)) +
    geom_histogram() +
    labs(title = 'Histogram of Residuals for Model 2', x = 'Residuals', y = 'Frequency')

ggplot(data =hurrican.ts, aes(x = model3$residuals)) +
    geom_histogram() +
    labs(title = 'Histogram of Residuals for Model 3', x = 'Residuals', y = 'Frequency')


```


You can now forecast the data with these models:


```{r forecast ARIMA}

forecast::forecast(model1, h = 10)
autoplot(forecast::forecast(model1, h = 10))
autoplot(forecast::forecast(model2, h = 10))
autoplot(forecast::forecast(model3, h = 10))

```


## Python Code for ARMA/ARIMA models


```{python ADF, correlation plots and ARIMA}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from pandas import DataFrame
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA

quotes=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\fpp_insurance.csv")
y=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\ar2.csv")

result=adfuller(quotes["Quotes"])
print(f'ADF p-value: {result[1]}')

plot_acf(quotes["Quotes"],lags=12)
pyplot.show()
plot_pacf(quotes["Quotes"],lags=12)
pyplot.show

model = ARIMA(y, order=(2,0,0))
model_fit = model.fit()
print(model_fit.summary())

residuals = DataFrame(model_fit.resid)
residuals.plot()
pyplot.show()
print(residuals.describe())

plot_acf(residuals,lags=12)
pyplot.show()
plot_pacf(residuals,lags=12)
pyplot.show()
```


Checking for white noise:
The first value in the Ljung-Box test is the test statistic and the second value is the p-value.

```{python white noise with Ljung-Box}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from pandas import DataFrame
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA

quotes=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\fpp_insurance.csv")
y=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\ar2.csv")


model = ARIMA(y, order=(2,0,0))
model_fit = model.fit()
print(model_fit.summary())

lag_test=[3,4,5,6,7,8,9,10]

for x in lag_test:
  sm.stats.acorr_ljungbox(model_fit.resid, lags=[x], model_df=2)
  
```

Trending series...

```{python Trending series}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from pandas import DataFrame
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA


Daily_High_all=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\ebay9899.csv")
Daily_High=Daily_High_all["DailyHigh"]
Daily_High2=Daily_High.interpolate(option='spline')
result=adfuller(Daily_High2)
print(f'ADF p-value: {result[1]}')

##Fitting a Random Walk
model = ARIMA(y, order=(0,1,0))
model_fit = model.fit()

###NOT correct, but if you wanted to fit a regression to data
x=pd.Series(range(318))
model2=ARIMA(endog=Daily_High2,exog=x,order=[0,0,0])
model2_fit=model2.fit()
model2_fit

### Now you can model residuals from the regression (if that is the route you took)
  
```


Fitting ARIMA models.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from pandas import DataFrame
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA
import pmdarima as pm


hurricane=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\hurrican.csv")
max_velocity=hurricane["MeanVMax"]
max2=max_velocity.dropna()

### Testing stationarity 

result=adfuller(max2)
print(f'ADF p-value: {result[1]}')

### Same result as auto.arima in R!
model1=pm.auto_arima(max2, start_p=0,start_q=0,max_p=5,max_q=5,seasonal=False)
model1.summary()

# Force d=0
model2=pm.auto_arima(max2, start_p=0,start_q=0,max_p=5,max_q=5,d=0,seasonal=False)
model2.summary()



```


## SAS Code for ARMA/ARIMA


_AUGMENTED DICKEY-FULLER TESTING_

proc arima data=Time.fpp_insurance plot=all; \  
	identify var=quotes nlag=10 stationarity=(adf=2); \  
		identify var=quotes(1) nlag=10 stationarity=(adf=2); \  
		run;
quit;

_CORRELATION FUNCTIONS_ \

_Notice no model statement!_ \  

proc arima data=Time.ar2 plot(unpack)=all; \  
	identify var=y nlag=10 outcov=Corr; \  
		estimate method=ML; \  
		run;
quit;

_BUILDING AN AUTOREGRESSIVE MODEL_

_Fit an AR2 model_ \  

proc arima data=Time.AR2 plot=all; \  
	identify var=y nlag=10; \  
		estimate p=2 method=ML; \  
		run;
quit; \  


_Add another estimate statement_ 
proc arima data=Time.AR2 plot=all;
	identify var=y nlag=10;
	estimate p=(2) method=ML;
	estimate p=(1,2,4) method=ML;
run;
quit; \  


_BUILDING A MOVING AVERAGE MODEL_

proc arima data=Time.ma2; \  
	identify var=x; \  
		estimate q=2 method=ML; \  
		run;
quit; \  

_Need to check for how to take care of trend_

proc arima data=Time.Ebay9899 plot=all; \  
	identify var=DailyHigh nlag=10 stationarity=(adf=2); \  
	run;
quit;


_It is a random walk!!  The way to model a random walk is by using differences_ \  


proc arima data=Time.Ebay9899 plot=all; \  
	identify var=DailyHigh(1) nlag=10 stationarity=(adf=2); \  
	run;
quit;

_IF it would have been deterministic...CAUTION, it is NOT deterministic!!_ \  


_You would need to create an "x" variable for time_ \  

data newebay; \  
set Time.Ebay9899; \  
time=_n_; \  
run; \  


_Keep in mind that this data is not deterministic!!_ \  

 
proc arima data=newebay plot=all; \  
	identify var=DailyHigh crosscorr=time; \  
		estimate input=time; \  
		run;
quit;



_BUILDING AN AUTOREGRESSIVE MOVING AVERAGE MODEL_ \  
   _(AUTOMATIC SELECTION TECHNIQUES)_ \  
   


_Fit an ARIMA model_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 stationarity=(adf=2); \  
	run;
quit; \  


_Model identification with minimum information criterion (MINIC)_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 minic P=(0:12) Q=(0:12); \  
	run;
quit; \  


_Model identification with smallest canonical correlation (SCAN);_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 scan P=(0:12) Q=(0:12); \  
	run;
quit; \  


_Model identificaiton with extended sample autocorrelation function (ESACF)_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 esacf P=(0:12) Q=(0:12); \  
	run;
quit; \  


_Create estimates with our ARIMA model p=2, q=3_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12; \  
		estimate p=2 q=3 method=ML; \  
		run;
quit;

_FORECASTING_ \ 

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=10 ; \  
		estimate p=2 q=3 method=ML; \  
		forecast lead=10; \  
		run;
quit;


