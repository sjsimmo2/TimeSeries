--- 
title: "Time Series 1"
author: "by Dr. Susan Simmons"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# Time series 1

This is your introduction in Time Series.  By the end of this course, you should be able to understand time series data and different formats for doing time series; understand and identify the different characteristics of a time series (trend, cycle, seasonality, etc); perform a time series decomposition on seasonal data; perform numerous exponential smoothing models; understand stationarity and how to test for it; understand AR, MA and mixed models and when to use differencing; and be able to identify if a series has white noise.  All of the data sets needed for this class are on the github repository.  Enjoy your TIME in Time Series!!


<!--chapter:end:index.Rmd-->

# Introduction


This document provides the code and output from Time Series I class. This is a living document and may be updated throughout the semester (when this occurs, you will be notified that it has been updated).  First, here is a list of all the libraries that you will need:

```{r libraries, message=FALSE,warning=FALSE}
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
library(reticulate)
library(prophet)
use_python("C:\\ProgramData\\Anaconda3\\envs\\R_Env2\\python.exe")
#use_python("C:\\ProgramData\\Anaconda3\\python.exe")
```

The data sets that you will need are as follows (be sure to put the correct location and file name for your computer):

```{r data sets}

file.dir = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/" 
input.file1 = "usairlines.csv"
input.file2 = "steel.csv"
input.file3 = "leadyear.csv"
input.file4 = "ebay9899.csv"
input.file5 = "fpp_insurance.csv" 
input.file6 = "ar2.csv"
input.file7 = "MA2.csv"
input.file8 = "hurrican.csv"

# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines = read.csv(paste(file.dir, input.file1,sep = ""))
Steel = read.csv(paste(file.dir, input.file2, sep = ""))
Lead.Year = read.csv(paste(file.dir, input.file3, sep = ""))
Ebay = read.csv(paste(file.dir, input.file4, sep = ""))
Quotes= read.csv(paste(file.dir, input.file5, sep = ""))
Y= read.csv(paste(file.dir, input.file6, sep = ""))
x=read.csv(paste(file.dir, input.file7, sep = ""))
hurricane=read.csv(paste(file.dir, input.file8, sep = ""))
```


For many time series applications, you will need a time series object in R.  This is created using the function ts.  For example, the time series data set in the airlines data frame is in the column "passengers".  Let's go ahead and create the time series object for this data set and graph it.

```{r Time Series object}

Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)

autoplot(Passenger)+labs(title="Time Series plot for Passengers", x="Date",y="Passengers")

```



Within the ts command, the only required argument is the vector of data that contains the time series information (in this case USAirlines$Passengers).  The optional arguments of "start" is for nice plotting purposes (it has the correct time frame when it plots instead of just using 1,2,3 for time).  The last argument shown is "frequency", which is for seasonal data.  If your time series object has a seasonality to it, then you should specify the length of the season (it does not know this unless you provide it).  For future analysis, we will need to create the time series objects for Steel (this data is monthly, so we will set frequency to 12, since there are 12 months in a year).

```{r ts2}
SteelShp <- ts(Steel$steelshp, start = 1984, frequency = 12)

```


## Time series decomposition


IF your time series has a seasonal component to it, a useful visualization is the decomposition.  We will be using the STL decomposition (which can only do the additive decomposition, NOT multiplicative!).  The following code creates the decomposition and then plots it:

```{r decomposition}
# Time Series Decomposition ...STL#
decomp_stl <- stl(Passenger, s.window = 7)

# Plot the individual components of the time series
plot(decomp_stl)

autoplot(decomp_stl)

```

NOTE: You will need to specify the length of the seasonal window for STL...this is done by specifying s.window = "size of window".  Your window should be an odd number.\\
You can pull off the different components (Seasonal, Trend or Remainder). The below command provides the first few rows of the decomposition (you can see what information is contained within the decomposition...season, trend and remainder).

```{r decomposition components}

head(decomp_stl$time.series)

```

Which means we can overlay the original data with the trend component (which is the second column.)

```{r overlay trend}

autoplot(Passenger)+geom_line(aes(y=decomp_stl$time.series[,2]),color="blue")

```

Notice that the trend component is VERY similar to the "seasonally adjusted" data!  Do you know what the difference between the two series is?

```{r seasonally adjusted}

seas_adj=Passenger-decomp_stl$time.series[,1]

autoplot(Passenger) +
  geom_line(aes(y=decomp_stl$time.series[,2]),color="blue") +
  geom_line(aes(y=seas_adj),color="orange")

```

Another interesting plot is the subseries plot.  This looks at the individual series (in this case, the series for January, the series for February, etc....).

```{r subseries}

# Plot seasonal subseries by months
ggsubseriesplot(Passenger)

```
Just a quick note.  STL ONLY does additive seasonal decomposition.  There is a decompose library that will do both additive AND multiplicative decomposition.

To get a measure of strength for trend and seasonality using Hyndman and Athanasopoulos' technique:

```{r}
Ft=max(0,1-var(decomp_stl$time.series[,3])/(var(decomp_stl$time.series[,3])+var(decomp_stl$time.series[,2]) + 2*cov(decomp_stl$time.series[,3],decomp_stl$time.series[,2])))
Ft

Fs=max(0,1-var(decomp_stl$time.series[,3])/(var(decomp_stl$time.series[,3])+var(decomp_stl$time.series[,1])+2*cov(decomp_stl$time.series[,3],decomp_stl$time.series[,1])))
Fs
```




## Python code and results for Time Series plot

The following code shows the time series plot and classical decomposition (using additive model).

```{python}

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import pyplot
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import STL
usair_p=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\usairlines.csv")
df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair_p.index=pd.to_datetime(df)
print(usair_p.head())
plt.plot(usair_p["Passengers"])
plt.xlabel("Date")
plt.ylabel("Passengers")
plt.title("Airline Passengers")
plt.show()

## Classical Decomposition
result = seasonal_decompose(usair_p["Passengers"], model='additive', period=12)
result.plot()
pyplot.show()


```

 You can also specify multiplicative by changing "additive" to "multiplicative" in the Python code.  If you want to do the STL decomposition, then you can apply the following code:


```{python STL decomposition}
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import STL
usair_p=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\usairlines.csv")
df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair_p.index=pd.to_datetime(df)


## STL Decomposition

stl = STL(usair_p["Passengers"], period=12)
res = stl.fit()
fig = res.plot()
pyplot.show()

```


## SAS code for time series plot and decomposition

The following code is in SAS (creates the time series plot and the decomposition...which is the next section).  The use of the id statement can only be used if the series has a properly formatted date column.  Great thing about SAS is that if there is a missing value (or missing date), it will impute it for you.  It can also aggregate easily in SAS.  If you do not have a date column, you can still use this command, you will use seasonality = and then set the length of the season.  However, if you use the seasonality option, SAS will not recognize missing dates.

_Create library for data sets_  \  

libname Time 'Q:\My Drive\Fall 2017 - Time Series\Data';  \  
run; \  

_Time series plot and decomposition of Airline Passengers_ \  

proc timeseries data=Time.USAirlines plots=(series decomp sc); \  
	id date interval=month; \  
	var Passengers; \  
	run; \  

_Time series and decomposition of Airline Passengers, but now the seasonal cycle is set to a year_ \ 

proc timeseries data=Time.USAirlines plots=(series decomp sc) seasonality=12; \  
	var Passengers; \  
run; \  


proc timeseries data=Time.Steel plots=(series decomp sc) seasonality=12; \  
	var steelshp; \  
run; \  


## Self-study

Here is something to get you started if you want to take a look at the X13 decomposition!

```{r X13,message=FALSE}

decomp_x13=seas(Passenger)
summary(decomp_x13)

## Neat R shiny application....run OUTSIDE of RMarkdown
#view(decomp_x13)

```


<!--chapter:end:01-Introduction.Rmd-->

# Exponential Smoothing


```{r, message=FALSE,echo=FALSE}
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)

file.dir = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/" 
input.file1 = "usairlines.csv"
input.file2 = "steel.csv"
input.file3 = "leadyear.csv"
input.file4 = "ebay9899.csv"
input.file5 = "fpp_insurance.csv" 
input.file6 = "ar2.csv"
input.file7 = "MA2.csv"
input.file8 = "hurrican.csv"

# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines = read.csv(paste(file.dir, input.file1,sep = ""))
Steel = read.csv(paste(file.dir, input.file2, sep = ""))
Lead.Year = read.csv(paste(file.dir, input.file3, sep = ""))
Ebay = read.csv(paste(file.dir, input.file4, sep = ""))
Quotes= read.csv(paste(file.dir, input.file5, sep = ""))
Y= read.csv(paste(file.dir, input.file6, sep = ""))
x=read.csv(paste(file.dir, input.file7, sep = ""))
hurricane=read.csv(paste(file.dir, input.file8, sep = ""))
SteelShp <- ts(Steel$steelshp, start = 1984, frequency = 12)
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)

```

In R, we are able to do Simple (or Single) Exponential Smoothing Models, Holt Exponential Smoothing Models and Holt-Winters Exponential Smoothing Models.  For the first two (Simple and Holt), we will be using the Steel data set and for the last one (Holt-Winters), we will be using the airline data set (we will also use the airline data set to illustrate the damped trend model).  Each of these are shown below.  

## Simple Exponential Smoothing

For Simple Exponential Smoothing Models (SES), we have only one component, referred to as the level component.
$$\hat{Y}_{t+1}= L_{t}\\
  L_{t} = \theta Y_{t} + (1-\theta)L_{t-1}$$
  
This is basically a weighted average with the last observation and the last predicted value.  Since this only has a level component, forecasts from SES models will be a horizontal line (hence why this method is called "one-step ahead" forecasting).  

In the R code, you can choose how the initial values are selected.  If you specify simple, then the first few observations will be used to estimate the starting value.  If you select optimal, then the algorithm uses the ets algorithm (will be discussed later) to optimize the starting values and the smoothing parameters.  You can also specify the value for "h", which is the number of forecasts to create (take a look at the forecast...do you see a horizontal line?).

```{r ses}

# Building a Single Exponential Smoothing (SES) Model - Steel Data #
SES.Steel <- ses(SteelShp, initial = "simple", h = 24)
summary(SES.Steel)

# Plot the SES model on steel data
autoplot(SES.Steel)+
  autolayer(fitted(SES.Steel),series="Fitted")+ylab("US Steel Shipments") + geom_vline(xintercept = 1992,color="orange",linetype="dashed")


# Computes accuracy statistics for SES model on steel data (training data...NOT validation nor test)
round(accuracy(SES.Steel),2) 

```

## Holt ESM

The Holt model incorporates trend information.  So, now there are two components: level and trend.  For each component, there will be a smoothing coefficient (or weight).  CAREFUL, when you look at parameter estimates, these are NOT the estimates for the mean nor the linear trend...you should be thinking of them as weights (between 0 and 1).  The overall form for Holt's method is:

$$\hat{Y}_{t+h}= L_{t}+hT_{t}\\
  L_{t} = \theta Y_{t} + (1-\theta)(L_{t-1}+T_{t-1})\\
  T_{t} = \gamma (L_{t}-L_{t-1}) + (1-\gamma) T_{t-1}$$
  
For the Holt's method, when you forecast, you will see a trending line.  

```{r Holt}
# Building a Linear Exponential Smoothing Model - Steel Data #
LES.Steel <- holt(SteelShp, initial = "optimal", h = 24)
summary(LES.Steel)

# Plote the LES model on steel data

autoplot(LES.Steel)+
  autolayer(fitted(LES.Steel),series="Fitted")+labs(title="US Steel Shipment with Holt forecasts",y="US Steel Shipments") + geom_vline(xintercept = 1992,color="orange",linetype="dashed")

```

We can also perform Holt's method with a damped trend.  You will see the formula for the damped trend is similar to the previous Holt formula with an addition of a dampening parameter.

$$\hat{Y}_{t+h}= L_{t}+\sum_{i}^{k}\phi^{i}T_{t}\\
  L_{t} = \theta Y_{t} + (1-\theta)(L_{t-1}+\phi T_{t-1})\\
  T_{t} = \gamma (L_{t}-L_{t-1}) + (1-\gamma) \phi T_{t-1}$$

We will illustrate the damped trend on both the Steel and Airline data sets.

```{r damped trend}
LDES.Steel <- holt(SteelShp, initial = "optimal", h = 24, damped = TRUE)
summary(LDES.Steel)
autoplot(LDES.Steel)+
  autolayer(fitted(LDES.Steel),series="Fitted")+labs(title="US Steel Shipment Linear Damped ESM Forecast") + geom_vline(xintercept = 1992,color="orange",linetype="dashed")

LDES.USAir <- holt(Passenger, initial = "optimal", h = 24, damped = TRUE)
summary(LDES.USAir)
autoplot(LDES.USAir)+
  autolayer(fitted(LDES.USAir),series="Fitted")+labs(title="US Airline Passengers with Linear Damped ESM Forecast") + geom_vline(xintercept = 2008.25,color="orange",linetype="dashed")
```

## Holt-Winters

The Holt-Winters (HW) model has three components to it (level, trend and seasonality).  Seasonality is an interesting component to model since we can have an additive seasonal component or a multiplicative seasonal component.  Both models are shown below:

Additive HW
$$\hat{Y}_{t+h}= L_{t}+hT_{t} + S_{t-p+h}\\
  L_{t} = \theta (Y_{t} - S_{t-p}) + (1-\theta)(L_{t-1}+T_{t-1})\\
  T_{t} = \gamma (L_{t}-L_{t-1}) + (1-\gamma) T_{t-1}\\
  S_{t} = \delta (Y_{t}-L_{t-1}-T_{t-1}) + (1-\delta) S_{t-p}$$

Multiplicative HW
$$\hat{Y}_{t+h}= (L_{t}+hT_{t}) S_{t-p+h}\\
  L_{t} = \theta \frac{Y_{t}} {S_{t-p}} + (1-\theta)(L_{t-1}+T_{t-1})\\
  T_{t} = \gamma (L_{t}-L_{t-1}) + (1-\gamma) T_{t-1}\\
  S_{t} = \delta \frac{Y_{t}}{L_{t-1}+T_{t-1}} + (1-\delta) S_{t-p}$$

Where p is the frequency of the seasonality (i.e. how many "seasons" there are within one year).  


```{r Seasonal models}
# Building a Holt-Winters ESM - US Airlines Data - Additive Seasonality
HWES.USAir <- hw(Passenger, seasonal = "additive")
summary(HWES.USAir)

autoplot(HWES.USAir)+
  autolayer(fitted(HWES.USAir),series="Fitted")+ylab("Airlines Passengers")+ geom_vline(xintercept = 2008.25,color="orange",linetype="dashed")

# Building a Holt-Winters ESM - US Airlines Data - Multiplicative Seasonality
HWES.USAir <- hw(Passenger, seasonal = "multiplicative")
summary(HWES.USAir)


autoplot(HWES.USAir)+
  autolayer(fitted(HWES.USAir),series="Fitted")+ylab("Airlines Passengers")+ geom_vline(xintercept = 2008.25,color="orange",linetype="dashed")
```

## Evaluating forecasts

In order to get a better idea of the forecasting properties of the algorithms, it is best to divide your data into a training data set and a test data set.  Time series is VERY different than other algorithms in which you have done.  The test data set should come at the END of the time series (to truly see how well you can forecast!).  An example code is shown below in which the last 12 observations are used as the test data set:

```{r forecasts}
# Create training set from overall Airlines Data
training=subset(Passenger,end=length(Passenger)-12)

# Create test set from overall Airlines Data
test=subset(Passenger,start=length(Passenger)-11)

# Fit Holt-Winters ESM (multiplicative seasonality) on training data
HWES.USAir.train <- hw(training, seasonal = "multiplicative",initial='optimal',h=12)


# Calculate prediction errors from forecast
error=test-HWES.USAir.train$mean

# Calculate prediction error statistics (MAE and MAPE)
MAE=mean(abs(error))
MAPE=mean(abs(error)/abs(test))

MAE
MAPE
```

## ETS

You can also allow the computer to search for the best model.  The ETS (Error, Trend, Seasonality) algorithm will search for the best model and estimate the parameters.  For the error term, we can have either an additive or multiplicative error structure.  For the trend, we can have none, additive, multiplicative, damped additive or damped multiplicative . For the seasonal component, we can have none, additive or multiplicative (lots of choices!). An example of how to run this is:

```{r ets}
ets.passenger<-ets(training)
summary(ets.passenger)
ets.forecast.passenger<-ets.passenger%>%forecast::forecast(h=12)
error=mean(abs(test-ets.forecast.passenger$mean))
error
```

## Python Code for Exponential Smoothing

The following Python codes will produce exponential smoothing models.  I do recommend that you take some time to understand some of the differences between the models fit in R versus Python.

```{python simple exponential smoothing}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt

steel=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\steel.csv")
df=pd.date_range(start='1/1/1984', end='12/1/1991', freq='MS')
steel.index=pd.to_datetime(df)

fit = SimpleExpSmoothing(steel['steelshp']).fit()
fit.params['smoothing_level']
fcast = fit.forecast(24)
fcast
plt.plot(steel["steelshp"],color="black")
plt.plot(fcast,color="blue")
plt.show()
```


The Holt models in Python:

```{python holt models}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt

steel=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\steel.csv")
df=pd.date_range(start='1/1/1984', end='12/1/1991', freq='MS')
steel.index=pd.to_datetime(df)

fit1 = Holt(steel['steelshp']).fit()
fit1.summary()
fcast1 = fit1.forecast(24)
fcast1
fit2 = Holt(steel['steelshp'],exponential=True).fit()
fit2.summary()
fcast2 = fit2.forecast(24)
fcast2
fit3 = Holt(steel['steelshp'],damped=True).fit()
fit3.summary()
fcast3 = fit3.forecast(24)
fcast3
fit4 = Holt(steel['steelshp'],exponential=True,damped=True).fit()
fit4.summary()
fcast4 = fit4.forecast(24)
fcast4

ax=steel.plot(color="black",figsize=(12,8))
fcast1.plot(ax=ax,color="blue")
fcast2.plot(ax=ax,color="orange")
fcast3.plot(ax=ax,color="purple")
fcast4.plot(ax=ax,color="gray")
plt.show()



```
Seasonal models in Python:

```{python seasonal models}

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import STL
usair_p=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\usairlines.csv")
df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair_p.index=pd.to_datetime(df)


usair_p['HWES_ADD'] = ExponentialSmoothing(usair_p['Passengers'],trend='add',seasonal='add',seasonal_periods=12).fit().fittedvalues

usair_p['HWES_MUL'] = ExponentialSmoothing(usair_p['Passengers'],trend='mul',seasonal='mul',seasonal_periods=12).fit().fittedvalues

usair_p[['Passengers','HWES_ADD','HWES_MUL']].plot(title='Holt Winters Exponential Smoothing: Additive and Multiplicative Seasonality')
plt.show()

fit1=ExponentialSmoothing(usair_p['Passengers'],trend='add',seasonal='mul',seasonal_periods=12).fit()
fit1.summary()

fit2=ExponentialSmoothing(usair_p['Passengers'],trend='add',seasonal='add',seasonal_periods=12).fit()
fit2.summary()

fcast1=fit1.forecast(24)
fcast2=fit2.forecast(24)

ax=usair_p["Passengers"].plot(color="black",figsize=(12,8))
fcast1.plot(ax=ax,color="blue")
fcast2.plot(ax=ax,color="orange")
plt.show()
```
Evaluating forecasts in Python (and setting aside training/validation data):

```{python Forecasts and train}

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import pyplot
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import STL
usair_p=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\usairlines.csv")
df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair_p.index=pd.to_datetime(df)

train=usair_p.iloc[:207]
test=usair_p.iloc[207:]

fit=ExponentialSmoothing(train['Passengers'],trend='add',seasonal='mul',seasonal_periods=12).fit()

fcast=fit.forecast(12)
error=test['Passengers']-fcast
MAE=np.mean(abs(error))
MAE
MAPE=np.mean(abs(error)/test['Passengers'])
MAPE
```


## SAS Code for Exponential Smoothing Models

The following code is for Exponential Smoothing models in SAS.


_Create library for data sets_
libname Time 'Q:\My Drive\Fall 2017 - Time Series\Data'; \  
run;

_SIMPLE EXPONENTIAL SMOOTHING MODEL_

_Create a simple exponential smoothing model_ \  


proc esm data=Time.Steel print=all plot=all lead=24; \  
	forecast steelshp / model=simple; \  
run; \  

_Create a simple exponential smoothing model with ID statement_ \  

proc esm data=Time.USAirlines print=all plot=all lead=24; \  
	id date interval=month; \  
	forecast Passengers / model=simple; \  
run; \  


_LINEAR TREND FOR EXPONENTIAL SMOOTHING_ \  

_Double exponential smoothing_ \  

proc esm data=Time.Steel print=all plot=all lead=24; \  
	forecast steelshp / model=double; \  
run; \  

_linear exponential smoothing_ \  

proc esm data=Time.Steel print=all plot=all lead=24; \  
	forecast steelshp / model=linear; \  
	run; \  
	

_damped trend exponential smoothing_ \  

proc esm data=Time.Steel print=all plot=all lead=24; \  
	forecast steelshp / model=damptrend; \  
	run; \  
	

_linear exponential smoothign with interval = month_ \  

proc esm data=Time.USAirlines print=all plot=all lead=24; \  
	id date interval=month; \  
		forecast Passengers / model=linear; \  
		run; \  
		

_SEASONAL EXPONENTIAL SMOOTHING MODEL_ \  

_Additive seasonal exponential smoothing model_ \  

proc esm data=Time.USAirlines print=all plot=all \  
		 seasonality=12 lead=24 outfor=test1; \  
		 forecast Passengers / model=addseasonal; \  
		 run; \  
		 
_mulitplicative seasonal exponential smoothing model_ \  

proc esm data=Time.USAirlines print=all plot=all \  
		 seasonality=12 lead=24; \  
		 	forecast Passengers / model=multseasonal; \  
		 	run; \  
		 	

_Winters additive exponential smoothing model (includes trend)_ \  

proc esm data=Time.USAirlines print=all plot=all \  
		 seasonality=12 lead=24; \  
		 forecast Passengers / model=addwinters; \  
		 run; \  
		 
_Winters multiplicative exponential smoothing model (includes trend) (Lead = 24)_ \  

proc esm data=Time.USAirlines print=all plot=all \  
		 seasonality=12 lead=24; \  
		 forecast Passengers / model=multwinters; \  
		 run; \  
		 
_Winters multiplicative exponential smoothing model (includes trend) Lead = 12_ \  

proc esm data=Time.USAirlines print=all plot=all lead=12 \  
  back=12 seasonality=12; \  
  forecast Passengers / model=multwinters; \  
  run; \  
  

_EXPLORATION of SEASONAL EXPONENTIAL SMOOTHING MODEL_ \  


_Winters multiplicative exponential smoothing model (includes trend) Lead = 12, uses outfor statement to output forecasts_ \  

proc esm data=Time.USAirlines print=all plot=all \
		 seasonality=12 lead=12 back=12 outfor=test; \  
		 	forecast Passengers / model=multwinters; \  
		 	run; \ 

_calculate |error|/|actual value|_ \  

data test2; \  
set test; \  
if _TIMEID_>207; \  
abs_error=abs(error); \  
abs_err_obs=abs_error/abs(actual); \  
run; \  


_mean of |error|/|actual value| for this forecast_ \  

proc means data=test2 mean; \  
var abs_error abs_err_obs; \  
run;




<!--chapter:end:02-Exponential-Smoothing.Rmd-->

# ARIMA


```{r, message=FALSE,echo=FALSE}
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
file.dir = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/" 
input.file1 = "usairlines.csv"
input.file2 = "steel.csv"
input.file3 = "leadyear.csv"
input.file4 = "ebay9899.csv"
input.file5 = "fpp_insurance.csv" 
input.file6 = "ar2.csv"
input.file7 = "MA2.csv"
input.file8 = "hurrican.csv"

# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines = read.csv(paste(file.dir, input.file1,sep = ""))
Steel = read.csv(paste(file.dir, input.file2, sep = ""))
Lead.Year = read.csv(paste(file.dir, input.file3, sep = ""))
Ebay = read.csv(paste(file.dir, input.file4, sep = ""))
Quotes= read.csv(paste(file.dir, input.file5, sep = ""))
Y= read.csv(paste(file.dir, input.file6, sep = ""))
x=read.csv(paste(file.dir, input.file7, sep = ""))
hurricane=read.csv(paste(file.dir, input.file8, sep = ""))
SteelShp <- ts(Steel$steelshp, start = 1984, frequency = 12)
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)

```

We will now be switching over to doing ARMA/ARIMA models!!  There are a number of different concepts you will need in order to do this type of modeling.

## Stationarity

Before we can try to model the dependency structure (the AR and MA terms), we must first have a stationarity!  The ADF test is one of the most well-known and accepted test for testing stationarity.  There are several packages that will do this for you, however, below, I am focusing on the ADF test within the package aTSA.

```{r plot}

Quotes.ts<-ts(Quotes$Quotes,start=2002,frequency=12)
autoplot(Quotes.ts)+labs(title="Time Series of Daily Stock quotes", x="Time", y="Quotes")
```


The following code produces output similar to the output seen in SAS (under the tau test).


```{r stationarity}


# Perform the ADF test (k=0)
aTSA::adf.test(Quotes.ts)

```


## Correlation Functions


The Acf and the Pacf in R will calculate the autocorrelation (up to the lag you specify) and the partial autocorrelation, respectively.  You can either output these values to look at them or plot them (see code below).

```{r correlation functions}
acf1=Acf(Y, lag=10)$acf
pacf1=Pacf(Y, lag=10)$acf

index1=seq(1,length(pacf1))

all.dat=data.frame(cbind(acf1[2:11],pacf1,index1))
colnames(all.dat)=c("acf","pacf","index")

ggplot(all.dat,aes(x=factor(index),y=acf))+geom_col()+labs(x="Lags")
```


## AutoRegressive Models (AR)

AutoRegressive (AR) models involve modeling the lags of Y.  We can write an autoregressive model as

$$ Y_{t} = c + \phi_{1}Y_{t-1}+\phi_{2}Y_{t-2}+...\phi_{p}Y_{t-p}+\epsilon_{t} $$
Where there are *p* lags of *Y*.  Below is the code to fit an AR(2) model. The *order* in the *Arima* function needs the p,d,q values (p=# of AR terms, d=how many differences should be taken and q=# of MA terms).

```{r AR2}

ggAcf(Y)
ggPacf(Y)

Y.ts <- ts(Y)
Y.ARIMA <- Arima(Y.ts, order=c(2,0,0))

ggAcf(Y.ARIMA$residuals)
ggPacf(Y.ARIMA$residuals)

```


## Moving Average model (MA)

Moving average (MA) models involve modeling the lags of the error.  We can write a moving average model as

$$ Y_{t} = c - \theta_{1}\epsilon_{t-1}-\theta_{2}\epsilon_{t-2}-...\theta_{q}\epsilon_{t-q}+\epsilon_{t} $$
Where there are *q* lags of $\epsilon$.  Below is code to fit an MA(2) model.

```{r MA2}

ggAcf(x)
ggPacf(x)

x.ts <- ts(x)
x.ARIMA <- Arima(x.ts, order=c(0,0,2))
summary(x.ARIMA)
ggAcf(x.ARIMA$residuals)
ggPacf(x.ARIMA$residuals)

```


## White noise

For residuals to exhibit white noise, they must be "independent" and normally distributed with mean 0 and constant variance.  You already know how to assess normality and constant variance, however, we need to focus on assessing "independence".  We can assess if there is significant dependence through the Ljung-Box test. The hypotheses being tested are

$$H_{0}:No\quad significant\quad autocorrelation\\
H_{A}:Significant\qquad autocorrletion $$

This should be assessed on a stationary time series.  Looking at a stationary time series, going back 10 lags should be sufficient (this will be different when we get to seasonal models).  We would like for all of the p-values (for lags 1-10) to be insignificant (large p-values).  However, keep in mind that sample size will matter when assessing significance.

```{r Ljung-Box test}

White.LB <- rep(NA, 10)
for(i in 1:10){
  White.LB[i] <- Box.test(Y, lag=i, type="Ljung-Box", fitdf = 0)$p.value
}

white.dat=data.frame(cbind(White.LB,index1))
colnames(white.dat)=c("pvalues","Lag")

ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Ljung-Box test p-values",x="Lags",y="p-values")+coord_cartesian(ylim = c(0, 0.025))


####Fit appropriate model
Y.ARIMA=Arima(Y,order=c(2,0,0))
White.LB <- rep(NA, 10)
for(i in 3:10){
  White.LB[i] <- Box.test(Y.ARIMA$residuals, lag=i, type="Ljung-Box", fitdf = 2)$p.value
}

white.dat=data.frame(cbind(White.LB[3:10],index1[3:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Ljung-Box test when there is white noise",x="Lags",y="p-values")

```


## Trending Series

If the series is trending then it is NOT stationary.  You will NEED to do something to the series in order to make it stationary!!  You will either fit a linear regression line (then use the residuals to model dependencies) or take differences and use the differenced series to model dependencies.  We will be using the Ebay stock data of "Daily High" (however, this data has missing values!!).  If you do not impute missing values, it simply ignores the missing values (could create an issue).  I would recommend FIRST imputing those values BEFORE doing the ADF test (in the na_interpolation algorithm, missing values are imputed).

```{r adf test with missing values}
Daily.High <- ts(Ebay$DailyHigh)
###NOT appropriate since there are missing values!!
aTSA::adf.test(Daily.High)

ggplot_na_distribution(Daily.High)+labs(y="Stock prices for Ebay")

# Interpolate the missing observations in this data set
Daily.High<-Daily.High %>% na_interpolation(option = "spline")

autoplot(Daily.High)+labs(title="Daily high stock quotes",x="Time",y="Quotes")

# Perform an ADF test
aTSA::adf.test(Daily.High)
### Definitely a Random Walk!!
```


How do we fit each situation?  If the series is stationary about the line, we need to fit a line (and then model the AR and MA terms on the residuals).  If series is a random walk with drift, then need to take differences.  Here is the R code for each situation.

```{r ARIMA with trend}

###Fitting a regression line...
time.high=seq(1,length(Daily.High))
resid.y =lm(Daily.High ~ time.high)$resid
#Now use resid.y to explore what p and q to fit
ggAcf(resid.y)
ggPacf(resid.y)
### You can CLEARLY see this is a random walk...can't really fit p,q

## DO NOT RUN!!
#ARIMA.line=Arima(Daily.High,order=c(p,0,q),xreg=time.high)



####Fitting a random walk with drift
diff.y=diff(Daily.High)
ggAcf(diff.y) 
ggPacf(diff.y)
ARIMA.RW=Arima(Daily.High,order=c(1,1,0))
summary(ARIMA.RW)
####  automatic procedure will determine if you need differencing or not!

```


## Fitting ARIMA models

We can use an automatic procedure to help us find a model. For this illustration, we will be using the mean of the maximum velocity in the hurricane data set.  This data also has some missing values which we need to look into first.

```{r visualize missing values}

max.velocity=hurricane$MeanVMax

ggplot_na_distribution(max.velocity)+labs(y="Mean Max Velocity")

```

This is yearly data and the reason those values are missing is because there were no hurricanes recorded for that year.  Since there is no trend (nor seasonality), I am going to remove these NA values and then run the Dickey-Fuller test.

```{r hurrican ADF test}

max.velocity=na.omit(max.velocity)
hurrican.ts=ts(max.velocity)
aTSA::adf.test(hurrican.ts)

```

Series is stationary!! Let's see if there is any significant dependecies here...

```{r Ljung-Box for Hurricane data}
index1=seq(1,10)
White.LB <- rep(NA, 10)
for(i in 1:10){
  White.LB[i] <- Box.test(hurrican.ts, lag=i, type="Ljung-Box", fitdf = 0)$p.value
}

white.dat=data.frame(cbind(White.LB[1:10],index1[1:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Hurricane data",x="Lags",y="p-values")

```

There is definitely something to be modeled here!!  Let's try an automated search first...

```{r auto.arima for Hurricane data}

model1=auto.arima(hurrican.ts)
model2=auto.arima(hurrican.ts,d=0)

```

Let's take a look at ACF and PACF plots and see how well we do manually..

```{r Correlation plots hurricane data}

ggAcf(hurrican.ts)
ggPacf(hurrican.ts)

```


Using the graphs and some trial and error, here was the model I chose...



```{r by hand hurricane model}

model3=Arima(hurrican.ts,order=c(2,0,3))
summary(model3)

```


Comparing the ACF and PACF plots for these models:

```{r comparing ACF and PACF}

ggAcf(model1$residuals,lag.max = 10)
ggPacf(model1$residuals,lag.max = 10)
ggAcf(model2$residuals,lag.max = 10)
ggPacf(model2$residuals,lag.max = 10)
ggAcf(model3$residuals,lag.max = 10)
ggPacf(model3$residuals,lag.max = 10)

```


Let's take a look at white noise for each model:


```{r white noise}

index1=seq(1,10)
White.LB <- rep(NA, 10)
for(i in 2:10){
  White.LB[i] <- Box.test(model1$residuals, lag=i, type="Ljung-Box", fitdf = 1)$p.value
}

white.dat=data.frame(cbind(White.LB[2:10],index1[2:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Model 1",x="Lags",y="p-values")

White.LB <- rep(NA, 10)
for(i in 3:10){
  White.LB[i] <- Box.test(model2$residuals, lag=i, type="Ljung-Box", fitdf = 2)$p.value
}

white.dat=data.frame(cbind(White.LB[3:10],index1[3:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Model 2",x="Lags",y="p-values")

White.LB <- rep(NA, 10)
for(i in 6:10){
  White.LB[i] <- Box.test(model3$residuals, lag=i, type="Ljung-Box", fitdf = 5)$p.value
}

white.dat=data.frame(cbind(White.LB[6:10],index1[6:10]))
colnames(white.dat)=c("pvalues","Lag") 


ggplot(white.dat,aes(x=factor(Lag),y=pvalues))+geom_col()+labs(title="Model 3",x="Lags",y="p-values")

ggplot(data =hurrican.ts, aes(x = model1$residuals)) +
    geom_histogram() +
    labs(title = 'Histogram of Residuals for Model 1', x = 'Residuals', y = 'Frequency')
ggplot(data =hurrican.ts, aes(x = model2$residuals)) +
    geom_histogram() +
    labs(title = 'Histogram of Residuals for Model 2', x = 'Residuals', y = 'Frequency')

ggplot(data =hurrican.ts, aes(x = model3$residuals)) +
    geom_histogram() +
    labs(title = 'Histogram of Residuals for Model 3', x = 'Residuals', y = 'Frequency')


```


You can now forecast the data with these models:


```{r forecast ARIMA}

forecast::forecast(model1, h = 10)
autoplot(forecast::forecast(model1, h = 10))
autoplot(forecast::forecast(model2, h = 10))
autoplot(forecast::forecast(model3, h = 10))

```


## Python Code for ARMA/ARIMA models


```{python ADF, correlation plots and ARIMA}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from pandas import DataFrame
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA

quotes=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\fpp_insurance.csv")
y=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\ar2.csv")

result=adfuller(quotes["Quotes"])
print(f'ADF p-value: {result[1]}')

plot_acf(quotes["Quotes"],lags=12)
pyplot.show()
plot_pacf(quotes["Quotes"],lags=12)
pyplot.show

model = ARIMA(y, order=(2,0,0))
model_fit = model.fit()
print(model_fit.summary())

residuals = DataFrame(model_fit.resid)
residuals.plot()
pyplot.show()
print(residuals.describe())

plot_acf(residuals,lags=12)
pyplot.show()
plot_pacf(residuals,lags=12)
pyplot.show()
```


Checking for white noise:
The first value in the Ljung-Box test is the test statistic and the second value is the p-value.

```{python white noise with Ljung-Box}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from pandas import DataFrame
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA

quotes=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\fpp_insurance.csv")
y=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\ar2.csv")


model = ARIMA(y, order=(2,0,0))
model_fit = model.fit()
print(model_fit.summary())

lag_test=[3,4,5,6,7,8,9,10]

for x in lag_test:
  sm.stats.acorr_ljungbox(model_fit.resid, lags=[x], model_df=2)
  
```

Trending series...

```{python Trending series}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from pandas import DataFrame
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA


Daily_High_all=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\ebay9899.csv")
Daily_High=Daily_High_all["DailyHigh"]
Daily_High2=Daily_High.interpolate(option='spline')
result=adfuller(Daily_High2)
print(f'ADF p-value: {result[1]}')

##Fitting a Random Walk
model = ARIMA(y, order=(0,1,0))
model_fit = model.fit()

###NOT correct, but if you wanted to fit a regression to data
x=pd.Series(range(318))
model2=ARIMA(endog=Daily_High2,exog=x,order=[0,0,0])
model2_fit=model2.fit()
model2_fit

### Now you can model residuals from the regression (if that is the route you took)
  
```


Fitting ARIMA models.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from pandas import DataFrame
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA
import pmdarima as pm


hurricane=pd.read_csv("Q:\\My Drive\\Fall 2017 - Time Series\\DataR\\hurrican.csv")
max_velocity=hurricane["MeanVMax"]
max2=max_velocity.dropna()

### Testing stationarity 

result=adfuller(max2)
print(f'ADF p-value: {result[1]}')

### Same result as auto.arima in R!
model1=pm.auto_arima(max2, start_p=0,start_q=0,max_p=5,max_q=5,seasonal=False)
model1.summary()

# Force d=0
model2=pm.auto_arima(max2, start_p=0,start_q=0,max_p=5,max_q=5,d=0,seasonal=False)
model2.summary()



```


## SAS Code for ARMA/ARIMA


_AUGMENTED DICKEY-FULLER TESTING_

proc arima data=Time.fpp_insurance plot=all; \  
	identify var=quotes nlag=10 stationarity=(adf=2); \  
		identify var=quotes(1) nlag=10 stationarity=(adf=2); \  
		run;
quit;

_CORRELATION FUNCTIONS_ \

_Notice no model statement!_ \  

proc arima data=Time.ar2 plot(unpack)=all; \  
	identify var=y nlag=10 outcov=Corr; \  
		estimate method=ML; \  
		run;
quit;

_BUILDING AN AUTOREGRESSIVE MODEL_

_Fit an AR2 model_ \  

proc arima data=Time.AR2 plot=all; \  
	identify var=y nlag=10; \  
		estimate p=2 method=ML; \  
		run;
quit; \  


_Add another estimate statement_ 
proc arima data=Time.AR2 plot=all;
	identify var=y nlag=10;
	estimate p=(2) method=ML;
	estimate p=(1,2,4) method=ML;
run;
quit; \  


_BUILDING A MOVING AVERAGE MODEL_

proc arima data=Time.ma2; \  
	identify var=x; \  
		estimate q=2 method=ML; \  
		run;
quit; \  

_Need to check for how to take care of trend_

proc arima data=Time.Ebay9899 plot=all; \  
	identify var=DailyHigh nlag=10 stationarity=(adf=2); \  
	run;
quit;


_It is a random walk!!  The way to model a random walk is by using differences_ \  


proc arima data=Time.Ebay9899 plot=all; \  
	identify var=DailyHigh(1) nlag=10 stationarity=(adf=2); \  
	run;
quit;

_IF it would have been deterministic...CAUTION, it is NOT deterministic!!_ \  


_You would need to create an "x" variable for time_ \  

data newebay; \  
set Time.Ebay9899; \  
time=_n_; \  
run; \  


_Keep in mind that this data is not deterministic!!_ \  

 
proc arima data=newebay plot=all; \  
	identify var=DailyHigh crosscorr=time; \  
		estimate input=time; \  
		run;
quit;



_BUILDING AN AUTOREGRESSIVE MOVING AVERAGE MODEL_ \  
   _(AUTOMATIC SELECTION TECHNIQUES)_ \  
   


_Fit an ARIMA model_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 stationarity=(adf=2); \  
	run;
quit; \  


_Model identification with minimum information criterion (MINIC)_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 minic P=(0:12) Q=(0:12); \  
	run;
quit; \  


_Model identification with smallest canonical correlation (SCAN);_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 scan P=(0:12) Q=(0:12); \  
	run;
quit; \  


_Model identificaiton with extended sample autocorrelation function (ESACF)_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12 esacf P=(0:12) Q=(0:12); \  
	run;
quit; \  


_Create estimates with our ARIMA model p=2, q=3_ \  

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=12; \  
		estimate p=2 q=3 method=ML; \  
		run;
quit;

_FORECASTING_ \ 

proc arima data=Time.Hurricanes plot=all; \  
	identify var=MeanVMax nlag=10 ; \  
		estimate p=2 q=3 method=ML; \  
		forecast lead=10; \  
		run;
quit;



<!--chapter:end:03-ARIMA.Rmd-->

# Seasonality

```{r, message=FALSE,echo=FALSE}
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
library(prophet)
file.dir = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/" 
input.file1 = "usairlines.csv"
input.file2 = "steel.csv"
input.file3 = "leadyear.csv"
input.file4 = "ebay9899.csv"
input.file5 = "fpp_insurance.csv" 
input.file6 = "ar2.csv"
input.file7 = "MA2.csv"
input.file8 = "hurrican.csv"

# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines = read.csv(paste(file.dir, input.file1,sep = ""))
Steel = read.csv(paste(file.dir, input.file2, sep = ""))
Lead.Year = read.csv(paste(file.dir, input.file3, sep = ""))
Ebay = read.csv(paste(file.dir, input.file4, sep = ""))
Quotes= read.csv(paste(file.dir, input.file5, sep = ""))
Y= read.csv(paste(file.dir, input.file6, sep = ""))
x=read.csv(paste(file.dir, input.file7, sep = ""))
hurricane=read.csv(paste(file.dir, input.file8, sep = ""))
SteelShp <- ts(Steel$steelshp, start = 1984, frequency = 12)
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)
library(reticulate)
#use_python("C:\\ProgramData\\Anaconda3\\python.exe")
use_python("C:\\ProgramData\\Anaconda3\\envs\\R_Env2\\python.exe")
```

Seasonality is the component of the time series that represents the effects of the seasonal variation within the dataset. This seasonal effect can be thought of as repetitive behavior that occurs every S time periods. Here, S is the seasonal period that gets repeated every S units of time. Remember though that seasonal data is **not** stationary. This is because the series itself does not revert to an overall, constant mean.

Let's explore the U.S. airline passenger data from 1990 to 2007.

First, we load the dataset into a time series object using the `ts` function. We use the `start =` option to define the starting point for our dataset, which is Janurary of 1990. The `frequency =` specifies the length of the seasonal period in our dataset. Our data is monthly with an annual seasonal pattern, so our frequency here is 12. From there we split our data into two pieces - training and testing - using the `subset` function. For the training set, we are subsetting the data to exclude the last 12 months. These last 12 months will be our test data set.

```{r}
# Load the Data
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)

autoplot(Passenger) + labs(title="Time Series plot for Passengers", x="Date",y="Passengers")
```

```{r}
# Create training set from overall Airlines Data
training <- subset(Passenger, end = length(Passenger)-12)

# Create test set from overall Airlines Data
test <- subset(Passenger, start = length(Passenger)-11)
```

Now, let's explore our training data by looking at a time series decomposition. This time series decomposition breaks down the data into three pieces - season, trend, and error.

```{r}
# Time Series Decomposition ...STL#
decomp_stl <- stl(Passenger, s.window = 7)

# Plot the individual components of the time series
plot(decomp_stl)
```

From the above plot we can see the annual season of our data as well as the upward trending pattern. We do have a shock to the system from the September 11 attacks. This impacted US airline travel for a number of months. We will have to account for this in our modeling as we go along. We can even see this shock in the error term of our decomposition as it resulted in large errors that were out of pattern of the rest of the data.

A great first place to start with any time series dataset is to build an exponential smoothing model. Exponential smoothing models are great first models as they are easy to build and relatively accurate. This allows us to set a baseline to try and beat with more sophisticated modeling approaches.

To build a Holt-Winters exponential smoothing model to account for both the trend and seasonality in our data, we use the `hw` function. With the `seasonal =` option we can specify whether our data has additive or multiplicative seasonality. The `initial =` option just specifies that the initial values of the HW model are optimized in the actual model building process as compared to just estimated with a simple calculation like the overall mean. The `h =` specifies that we want to forecast 12 time periods into the future on our training dataset.

```{r}
# Fit Holt-Winters ESM (multiplicative seasonality) on training data
HWES.USAir.train <- hw(training, seasonal = "multiplicative", initial='optimal', h=12)
```

Now that we have our model, let's plot the forecast as well as evaluate this forecast with our test dataset. The `mean` element in the ESM model object gives the 12 month forecast that we specified above. We can look at the difference between this forecast and our test dataset - what actually happened in those 12 months. From there we calculate the MAE and MAPE of this forecast. The `autoplot` function is used to visualize the forecast itself.

```{r}
# Calculate prediction errors from forecast
HW.error <- test - HWES.USAir.train$mean

# Calculate prediction error statistics (MAE and MAPE)
HW.MAE <- mean(abs(HW.error))
HW.MAPE <- mean(abs(HW.error)/abs(test))*100

HW.MAE
```

```{r}
HW.MAPE
```

```{r}
autoplot(HWES.USAir.train)+
  autolayer(fitted(HWES.USAir.train),series="Fitted")+ylab("Airlines Passengers")+ geom_vline(xintercept = 2007.25,color="orange",linetype="dashed")
```

From the output above, we see that we have an MAE of 1134.58 and a MAPE of 1.76%. This gives us a good baseline to try and beat with further modeling approaches.

## Seasonal Unit Root Testing

Exponential smoothing models do not require data to be stationary. However, ARIMA models do require stationary data for modeling. Since seasonal data is not stationary, we must account for this lack of stationarity in one of two broad categories of approaches - deterministic solutions and stochastic solutions.

Some examples of deterministic solutions to seasonality are seasonal dummy variables, Fourier transforms, and predictor variables. The stochastic solution to seasonality is taking a seasonal difference. Be careful, as the choice of these solutions will matter for modeling. This is why we need to evaluate which approach we need using seasonal unit root testing. Similar to trending data, unit root testing allows us to determine if differencing is the proper solution to make our seasonal data stationary. There are many tests for seasonal unit-root testing. Each test is a little different on what the null and alternative hypotheses are so we must be careful to know what we are looking it with results.

The `nsdiffs` function runs seasonal unit root tests and reports the number of seasonal differences that are needed for your dataset. If 0, that **does not** mean your data is stationary. It just means that your seasonality cannot be solved with differencing. If more than 0, then seasonal differencing is the solution to the seasonality problem of our data.

```{r}
training %>% nsdiffs()
```

```{r}
cbind("Airlines Passengers" = training,
      "Annual change in Passengers" = diff(training, 12)) %>%
  autoplot(facets=TRUE) +
  xlab("Time") + ylab("") +
  ggtitle("Comparison of Differenced Data to Original")
```

We can see from the output above that we need one seasonal difference to account for the seasonality in our dataset. That means we need to solve our seasonality stochastically. The plot shows both the original data as well as the differenced data. This is not the end of our unit root testing however. There might still be a "regular" unit root in our data. Our data does not appear to be trending, but it still might contain a unity root. Again, we will want to test for whether our data requires an additional difference or whether it is stationary.

The `ndiffs` function applied to the differenced data will help with this. The `diff` function is used with a `lag = 12` to take our seasonal difference.

```{r}
training %>% diff(lag = 12) %>% ndiffs()
```

From above we see that we require no more additional differencing. Since our data no longer appears to have a trend or season, we are ready to go to ARIMA modeling.

## Deterministic Solutions

If our dataset required a deterministic solution instead of a stochastic one, how would we go about solving this? Again, our dataset **does not** need a deterministic solution, but let's explore how we would solve seasonality deterministically for completion sake.

Unlike trend, there are many different approaches to accounting for seasonality deterministically. The approaches we will discuss are seasonal dummy variables, Fourier transforms, and predictor variables.

### Seasonal Dummy Variables

Seasonal dummy variables are exactly what their name implies. We will use a set of dummy variables to account for the seasonal effects in our dataset. For a time series with *S* periods within a season, there will be *S-1* dummy variables - one for each period (and one accounted for with the intercept). Since our dataset is monthly, we will build a dummy variable for each month and pick one to not include. For example, we could build the following model:

$$
Y_t = \beta_0 + \beta_1JAN + \beta_2FEB + \cdots + \beta_{11}NOV + e_t 
$$

In the above model, we chose to leave out the month of December's dummy variable since our model has an intercept. In fact, the intercept $\beta_0$ is the average effect of December. Each of the coefficients on the respective variables measures the average difference in effect from that month and December (our reference level).

There are a lot of ways to build month variables in a dataset. You could extract month from a time stamp variable for example. Here, we create our own month variable by repeating the values 1 to 12 across our dataset using the `rep` function. We then convert this to a factor with the `factor` function as well as set the reference level using the `relevel` function.

```{r}
# Seasonal Dummy Variables
Month <- rep(0, length(training))
Month <- Month + 1:12
```

```{r}
M <- factor(Month)
M <- relevel(M, ref="12")
```

By using an `lm` function we can see how building a linear regression with this factor variable would look. This is the exact same thing that is going on with the modeling in our data. If we were to use this factor variable in an ARIMA modeling function like `auto.arima`, the function will first build a linear regression and then evaluate which ARIMA model we need on the residuals from our model. Essentially, this "removes" the impact of the season from our dataset. The `auto.arima` function's `xreg =` option is how we incorporate any external variables (here our dummy variables) into the model. The `seasonal = FALSE` option tells the function to not try and account for season itself since we are accounting for it with seasonal dummy variables.

```{r}
Season.Lin <- lm(training ~ M)
summary(Season.Lin)
```

```{r}
M.Matrix <- model.matrix(~M)

Trend <- 1:length(training)

SD.ARIMA <- auto.arima(training, xreg = M.Matrix[,2:12], method="ML", seasonal = FALSE)
summary(SD.ARIMA)
```

There are some advantages and disadvantages to the seasonal dummy variable approach. The advantages are that the model's seasonal dummy variables have some nice interpretability. They tells us the average impact each seasonal component has on the target variable. It is also a rather straight-forward approach to implement. The main disadvantages however are that if your seasonal is especially long or complex, seasonal dummy variables are burdensome. Also, the constant effect of each season that is assumed may not be appropriate.

### Fourier Transforms

Harmonic regression using Fourier transforms is another approach to account for seasonality deterministically. Fourier showed that a series of sine and cosine terms of the right frequencies approximate periodic patterns in a data series. To do this, we add Fourier variables to a regression model to account for the seasonal pattern. The odd terms $k=1,3,5$ etc. are accounted for with sine variables:

$$
X_{k, t} = \sin(k \times \frac{2\pi t}{S})
$$

The even terms $k = 2, 4, 6$ etc. are accounted for with cosine variables:

$$
X_{k, t} = \cos(k \times \frac{2\pi t}{S})
$$

In fact, if you add the same number of Fourier variables as you have seasonal dummy variables, you will get the same predictions. However, we typically do not need all of the Fourier variables, especially with large values of *S*. We need to decide how many of these values we need. One way to approach this is to build many models, while increasing the number of Fourier terms in the models and compare them all via a metric like BIC.

Here, we loop through the first 6 Fourier terms to account for seasonality. We do this by adding the `fourier` function to the `auto.arima` function through the `xreg =` option. All of this is contained within a loop where we gradually increase the number of Fourier terms in the regression. The `K =` option in the `fourier` function does this. We loop through the first 6 Fourier terms (half of the seasonal length) and record the training BIC for each model. This BIC is reported on a plot with the 6 different forecasts (one from each model). The `gridExtra` package's `grid.arrange` function allows the plots to all be put together.

```{r}
plots <- list()
for (i in seq(6)) {
  fit <- auto.arima(training, xreg = fourier(training, K = i),
                    seasonal = FALSE, lambda = NULL)
  plots[[i]] <- autoplot(forecast::forecast(fit,
                                  xreg = fourier(training, K=i, h=12))) +
    xlab(paste("K=",i,"   BIC=",round(fit$bic,2))) +
    ylab("") + ylim(30000,80000)
}
gridExtra::grid.arrange(
  plots[[1]],plots[[2]],plots[[3]],
  plots[[4]],plots[[5]],plots[[6]], nrow=3)
```

From the above output we see that the best model had 6 Fourier terms. So we can build this model to account for the seasonality in our dataset.

```{r}
F.ARIMA <- auto.arima(training, xreg = fourier(training, K = 6), seasonal = FALSE)
summary(F.ARIMA)
```

There are some advantages and disadvantages of the Fourier term approach to accounting for seasonality. The main advantage is that Fourier terms can handle long and complex seasonality. In fact, for multiple seasons in a dataset, we can add more Fourier variables at different frequencies to account for this. The disadvantages of the Fourier approach are that the Fourier terms themselves are not really interpretable and that we need to use trial and error to find the "right" amount of variables to use.

After removing the seasonality through deterministic solutions, the remaining error term (residuals) are modeled with seasonal ARIMA models. The key to these new seasonal ARIMA models is that there still might be effects at seasonal lags in the dataset, even though the main seasonality is accounted for.

## Stochastic Solutions

The stochastic approach has been hinted at above. When a stochastic solution is best to solve seasonality, we need to take seasonal differences. A difference on a season is when we look at the difference between the current point and the same point in the previous season: $Y_t - Y_{t-S}$. For our dataset, it can be thought of as the year over year change in our data.

Here the `ggtsdisplay` function allows us to see the actual difference along with its correlation pattern with ACF and PACF plots. Notice, we are looking at these ACF and PACF plots with regards to the differences on the training data using the `diff` function with `lag = 12` specifying the length of the difference being the same as our season.

```{r}
training %>% diff(lag = 12) %>% ggtsdisplay()
```

After we remove the seasonal effect through stochastic approaches, the remaining differences are modeled with seasonal ARIMA models. The key to these new seasonal ARIMA models is that there still might be effects at seasonal lags in the dataset, even though the main seasonality is accounted for.

There are some limitations to differencing. Differencing is hard to evaluate for long and complex seasons due to the statistical tests for stochastic differencing typically ended at a season length of 24. Therefore, long and/or complex seasons are typically best approached with deterministic solutions. In fact, it is hard to imagine a difference being very long in terms of time points logically. For example, if you had daily data and thought you had an annual season, it is hard to imagine there is an actual impact of Jan 26 from last year on Jan 26 of this year.

## Seasonal ARIMA Modeling

When extending the ARIMA model framework to the seasonal ARIMA framework, we add another set of terms - P, D, Q, and S. Notice how these terms are capitalized.

$$
ARIMA(p, d, q)(P, D, Q)_S
$$

The terms represent the number of *seasonal* AR terms ($P$), the number of *seasonal* MA terms ($Q$), and the number of *seasonal* differences ($D$). The length of the season is still defined as $S$. Seasonal ARIMA models have the same structure and approach as typical ARIMA models with AR and MA patterns in the ACF and PACF. THe main difference is that the pattern is just on the seasonal lag instead of the individual lags. For example, if you had an $ARIMA(0,0,0)(1,0,0)_{12}$ model, then the ACF would have an exponentially decreasing pattern *every 12 lags* while the PACF only had a single spike *at lag 12*. The pattern is the same, but it is only seen on the season since we are dealing with the seasonal lag. The opposite would be true for an $ARIMA(0,0,0)(0,0,1)_{12}$ model. The PACF would have an exponentially decreasing pattern *every 12 lags* while the ACF only had a single spike *at lag 12*.

For our data we can try the $ARIMA(1,0,0)(1,1,1)_{12}$ model using the `Arima` function. With either the `ggtsdisplay` function on the residuals, or the `checkresiduals` function on the model object, we can see that the model still doesn't capture all of the signal and pattern in the dataset.

```{r}
# Seasonal ARIMA 
training %>% 
  Arima(order=c(1,0,0), seasonal=c(1,1,1)) %>%
  residuals() %>% ggtsdisplay()
```

```{r}
S.ARIMA <- Arima(training, order=c(1,0,0), seasonal=c(1,1,1))
summary(S.ARIMA)
```

```{r}
checkresiduals(S.ARIMA)
```

We can still use the `auto.arima` function to select the "best" starting point model for us as well. We can use the original data in this function along with the `seasonal = TRUE` option to allow the function to take the seasonal difference for us.

```{r}
S.ARIMA <- auto.arima(training, method="ML", seasonal = TRUE)
summary(S.ARIMA)
```

```{r}
checkresiduals(S.ARIMA)
```

Above we can see that the `auto.arima` function selected the $ARIMA(1,0,1)(0,1,1)_{12}$ model. By using the `checkresiduals` function we see that we are left with white noise after our modeling. We still have a large outlier that we will have to account for in the next section on dynamic regression.

We can use the `forecast` function with `h = 12` to forecast the next 12 observations in our dataset. Similar to the exponential smoothing model above, we plot the forecast as well as evaluate the MAE and MAPE from this model.

```{r}
forecast::forecast(S.ARIMA, h = 12)
```

```{r}
autoplot(forecast::forecast(S.ARIMA, h = 12)) + autolayer(fitted(S.ARIMA), series="Fitted") + 
  ylab("Airlines Passengers") + 
  geom_vline(xintercept = 2007.25,color="orange",linetype="dashed")
```

```{r}
S.ARIMA.error <- test - forecast::forecast(S.ARIMA, h = 12)$mean

# Calculate prediction error statistics (MAE and MAPE)
S.ARIMA.MAE <- mean(abs(S.ARIMA.error))
S.ARIMA.MAPE <- mean(abs(S.ARIMA.error)/abs(test))*100

S.ARIMA.MAE
```

```{r}
S.ARIMA.MAPE
```

From the above output we see that our seasonal ARIMA did not beat the Holt-Winters exponential smoothing model in terms of MAE or MAPE. That outlier might be impacting the estimation of our model, so we will have to address that.

The default for the `Arima` function in R (which is what `auto.arima` is built off of) uses the multiplicative approach to seasonal ARIMA models. To get an additive seasonal ARIMA model instead we need to use the `fixed =` option in the `Arima` function to specify exactly what terms we want to estimate instead as shown below. The zeroes are for terms we *do not* want to estimate, while the *NA* values will be estimated by the function.

```{r}
# Additive Seasonal ARIMA  
S.ARIMA <- Arima(training, order=c(1,0,13), seasonal=c(0,1,0),
                 fixed=c(NA,NA,0,0,0,0,0,0,0,0,0,0,NA,NA), method="ML",)
summary(S.ARIMA)
```

## Python Code for Seasonality

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.graphics import tsaplots
from statsmodels.graphics import tsaplots
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.arima_model import ARMA
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing
```

```{python}
usair = pd.read_csv("https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/usairlines.csv")

df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair.index=pd.to_datetime(df)

training = usair.head(207)
test = usair.tail(12)
```

```{python}
plt.plot(training["Passengers"])
plt.xlabel("Date")
plt.ylabel("Passengers")
plt.title("Airline Passengers")
plt.show()
```

```{python}
training['HWES_MUL'] = ExponentialSmoothing(training['Passengers'], trend='mul', seasonal='mul', seasonal_periods = 12).fit().fittedvalues

training[['Passengers', 'HWES_MUL']].plot(title='Holt Winters Exponential Smoothing: Additive and Multiplicative Seasonality')
plt.show()
```

```{python}
fit = ExponentialSmoothing(training['Passengers'],trend='mul',seasonal='mul',seasonal_periods=12).fit()

plt.cla()
fcast=fit.forecast(12)
ax = training["Passengers"].plot(figsize=(12,8))
fcast.plot(ax = ax,color = "orange")
plt.show()
```

```{python}
error=test['Passengers']-fcast
MAPE=np.mean(abs(error)/test['Passengers'])*100
MAPE
```

### Seasonality

```{python}
from statsmodels.tsa.stattools import adfuller

training["Passengers_diff"] = training["Passengers"] - training["Passengers"].shift(12)

plt.cla()
plt.plot(training["Passengers_diff"].dropna())
plt.xlabel("Date")
plt.ylabel("Passengers")
plt.title("Seasonal Difference in Airline Passengers")
plt.show()
```

```{python}
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import kpss

adfuller(training["Passengers_diff"].dropna())
kpss(training["Passengers_diff"].dropna())
```

### Deterministic Solutions

```{python}
results = sm.OLS.from_formula("Passengers ~ C(Month)",
                              data=training).fit()
print(results.summary())
```

```{python}
import pmdarima as pm

model = pm.auto_arima(results.resid.dropna(), start_p = 0, start_q = 0, max_p = 5, max_q = 5, seasonal = False)
model.summary()
```

```{python}
training['t'] = range(1, 208, 1)

training['x1'] = np.sin(2*np.pi*training['t']/12)
training['x2'] = np.cos(2*np.pi*training['t']/12)
training['x3'] = np.sin(4*np.pi*training['t']/12)
training['x4'] = np.cos(4*np.pi*training['t']/12)
training['x5'] = np.sin(6*np.pi*training['t']/12)
training['x6'] = np.cos(6*np.pi*training['t']/12)
training['x7'] = np.sin(8*np.pi*training['t']/12)
training['x8'] = np.sin(8*np.pi*training['t']/12)
training['x9'] = np.sin(10*np.pi*training['t']/12)
training['x10'] = np.sin(10*np.pi*training['t']/12)
training['x11'] = np.sin(12*np.pi*training['t']/12)
training['x12'] = np.sin(12*np.pi*training['t']/12)

results = sm.OLS.from_formula("Passengers ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x12",
                              data=training).fit()
print(results.summary())

model = pm.auto_arima(results.resid.dropna(), start_p = 0, start_q = 0, max_p = 5, max_q = 5, seasonal = False)
model.summary()
```

### Stochastic Solution

```{python}
df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair.index=pd.to_datetime(df)

training = usair.head(207)
test = usair.tail(12)

training["Passengers_diff"] = training["Passengers"] - training["Passengers"].shift(12)

plt.cla()
plt.plot(training["Passengers_diff"].dropna())
plt.xlabel("Date")
plt.ylabel("Passengers")
plt.title("Seasonal Difference in Airline Passengers")
plt.show()
```

```{python}
from matplotlib import pyplot
from statsmodels.graphics.tsaplots import plot_acf

plot_acf(training['Passengers_diff'].dropna(), lags = 36)
pyplot.show()
```

```{python}
from statsmodels.graphics.tsaplots import plot_pacf

plot_pacf(training['Passengers_diff'].dropna(), lags = 36)
pyplot.show()
```

### Seasonal ARIMA

```{python}
from statsmodels.tsa.statespace.sarimax import SARIMAX

model2 = SARIMAX(training['Passengers'], order = (1,0,0), seasonal_order = (1,1,1,12)).fit()
model2.summary()
```

```{python}
lag_test=[4,5,6,7,8,9,10,11,12,13]

## Not run
#for x in lag_test:
#  sm.stats.acorr_ljungbox(model.resid, lags=[x], model_df=2)
```

```{python}
plot_acf(model2.resid, lags = 36)
pyplot.show()
```

```{python}
plot_pacf(model2.resid, lags = 36)
pyplot.show()
```

```{python}
model3 = pm.auto_arima(training['Passengers'], start_p = 0, start_q = 0, max_p = 5, max_q = 5, seasonal = True, m = 12)
model3.summary()
```

```{python}
model3 = SARIMAX(training['Passengers'], order = (1,0,1), seasonal_order = (0,1,1,12), trend = 't').fit()
model3.summary()

plt.cla()
fcast2 = model3.forecast(12)
ax = training["Passengers"].plot(figsize=(12,8))
fcast2.plot(ax = ax,color = "orange")
plt.show()
```

```{python}
error2 = test['Passengers']-fcast2
MAPE2 = np.mean(abs(error2)/test['Passengers'])*100
MAPE2
```

<!--chapter:end:04-Seasonality.Rmd-->

# Dynamic Regression Models

```{r, message=FALSE,echo=FALSE}
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
library(prophet)
library(reticulate)
use_python("C:\\ProgramData\\Anaconda3\\envs\\R_Env2\\python.exe")

file.dir = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/" 
input.file1 = "usairlines.csv"
input.file2 = "steel.csv"
input.file3 = "leadyear.csv"
input.file4 = "ebay9899.csv"
input.file5 = "fpp_insurance.csv" 
input.file6 = "ar2.csv"
input.file7 = "MA2.csv"
input.file8 = "hurrican.csv"

# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines = read.csv(paste(file.dir, input.file1,sep = ""))
Steel = read.csv(paste(file.dir, input.file2, sep = ""))
Lead.Year = read.csv(paste(file.dir, input.file3, sep = ""))
Ebay = read.csv(paste(file.dir, input.file4, sep = ""))
Quotes= read.csv(paste(file.dir, input.file5, sep = ""))
Y= read.csv(paste(file.dir, input.file6, sep = ""))
x=read.csv(paste(file.dir, input.file7, sep = ""))
hurricane=read.csv(paste(file.dir, input.file8, sep = ""))
SteelShp <- ts(Steel$steelshp, start = 1984, frequency = 12)
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)


```

```{r include=FALSE}
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)

# Create training set from overall Airlines Data
training <- subset(Passenger, end = length(Passenger)-12)

# Create test set from overall Airlines Data
test <- subset(Passenger, start = length(Passenger)-11)
```

Predictor variables are used for variety of things. Previously we have seen them used for accounting for trend and for seasonality. However, we can also use external variables to help make our forecasts better. Variables such as holiday effects, sales promotions, economic factors, and changes in policy are just a few examples.

Predictor variables are incorporated in a regression and the time series component is applied to the errors from this regression model:

$$
Y_t = \beta_0 + \beta_1X_1 + \cdots + \beta_kX_k + Z_t
$$

The $Z_t$ in the above equation is where the time series (typically ARIMA) model is applied. This form of modeling has many names - dynamic regression, ARIMAX, transfer functions, just to name a few.

## Intervention Variables

An intervention variable is a variable that contains discrete values that flag the occurrence of an event affecting the response series. These variables are used to model and forecast the series itself or analyze the impact of the specific event. For example, We can measure the impact of a previous sales promotion and forecast a future sales promotion\'s impact. We add these discrete variables in models to adjust the intercept of the model during the events.

The three most common types of intervention variables are:

-   Point interventions

-   Shift interventions

-   Ramp interventions

A **point intervention** is typically denoted with a binary variable that flags when event occurs by taking a value of 1 with all other values set to zero. By putting this variable in our model, the coefficient on the intervention variable in the regression measures the estimated impact of that intervention.

A **step intervention** is typically denoted with a binary variable that flags when an event occurs as well as the time period that the effects of the event last. For example, if you have a change in policy, you would have 0 values for dates before the policy change and values of 1 for every date after the change in policy. By putting this variable in our model, the coefficient on the intervention variable in the regression measures the estimated impact of that intervention\'s shift.

A **ramp intervention** is typically denoted by 0 values before an event and values that increase by 1 (1,2,3, etc.) starting with the event time point itself. By putting this variable in our model, the coefficient on the intervention variable in the regression measures the estimated slope of this new relationship after the event.

For our dataset we have a point intervention at September, 2001. We can easily create a binary variable that takes a value of 1 during that month and 0 otherwise. By using the `xreg =` option in the `auto.arima` function, we can add this intervention to our model as well as build out a seasonal ARIMA. We use the `summary` and `checkresiduals` functions to evaluate our model.

```{r}
Sep11 <- rep(0, 207)
Sep11[141] <- 1

Full.ARIMA <- auto.arima(training, seasonal = TRUE, xreg = Sep11, method = "ML")
summary(Full.ARIMA)
```

```{r}
checkresiduals(Full.ARIMA)
```

We can see from the output above that our model now accounts for the outlier that we previously saw in our data. Notice how our model has changed slightly after accounting for this outlier. This will improve the forecasts going forward, even though we do not have any outliers in our validation dataset.

To forecast these values going forward, we need future values of our predictor variable. For this dataset, we have our *Sep11* variable that we forecast to have values of 0 for each of the 12 observations we are forecasting for airline passengers. To do this we create another variable (of the same name) that has future values. This new variable is then put into the `xreg =` option inside of the `forecast` function. Notice how this forecast function comes from the `forecast` package with the input being the model we previously built. The `forecast` function is expecting a variable in the `xreg =` option with the same name as in our model.

```{r}
Sep11 <- rep(0, 12)

forecast::forecast(Full.ARIMA, xreg = Sep11, h = 12)
```

```{r}
autoplot(forecast::forecast(Full.ARIMA, xreg = Sep11, h = 12)) + autolayer(fitted(Full.ARIMA), series="Fitted") + 
  ylab("Airlines Passengers") + 
  geom_vline(xintercept = 2007.25,color="orange",linetype="dashed")
```

We can plot our forecasts just as we have done previously.

## Predictor Variables

Most forecasting models also need to account for explanatory variables such as price, advertising spending, or income. These kinds of models can be called any of the following: dynamic regression, ARIMAX, transfer functions. Through the examples above, we have seen how to implement this in R.

However, there are often **lagged variables** (lags of the predictor variables) as well as (or instead of) immediate impacts of these variables. In other words, previous values of the predictor variables may still play a role in the current prediction of the target variable. The question becomes, how many lags of a variable need to be included in the model. There are multiple ways to evaluate how many lags of a predictor variable you need in a model.

The first, more theoretical approach, involves cross correlations and pre-whitening of series. This approach is time consuming, requires building a model for the predictor variables themselves, and is therefore best used for small numbers of predictor variables.

The second approach evaluates many difference combinations of lags of the predictor variable in many different models. These models are compared on a metric like AIC/BIC on a validation dataset to see which models the data best. This approach is more efficient, handles many variables much easier, and is similar in accuracy to the first approach.

For our model, the impact of September 11, 2001 was felt for many months (lag impacts). In the code below we add six lags as well as one seasonal lag (the first anniversary of September 11, 2001) to the model through the same `xreg =` option in `auto.arima`.

```{r}
# Intervention Analysis with Lags
Sep11 <- rep(0, 207)
Sep11[141] <- 1

Sep11.L1 <- rep(0, 207)
Sep11.L1[142] <- 1

Sep11.L2 <- rep(0, 207)
Sep11.L2[143] <- 1

Sep11.L3 <- rep(0, 207)
Sep11.L3[144] <- 1

Sep11.L4 <- rep(0, 207)
Sep11.L4[145] <- 1

Sep11.L5 <- rep(0, 207)
Sep11.L5[146] <- 1

Sep11.L6 <- rep(0, 207)
Sep11.L6[147] <- 1

Anniv <- rep(0, 207)
Anniv[153] <- 1

Full.ARIMA <- auto.arima(training, seasonal = TRUE, xreg = cbind(Sep11, Sep11.L1, Sep11.L2, Sep11.L3, Sep11.L4, Sep11.L5, Sep11.L6, Anniv), method = "ML")
summary(Full.ARIMA)
```

```{r}
checkresiduals(Full.ARIMA)
```

From the output above, we see that the model produces white noise and does not appear to have any more intervention impacts in the dataset.

Forecasting in time series with only lagged values of the target variable is easy because of the recursive nature of the function - one prediction feeds into the next prediction. Forecasting with external variables is much trickier. For intervention variables it is easier since we should know future holiday, promotion, etc. values. However, some variables need forecasting. We could either use external estimate of future values or we might need to forecast future values ourselves.

To forecast this series, we need to have future values of these input variables. Luckily, for our variables, these are just 0 values. We still need to use the `forecast` function from the `forecast` package. This function also has an `xreg =` option where we need future values of these predictor variables. Ideally, these future values have the same name as the original predictor variables for the `forecast` function to work best. We can plot these updated forecasts as well as calculate the MAE and MAPE.

```{r}
Sep11 <- rep(0, 12)
Sep11.L1 <- rep(0, 12)
Sep11.L2 <- rep(0, 12)
Sep11.L3 <- rep(0, 12)
Sep11.L4 <- rep(0, 12)
Sep11.L5 <- rep(0, 12)
Sep11.L6 <- rep(0, 12)
Anniv <- rep(0, 12)

forecast::forecast(Full.ARIMA, xreg = cbind(Sep11, Sep11.L1, Sep11.L2, Sep11.L3, Sep11.L4, Sep11.L5, Sep11.L6, Anniv), h = 12)
```

```{r}
autoplot(forecast::forecast(Full.ARIMA, xreg = cbind(Sep11, Sep11.L1, Sep11.L2, Sep11.L3, Sep11.L4, Sep11.L5, Sep11.L6, Anniv), h = 12)) + autolayer(fitted(Full.ARIMA), series="Fitted") + 
  ylab("Airlines Passengers") + 
  geom_vline(xintercept = 2007.25,color="orange",linetype="dashed")
```

```{r}
# Calculate prediction errors from forecast
Full.ARIMA.error <- test - forecast::forecast(Full.ARIMA, xreg = cbind(Sep11, Sep11.L1, Sep11.L2, Sep11.L3, Sep11.L4, Sep11.L5, Sep11.L6, Anniv), h = 12)$mean

# Calculate prediction error statistics (MAE and MAPE)
Full.ARIMA.MAE <- mean(abs(Full.ARIMA.error))
Full.ARIMA.MAPE <- mean(abs(Full.ARIMA.error)/abs(test))*100

Full.ARIMA.MAE
```

```{r}
Full.ARIMA.MAPE
```

From the above output we see that although the MAE and MAPE have improved from the ARIMA model without accounting for September 11, but still not better than the exponential smoothing model.

## Python Code for Dynamic Regression Models

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.graphics import tsaplots
from statsmodels.graphics import tsaplots
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.arima_model import ARMA
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing
import pmdarima as pm
usair = pd.read_csv("https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/usairlines.csv")

df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair.index=pd.to_datetime(df)

training = usair.head(207)
test = usair.tail(12)


training['Sep11'] = 0 * 207
training['Sep11'][140] = 1

results = sm.OLS.from_formula("Passengers ~ Sep11",
                              data=training).fit()
print(results.summary())

model = pm.auto_arima(results.resid.dropna(), start_p = 0, start_q = 0, max_p = 5, max_q = 5, start_P = 0, start_Q = 0, max_D = 1, max_P = 2, map_Q = 2, seasonal = True, m = 12)
model.summary()
```

### Intervention Variables

```{python}
#training['Sep11'] = 0 * 207
#training['Sep11'][140] = 1

#model3 = SARIMAX(training['Passengers'], exog = training['Sep11'], order = (1,0,1), seasonal_order = (0,1,1,12), trend = 't').fit()
#model3.summary()

#plt.cla()
#fcast2 = model3.forecast(12)
#ax = training["Passengers"].plot(figsize=(12,8))
#fcast2.plot(ax = ax,color = "orange")
#plt.show()
```

### Predictor Variables

```{python}
training['Sep11'] = 0 * 207
training['Sep11'][140] = 1
training['Sep11_L1'] = 0 * 207
training['Sep11_L1'][141] = 1
training['Sep11_L2'] = 0 * 207
training['Sep11_L2'][142] = 1
training['Sep11_L3'] = 0 * 207
training['Sep11_L3'][143] = 1
training['Sep11_L4'] = 0 * 207
training['Sep11_L4'][144] = 1
training['Sep11_L5'] = 0 * 207
training['Sep11_L5'][145] = 1
training['Sep11_L6'] = 0 * 207
training['Sep11_L6'][146] = 1
training['Anniv'] = 0 * 207
training['Anniv'][152] = 1

X = np.array((training['Sep11'],
             training['Sep11_L1'],
             training['Sep11_L2'],
             training['Sep11_L3'],
             training['Sep11_L4'],
             training['Sep11_L5'],
             training['Sep11_L6'],
             training['Anniv']))

X = np.transpose(X)

model4 = SARIMAX(training['Passengers'], exog = X, order = (2,0,0), seasonal_order = (1,1,1,12), trend = 't').fit()
model4.summary()

```

```{python}
#lag_test=[4,5,6,7,8,9,10,11,12,13]

#for x in lag_test:
 # sm.stats.acorr_ljungbox(model4.resid, lags=[x], model_df=2)
```

```{python}
test['Sep11'] = 0 * 12
test['Sep11_L1'] = 0 * 12
test['Sep11_L2'] = 0 * 12
test['Sep11_L3'] = 0 * 12
test['Sep11_L4'] = 0 * 12
test['Sep11_L5'] = 0 * 12
test['Sep11_L6'] = 0 * 12
test['Anniv'] = 0 * 12

X_test = np.array((test['Sep11'],
             test['Sep11_L1'],
             test['Sep11_L2'],
             test['Sep11_L3'],
             test['Sep11_L4'],
             test['Sep11_L5'],
             test['Sep11_L6'],
             test['Anniv']))

X_test = np.transpose(X_test)

plt.cla()
fcast4 = model4.forecast(12, exog = X_test)
ax = training["Passengers"].plot(figsize=(12,8))
fcast4.plot(ax = ax,color = "orange")
plt.show()
```

```{python}
error4 = test['Passengers']-fcast4
MAPE4 = np.mean(abs(error4)/test['Passengers'])*100
MAPE4
```


<!--chapter:end:05-DynamicRegression.Rmd-->

# Prophet

```{r, message=FALSE,echo=FALSE}
library(tseries)
library(forecast)
library(haven)
library(fma)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(ggplot2)
library(seasonalview)
library(aTSA)
library(imputeTS)
library(prophet)
file.dir = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/" 
input.file1 = "usairlines.csv"
input.file2 = "steel.csv"
input.file3 = "leadyear.csv"
input.file4 = "ebay9899.csv"
input.file5 = "fpp_insurance.csv" 
input.file6 = "ar2.csv"
input.file7 = "MA2.csv"
input.file8 = "hurrican.csv"

# Reads the data at specified directory
# If the file directory is incorrect, then this won't run
USAirlines = read.csv(paste(file.dir, input.file1,sep = ""))
Steel = read.csv(paste(file.dir, input.file2, sep = ""))
Lead.Year = read.csv(paste(file.dir, input.file3, sep = ""))
Ebay = read.csv(paste(file.dir, input.file4, sep = ""))
Quotes= read.csv(paste(file.dir, input.file5, sep = ""))
Y= read.csv(paste(file.dir, input.file6, sep = ""))
x=read.csv(paste(file.dir, input.file7, sep = ""))
hurricane=read.csv(paste(file.dir, input.file8, sep = ""))
SteelShp <- ts(Steel$steelshp, start = 1984, frequency = 12)
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)
library(reticulate)
use_python("C:\\ProgramData\\Anaconda3\\envs\\R_Env2\\python.exe")
```

```{r include=FALSE}
Passenger <- ts(USAirlines$Passengers, start = 1990, frequency =12)

# Create training set from overall Airlines Data
training <- subset(Passenger, end = length(Passenger)-12)

# Create test set from overall Airlines Data
test <- subset(Passenger, start = length(Passenger)-11)
```

Prophet is a model framework introduced to the public by Facebook in 2018. Facebook uses this algorithm to forecast univariate time series by decomposing the it into pieces. This is similar to exponential smoothing, ETS, etc. In the Prophet model structure, the signal is broken down into three pieces - growth/trend, season, holiday.

The growth/trend component uses trend lines (time) as variables in the model. This trend is a **piecewise trend** that brakes the pattern into different pieces using **knots** to change the slope. The user of the algorithm can specify the knots, or the algorithm will try to automatically select them. This trend can also be a **logarithmic trend** that is similar in design to the dampened trend approach to exponential smoothing.

The seasonal component consists of Fourier variables to account for the seasonal pattern. The algorithm was originally designed for daily data with weekly and yearly seasonal effects. This can be expanded though to handle different types of data and seasons. The yearly season is set to 10 Fourier variables by default:

$$
X_Y = \cos(\frac{2\pi t}{365.25}) + \sin(2 \times \frac{2\pi t}{365.25}) + \cdots + \sin(10 \times \frac{2\pi t}{365.25})
$$

The weekly season is set to three Fourier variables by default:

$$
X_W = \cos(\frac{2\pi t}{7}) + \sin(2 \times \frac{2\pi t}{7}) + \cos(3 \times \frac{2\pi t}{7})
$$

The last component is the holiday component that consists of binary dummy variables to flag holidays in the dataset.

The `prophet` package contains all the functions needed for the Prophet algorithm, but it is in a different format than the previous R time series functions. To start, we define our own "holidays" in the dataset. This list is a list of dates that you want flagged with binary variables. For our dataset, this would be September, 2001 as well as its 6 lags and the one seasonal lag (anniversary). These must be in a data frame (using the `data.frame` function) as date objects. The `as.Date` function will do this. This data frame needs to have specific naming conventions. The variable name is called *holiday* while the date variable is called *ds*. Next, the training data needs a time variable also called *ds*.

Once the dataset is structured and ready, the `prophet` function can be employed. If we didn't have our own "holidays" to add to the algorithm, we would just use `prophet()` to activate the prophet model framework. However, we use the `holidays = holidays` option to input our own additional holidays. The `add_country_holidays` function is used to add pre-programmed holidays by country. We use the `US` option to specify US holidays. Next, we use the `add_seasonlity` function to add our own monthly seasonality. The `name = 'monthly'` option along with the `period = 30.5` defines the monthly season with a period of 30.5 days per season. We can also specify the number of Fourier variables with the `fourier.order` option. Now that we have the structure created, we fit the algorithm using the `fit.prophet`function on our model object *Prof* and dataset *prophet.data*.

```{r warning=FALSE}
holidays <- data.frame(
  holiday = 'Sep11',
  ds = as.Date(c('2001-09-01', '2001-10-01', '2001-11-01',
                 '2001-12-01', '2002-01-01', '2002-02-01',
                 '2002-09-01')),
  lower_window = 0,
  upper_window = 1
)
prophet.data <- data.frame(ds = seq(as.Date('1990-01-01'), as.Date('2007-03-01'), by = 'm'), y = training)

Prof <- prophet(holidays = holidays)
Prof <- add_country_holidays(Prof, "US")
Prof <- add_seasonality(Prof, name='monthly', period=30.5, fourier.order=6)
Prof <- fit.prophet(Prof, prophet.data)
```

Now that the model is created, we can forecast our dataset with the `make_future_dataframe` function with the `periods = 12` and `freq = 'month'` options to specify that we want 12 months of forecasts. This function structures the dataset (inputs) needed to forecast. The `predict` function actually does the forecasting. We can plot this forecast with the `plot` function and calculate our test dataset MAE and MAPE as well.

```{r warning=FALSE}
forecast.data <- make_future_dataframe(Prof, periods = 12, freq = 'month')
predict(Prof, forecast.data)$yhat
```

```{r warning=FALSE}
plot(Prof, predict(Prof, forecast.data))
```

```{r warning=FALSE}
# Calculate prediction errors from forecast
Prophet.error <- test - tail(predict(Prof, forecast.data)$yhat, 12)
```

```{r}
# Calculate prediction error statistics (MAE and MAPE)
Prophet.MAE <- mean(abs(Prophet.error))
Prophet.MAPE <- mean(abs(Prophet.error)/abs(test))*100

Prophet.MAE
```

```{r}
Prophet.MAPE
```

## Python Code for Prophet

### Model Structure

```{python}
from prophet import Prophet
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.graphics import tsaplots
from statsmodels.graphics import tsaplots
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.arima_model import ARMA
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing
import pmdarima as pm
usair = pd.read_csv("https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/usairlines.csv")
df=pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')
usair.index=pd.to_datetime(df)

training = usair.head(207)
test = usair.tail(12)


d = {'ds': training.index, 'y': training['Passengers']}
training_prophet = pd.DataFrame(data = d)
```

```{python}
m = Prophet(seasonality_mode='multiplicative').fit(training_prophet)

```

### Forecasting

```{python}
future = m.make_future_dataframe(periods=12, freq='MS')
fcst = m.predict(future)
fig = m.plot(fcst)

plt.show()
```

```{python}
fcst.index = fcst['ds']

error5 = test['Passengers'] - fcst['yhat'].tail(12)
MAPE5 = np.mean(abs(error5)/test['Passengers'])*100
MAPE5
```

<!--chapter:end:06-Prophet.Rmd-->

